{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.224088Z",
     "start_time": "2024-07-03T23:10:41.213413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 6.561726464\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print('-1:', psutil.virtual_memory()[3] / 1000000000)\n",
    "\n",
    "from typing import Callable, Optional, Tuple\n",
    "from fractions import Fraction\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.robust.internals.utils import ParamDict\n",
    "from chirho.robust.handlers.estimators import one_step_corrected_estimator \n",
    "from chirho.robust.ops import influence_fn\n",
    "from chirho.robust.handlers.predictive import PredictiveModel, PredictiveFunctional\n",
    "from chirho.robust.internals.nmc import BatchedNMCLogMarginalLikelihood\n",
    "from chirho.robust.handlers.estimators import MonteCarloInfluenceEstimator\n",
    "import psutil\n",
    "\n",
    "# import profiling tools\n",
    "from torch.profiler import profile, ProfilerActivity, record_function\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(32891) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.243309Z",
     "start_time": "2024-07-03T23:10:41.227839Z"
    }
   },
   "outputs": [],
   "source": [
    "class CausalGLM(pyro.nn.PyroModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p: int,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        prior_scale: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "        if prior_scale is None:\n",
    "            self.prior_scale = 1 / math.sqrt(self.p)\n",
    "        else:\n",
    "            self.prior_scale = prior_scale\n",
    "\n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\n",
    "            \"outcome_weights\",\n",
    "            dist.Normal(0.0, self.prior_scale).expand((self.p,)).to_event(1),\n",
    "        )\n",
    "\n",
    "    def sample_intercept(self):\n",
    "        return pyro.sample(\"intercept\", dist.Normal(0.0, 1.0))\n",
    "\n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\n",
    "            \"propensity_weights\",\n",
    "            dist.Normal(0.0, self.prior_scale).expand((self.p,)).to_event(1),\n",
    "        )\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0.0, 1.0))\n",
    "\n",
    "    def sample_covariate_loc_scale(self):\n",
    "        return torch.zeros(self.p), torch.ones(self.p)\n",
    "\n",
    "    def forward(self):\n",
    "        with record_function(\"causal_glm_forward\"):\n",
    "            intercept = self.sample_intercept()\n",
    "            outcome_weights = self.sample_outcome_weights()\n",
    "            propensity_weights = self.sample_propensity_weights()\n",
    "            tau = self.sample_treatment_weight()\n",
    "            x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "            X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1))\n",
    "            A = pyro.sample(\n",
    "                \"A\",\n",
    "                dist.Bernoulli(\n",
    "                    logits=torch.einsum(\"...i,...i->...\", X, propensity_weights)\n",
    "                ),\n",
    "            )\n",
    "    \n",
    "            return pyro.sample(\n",
    "                \"Y\",\n",
    "                self.link_fn(\n",
    "                    torch.einsum(\"...i,...i->...\", X, outcome_weights) + A * tau + intercept\n",
    "                ),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.252236Z",
     "start_time": "2024-07-03T23:10:41.232450Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConditionedCausalGLM(CausalGLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        A: torch.Tensor,\n",
    "        Y: torch.Tensor,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        prior_scale: Optional[float] = None,\n",
    "    ):\n",
    "        p = X.shape[1]\n",
    "        super().__init__(p, link_fn, prior_scale)\n",
    "        self.X = X\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "\n",
    "    def forward(self):\n",
    "        with record_function(\"conditioned_causal_glm_forward\"):\n",
    "            intercept = self.sample_intercept()\n",
    "            outcome_weights = self.sample_outcome_weights()\n",
    "            propensity_weights = self.sample_propensity_weights()\n",
    "            tau = self.sample_treatment_weight()\n",
    "            x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "            with pyro.plate(\"__train__\", size=self.X.shape[0], dim=-1):\n",
    "                X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1), obs=self.X)\n",
    "                A = pyro.sample(\n",
    "                    \"A\",\n",
    "                    dist.Bernoulli(\n",
    "                        logits=torch.einsum(\"ni,i->n\", self.X, propensity_weights)\n",
    "                    ),\n",
    "                    obs=self.A,\n",
    "                )\n",
    "                pyro.sample(\n",
    "                    \"Y\",\n",
    "                    self.link_fn(\n",
    "                        torch.einsum(\"ni,i->n\", X, outcome_weights)\n",
    "                        + A * tau\n",
    "                        + intercept\n",
    "                    ),\n",
    "                    obs=self.Y,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.255090Z",
     "start_time": "2024-07-03T23:10:41.245279Z"
    }
   },
   "outputs": [],
   "source": [
    "class GroundTruthModel(CausalGLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        treatment_weight: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha  # sparsity of propensity weights\n",
    "        self.beta = beta  # sparsity of outcome weights\n",
    "        self.treatment_weight = treatment_weight\n",
    "\n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta :] = 0.0\n",
    "        return outcome_weights\n",
    "\n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha :] = 0.0\n",
    "        return propensity_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(self.treatment_weight)\n",
    "\n",
    "    def sample_intercept(self):\n",
    "        return torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.276516Z",
     "start_time": "2024-07-03T23:10:41.256027Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_data(p_dim, alpha, beta, N_train, N_test):\n",
    "    true_model = GroundTruthModel(p_dim, alpha, beta)\n",
    "\n",
    "    # Generate data\n",
    "    D_train = Predictive(\n",
    "        true_model, num_samples=N_train, return_sites=[\"X\", \"A\", \"Y\"]\n",
    "    )()\n",
    "    D_test = Predictive(\n",
    "        true_model, num_samples=N_test, return_sites=[\"X\", \"A\", \"Y\"]\n",
    "    )()\n",
    "    return D_train, D_test\n",
    "\n",
    "# N_datasets = len(simulated_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.277985Z",
     "start_time": "2024-07-03T23:10:41.259719Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_params(D_train, p_dim, alpha, beta):\n",
    "    with record_function(\"fit_params\"):\n",
    "        # Fit the model\n",
    "        start = time.time()\n",
    "        conditioned_model = ConditionedCausalGLM(\n",
    "                X=D_train[\"X\"], A=D_train[\"A\"], Y=D_train[\"Y\"]\n",
    "            )\n",
    "        guide_train = pyro.infer.autoguide.AutoDelta(conditioned_model)\n",
    "        elbo = pyro.infer.Trace_ELBO()(conditioned_model, guide_train)\n",
    "    \n",
    "        # initialize parameters\n",
    "        elbo()\n",
    "        adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "    \n",
    "        # Do gradient steps\n",
    "        for _ in range(2000):\n",
    "            adam.zero_grad()\n",
    "            loss = elbo()\n",
    "            loss.backward()\n",
    "            adam.step()\n",
    "    \n",
    "        model_fitting_time = time.time() - start\n",
    "    \n",
    "        theta_hat = {\n",
    "            k: v.clone().detach().requires_grad_(True) for k, v in guide_train().items()\n",
    "        }\n",
    "    \n",
    "        return theta_hat, model_fitting_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.297222Z",
     "start_time": "2024-07-03T23:10:41.279202Z"
    }
   },
   "outputs": [],
   "source": [
    "# fitted_params = []\n",
    "# model_fitting_time = []\n",
    "# N_trials = 10\n",
    "# for i in range(N_trials):\n",
    "#     for p_dim in p_grid:\n",
    "#         # Generate data\n",
    "#         D_train, D_test = generate_data\n",
    "\n",
    "#         start = time.time()\n",
    "#         # Fit model using maximum likelihood\n",
    "#         conditioned_model = ConditionedCausalGLM(\n",
    "#             X=D_train[\"X\"], A=D_train[\"A\"], Y=D_train[\"Y\"]\n",
    "#         )\n",
    "        \n",
    "#         guide_train = pyro.infer.autoguide.AutoDelta(conditioned_model)\n",
    "#         elbo = pyro.infer.Trace_ELBO()(conditioned_model, guide_train)\n",
    "\n",
    "#         # initialize parameters\n",
    "#         elbo()\n",
    "#         adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "\n",
    "#         # Do gradient steps\n",
    "#         for _ in range(2000):\n",
    "#             adam.zero_grad()\n",
    "#             loss = elbo()\n",
    "#             loss.backward()\n",
    "#             adam.step()\n",
    "\n",
    "#         model_fitting_time.append(time.time() - start)\n",
    "\n",
    "#         theta_hat = {\n",
    "#             k: v.clone().detach().requires_grad_(True) for k, v in guide_train().items()\n",
    "#         }\n",
    "#         fitted_params.append(theta_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:41.312352Z",
     "start_time": "2024-07-03T23:10:41.301002Z"
    }
   },
   "outputs": [],
   "source": [
    "class ATEFunctional(torch.nn.Module):\n",
    "    def __init__(self, model: Callable, *, num_monte_carlo: int = 100):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_monte_carlo = num_monte_carlo\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        with record_function(\"ate_functional\"):\n",
    "            with MultiWorldCounterfactual():\n",
    "                with pyro.plate(\"monte_carlo_functional\", size=self.num_monte_carlo, dim=-2):\n",
    "                    with do(actions=dict(A=(torch.tensor(0.0), torch.tensor(1.0)))):\n",
    "                        Ys = self.model(*args, **kwargs)\n",
    "                    Y0 = gather(Ys, IndexSet(A={1}), event_dim=0)\n",
    "                    Y1 = gather(Ys, IndexSet(A={2}), event_dim=0)\n",
    "            ate = (Y1 - Y0).mean(dim=-2, keepdim=True).mean(dim=-1, keepdim=True).squeeze()\n",
    "            return pyro.deterministic(\"ATE\", ate)\n",
    "    \n",
    "# Closed form expression\n",
    "def closed_form_doubly_robust_ate_correction(X_test, theta) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = X_test[\"X\"]\n",
    "    A = X_test[\"A\"]\n",
    "    Y = X_test[\"Y\"]\n",
    "    pi_X = torch.sigmoid(X.mv(theta[\"propensity_weights\"]))\n",
    "    mu_X = (\n",
    "        X.mv(theta[\"outcome_weights\"])\n",
    "        + A * theta[\"treatment_weight\"]\n",
    "        + theta[\"intercept\"]\n",
    "    )\n",
    "    analytic_eif_at_test_pts = (A / pi_X - (1 - A) / (1 - pi_X)) * (Y - mu_X)\n",
    "    analytic_correction = analytic_eif_at_test_pts.mean()\n",
    "    return analytic_correction, analytic_eif_at_test_pts\n",
    "\n",
    "# Helper class to create a trivial guide that returns the maximum likelihood estimate\n",
    "class MLEGuide(torch.nn.Module):\n",
    "    def __init__(self, mle_est: ParamDict):\n",
    "        super().__init__()\n",
    "        self.names = list(mle_est.keys())\n",
    "        for name, value in mle_est.items():\n",
    "            setattr(self, name + \"_param\", torch.nn.Parameter(value))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        with record_function(\"mle_guide\"):\n",
    "            for name in self.names:\n",
    "                value = getattr(self, name + \"_param\")\n",
    "                pyro.sample(\n",
    "                    name, pyro.distributions.Delta(value, event_dim=len(value.shape))\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 6.577504256\n",
      "A: 6.537003008\n"
     ]
    }
   ],
   "source": [
    "p=50  # memory scales with this because jvp is bad\n",
    "alpha=50\n",
    "beta=50\n",
    "N_train=5000\n",
    "N_test=500  # memory scales with this because the vmap operates over this dimension, with one jvp call each.\n",
    "\n",
    "print('0:', psutil.virtual_memory()[3] / 1000000000)\n",
    "\n",
    "D_train, D_test = generate_data(p, alpha, beta, N_train, N_test)\n",
    "theta_hat, model_fitting_time = fit_params(D_train, p, alpha, beta)\n",
    "# model_fitting_times[p].append(model_fitting_time)\n",
    "\n",
    "mle_guide = MLEGuide(theta_hat)\n",
    "functional = functools.partial(ATEFunctional, num_monte_carlo=10000)\n",
    "ate_plug_in = functional(\n",
    "    PredictiveModel(CausalGLM(p), mle_guide)\n",
    ")()\n",
    "analytic_correction, analytic_eif_at_test_pts = closed_form_doubly_robust_ate_correction(D_test, theta_hat)\n",
    "\n",
    "# start = time.time()\n",
    "monte_eif = influence_fn(functional, D_test)\n",
    "\n",
    "print('A:', psutil.virtual_memory()[3] / 1000000000)\n",
    "\n",
    "with MonteCarloInfluenceEstimator(\n",
    "    num_samples_outer=1000,\n",
    "    num_samples_inner=1,\n",
    "    cg_iters=1\n",
    "):\n",
    "    with torch.no_grad():\n",
    "        # with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True, with_stack=True) as prof:\n",
    "        #     with record_function(\"monte_eif\"):\n",
    "        monte_eif_at_test_pts = monte_eif(PredictiveModel(CausalGLM(p), mle_guide))()\n",
    "# end = time.time()\n",
    "# automated_monte_carlo_times[p].append(end - start)\n",
    "# automated_monte_carlo_at_test[p].append(monte_eif_at_test_pts)\n",
    "# analytic_at_test[p].append(analytic_eif_at_test_pts)\n",
    "\n",
    "# plug_in_ates[p].append(ate_plug_in.detach().item())\n",
    "# analytic_corrections[p].append(ate_plug_in.detach().item() + analytic_correction.detach().item())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:47.314993Z",
     "start_time": "2024-07-03T23:10:41.305714Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Axes: ylabel='Count'>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.histplot(monte_eif_at_test_pts.detach())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-03T23:10:47.462999Z",
     "start_time": "2024-07-03T23:10:47.329023Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prof.export_memory_timeline(\"/Users/azane/Desktop/mceif_prof.html\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "prof.export_chrome_trace(\"/Users/azane/Desktop/trace.json\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Memory profile the forward op of the glm.\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True, profile_memory=True, with_stack=True) as glm_prof:\n",
    "    with record_function(\"glm_forward\"):\n",
    "        # for pd in range(1, 1000):\n",
    "        CausalGLM(p=1000)()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(glm_prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Memory timeline.\n",
    "glm_prof.export_memory_timeline(\"/Users/azane/Desktop/glm_prof.html\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "##################################################\n",
    "# below not currently relevant to memory profile #\n",
    "##################################################"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data configuration\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 500\n",
    "N_test = 50\n",
    "p_grid = [1, 10, 25, 50, 100, 150, 200, 250, 300, 500]\n",
    "\n",
    "\n",
    "# Compute doubly robust ATE estimates using both the automated and closed form expressions\n",
    "N_trials = 25\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "RESULTS_PATH = 'results/error_vs_dimension.pkl'\n",
    "\n",
    "if os.path.exists(RESULTS_PATH):\n",
    "    with open(RESULTS_PATH, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "        plug_in_ates = results['plug_in_ates']\n",
    "        model_fitting_times = results['model_fitting_times']\n",
    "        analytic_corrections = results['analytic_corrections']\n",
    "        automated_monte_carlo_corrections = results['automated_monte_carlo_corrections']\n",
    "        automated_monte_carlo_at_test = results['automated_monte_carlo_at_test']\n",
    "        automated_monte_carlo_times = results['automated_monte_carlo_times']\n",
    "        analytic_at_test = results['analytic_at_test']\n",
    "    i_start = len(plug_in_ates[p_grid[0]])\n",
    "else:\n",
    "    plug_in_ates = {p:[] for p in p_grid}\n",
    "    model_fitting_times = {p:[] for p in p_grid}\n",
    "    analytic_corrections = {p:[] for p in p_grid}\n",
    "    automated_monte_carlo_corrections = {p:[] for p in p_grid}\n",
    "    automated_monte_carlo_at_test = {p:[] for p in p_grid}\n",
    "    automated_monte_carlo_times = {p:[] for p in p_grid}\n",
    "    analytic_at_test = {p:[] for p in p_grid}\n",
    "    i_start = 0\n",
    "\n",
    "for i in range(i_start, N_trials):\n",
    "    for p in p_grid:\n",
    "        print(i, p)\n",
    "        D_train, D_test = generate_data(p, alpha, beta, N_train, N_test)\n",
    "        theta_hat, model_fitting_time = fit_params(D_train, p, alpha, beta)\n",
    "        model_fitting_times[p].append(model_fitting_time)\n",
    "\n",
    "        mle_guide = MLEGuide(theta_hat)\n",
    "        functional = functools.partial(ATEFunctional, num_monte_carlo=10000)\n",
    "        ate_plug_in = functional(\n",
    "            PredictiveModel(CausalGLM(p), mle_guide)\n",
    "        )()\n",
    "        analytic_correction, analytic_eif_at_test_pts = closed_form_doubly_robust_ate_correction(D_test, theta_hat)\n",
    "\n",
    "        start = time.time()\n",
    "        monte_eif = influence_fn(functional, D_test, num_samples_outer=10000, num_samples_inner=1)\n",
    "        monte_eif_at_test_pts = monte_eif(PredictiveModel(CausalGLM(p), mle_guide))()\n",
    "        end = time.time()\n",
    "        automated_monte_carlo_times[p].append(end - start)\n",
    "        automated_monte_carlo_at_test[p].append(monte_eif_at_test_pts)\n",
    "        analytic_at_test[p].append(analytic_eif_at_test_pts)\n",
    "\n",
    "        plug_in_ates[p].append(ate_plug_in.detach().item())\n",
    "        analytic_corrections[p].append(ate_plug_in.detach().item() + analytic_correction.detach().item())\n",
    "        # automated_monte_carlo_corrections[p].append(ate_plug_in.detach().item() + monte_eif_at_test_pts.mean().detach().item())\n",
    "\n",
    "    with open(RESULTS_PATH, 'wb') as f:\n",
    "        results = {\n",
    "            'plug_in_ates': plug_in_ates,\n",
    "            'model_fitting_times': model_fitting_times,\n",
    "            'analytic_corrections': analytic_corrections,\n",
    "            'automated_monte_carlo_corrections': automated_monte_carlo_corrections,\n",
    "            'automated_monte_carlo_at_test': automated_monte_carlo_at_test,\n",
    "            'automated_monte_carlo_times': automated_monte_carlo_times,\n",
    "            'analytic_at_test': analytic_at_test\n",
    "        }\n",
    "        pickle.dump(results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_rel_error(x, y):\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "    return torch.median(torch.abs(x - y) / torch.abs(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_eif_errors = {p: torch.tensor([median_rel_error(x, y) for x, y in zip(automated_monte_carlo_at_test[p], analytic_at_test[p])]) for p in p_grid}\n",
    "monte_eif_error_means = {p: monte_eif_errors[p].mean() for p in p_grid}\n",
    "monte_eif_error_stderrs = {p: monte_eif_errors[p].std() / math.sqrt(N_trials) for p in p_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dimension_grid = [2 * p + 2 for p in p_grid]\n",
    "unpacked_model_dimension_grid = [2 * p + 2 for p in p_grid for _ in monte_eif_errors[p]]\n",
    "unpacked_monte_eif_errors = [e.item() for p in p_grid for e in monte_eif_errors[p]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_rate_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_rate_denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "obs_slope, obs_intercept = np.polyfit(\n",
    "    np.log10(unpacked_model_dimension_grid), \n",
    "    np.log10(unpacked_monte_eif_errors), 1)\n",
    "\n",
    "theory_slope = 1/2\n",
    "theory_intercept = (np.log10(unpacked_monte_eif_errors) - theory_slope * np.log10(unpacked_model_dimension_grid)).mean()\n",
    "\n",
    "# Plot results\n",
    "plt.errorbar(model_dimension_grid, \n",
    "             list(monte_eif_error_means.values()), \n",
    "             yerr=list(monte_eif_error_stderrs.values()), \n",
    "             fmt='.-',\n",
    "             label='MC-EIF (10k Monte Carlo samples)',\n",
    "             color='#154c79',\n",
    "             marker='o')\n",
    "obs_rate_num = Fraction(int(round(8*obs_slope)),8).numerator\n",
    "obs_rate_denom = Fraction(int(round(8*obs_slope)),8).denominator\n",
    "plt.plot(model_dimension_grid, \n",
    "    10**obs_intercept * np.power(model_dimension_grid, obs_slope), \n",
    "    label='Estimated Error Rate: O($p^{7/16}$)',\n",
    "    color='red',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.plot(model_dimension_grid, \n",
    "    10**theory_intercept * np.power(model_dimension_grid, theory_slope), \n",
    "    label='Our Theory: O($p^{1/2}$)',\n",
    "    color='green',\n",
    "    linestyle='--'\n",
    ")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Median Relative Error', fontsize=24)\n",
    "plt.xlabel('Model Dimension (p)', fontsize=24)\n",
    "sns.despine()\n",
    "plt.legend(fontsize=13)\n",
    "\n",
    "plt.yticks([0.01, 0.1, 0.5], fontsize=20)\n",
    "plt.xticks([10, 100, 1000], fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('./figures/error_rate_causal_glm_vs_dim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fitting_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_monte_carlo_time_means = {p: np.mean(automated_monte_carlo_times[p]) for p in p_grid}\n",
    "automated_monte_carlo_time_stderrs = {p: np.std(automated_monte_carlo_times[p]) / math.sqrt(N_trials) for p in p_grid}\n",
    "\n",
    "model_fitting_time_means = {p: np.mean(model_fitting_times[p]) for p in p_grid}\n",
    "model_fitting_time_stderrs = {p: np.std(model_fitting_times[p]) / math.sqrt(N_trials) for p in p_grid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "plt.errorbar(model_dimension_grid, \n",
    "             list(automated_monte_carlo_time_means.values()), \n",
    "             yerr=list(automated_monte_carlo_time_stderrs.values()), \n",
    "             fmt='.-',\n",
    "             label='MC-EIF (10k Monte Carlo samples)',\n",
    "             color='#154c79',\n",
    "             marker='o')\n",
    "\n",
    "plt.errorbar(model_dimension_grid,\n",
    "                list(model_fitting_time_means.values()),\n",
    "                yerr=list(model_fitting_time_stderrs.values()),\n",
    "                fmt='.-',\n",
    "                label='Model Fitting Time',\n",
    "                color='#f95d6a',\n",
    "                marker='o'\n",
    "    )\n",
    "\n",
    "# plt.plot(\n",
    "#     model_dimension_grid, \n",
    "#     model_fitting_time, \n",
    "#     label='Model Fitting Time', \n",
    "#     color='#f95d6a',\n",
    "#     marker='o'\n",
    "\n",
    "# )\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "plt.ylabel('Runtime (s)', fontsize=24)\n",
    "plt.xlabel('Model Dimension (p)', fontsize=24)\n",
    "sns.despine()\n",
    "plt.legend(fontsize=13)\n",
    "plt.tight_layout()\n",
    "# plt.yticks([0.1, 0.2, 0.3, 0.4, 0.5], fontsize=20)\n",
    "# plt.xticks([10, 100, 1000], fontsize=20)\n",
    "plt.savefig('./figures/runtime_causal_glm_vs_dim.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
