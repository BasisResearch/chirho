{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=-1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=-1\n",
    "\n",
    "import os\n",
    "\n",
    "import contextlib\n",
    "from typing import Callable, Mapping, TypeVar, Union\n",
    "\n",
    "import pyro.distributions.constraints as constraints\n",
    "import torch\n",
    "\n",
    "from chirho.explainable.handlers.components import (\n",
    "    consequent_eq,\n",
    "    consequent_neq,\n",
    "    consequent_eq_neq,\n",
    "    random_intervention,\n",
    "    sufficiency_intervention,\n",
    "    undo_split,\n",
    "    do,\n",
    ")\n",
    "from chirho.explainable.handlers.preemptions import Preemptions\n",
    "#from chirho.interventional.handlers import do\n",
    "from chirho.interventional.ops import Intervention\n",
    "from chirho.observational.handlers.condition import Factors\n",
    "\n",
    "S = TypeVar(\"S\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Callable, Mapping, TypeVar, Union\n",
    "\n",
    "\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\n",
    "from chirho.explainable.handlers import SplitSubsets, SearchForExplanation #, SearchForNS\n",
    "\n",
    "from chirho.indexed.ops import (IndexSet, gather, indices_of) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_disjunctive():\n",
    "        match_dropped = pyro.sample(\"match_dropped\", dist.Bernoulli(0.7)) # notice uneven probs here\n",
    "        lightning = pyro.sample(\"lightning\", dist.Bernoulli(0.4))\n",
    "\n",
    "        forest_fire = pyro.deterministic(\"forest_fire\", torch.max(match_dropped, lightning), event_dim=0)\n",
    "\n",
    "        return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n",
    "\n",
    "observations = {\"match_dropped\": torch.tensor(1.), \n",
    "                \"lightning\": torch.tensor(0.),\n",
    "                \"forest_fire\": torch.tensor(1.)}\n",
    "\n",
    "antecedents = {\"match_dropped\": torch.tensor(0.0)} \n",
    "\n",
    "witnesses = {} # ignore witnesses for now\n",
    "\n",
    "consequents = {\"forest_fire\": constraints.boolean}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def SearchForNS(\n",
    "    antecedents: Union[\n",
    "        Mapping[str, Intervention[T]],\n",
    "        Mapping[str, constraints.Constraint],\n",
    "    ],\n",
    "    witnesses: Union[\n",
    "        Mapping[str, Intervention[T]], Mapping[str, constraints.Constraint]\n",
    "    ],\n",
    "    consequents: Union[\n",
    "        Mapping[str, Callable[[T], Union[float, torch.Tensor]]],\n",
    "        Mapping[str, constraints.Constraint],\n",
    "    ],\n",
    "    *,\n",
    "    antecedent_bias: float = 0.0,\n",
    "    witness_bias: float = 0.0,\n",
    "    consequent_scale: float = 1e-2,\n",
    "    antecedent_prefix: str = \"__antecedent_\",\n",
    "    witness_prefix: str = \"__witness_\",\n",
    "    consequent_prefix: str = \"__consequent_\",\n",
    "    intervention_postfix = \"\",\n",
    "):\n",
    "    \"\"\"\n",
    "    # TODO revise this docstring\n",
    "    Effect handler used for causal explanation search. On each run:\n",
    "\n",
    "      1. The antecedent nodes are intervened on with the values in ``antecedents`` \\\n",
    "        using :func:`~chirho.counterfactual.ops.split` . \\\n",
    "        Unless alternative interventions are provided, \\\n",
    "        counterfactual values are uniformly sampled for each antecedent node \\\n",
    "        using :func:`~chirho.explainable.internals.uniform_proposal` \\\n",
    "        given its support as a :class:`~pyro.distributions.constraints.Constraint`.\n",
    "\n",
    "      2. These interventions are randomly :func:`~chirho.explainable.ops.preempt`-ed \\\n",
    "        using :func:`~chirho.explainable.handlers.undo_split` \\\n",
    "        by a :func:`~chirho.explainable.handlers.SplitSubsets` handler.\n",
    "\n",
    "      3. The witness nodes are randomly :func:`~chirho.explainable.ops.preempt`-ed \\\n",
    "        to be kept at the values given in ``witnesses``.\n",
    "\n",
    "      4. A :func:`~pyro.factor` node is added tracking whether the consequent nodes differ \\\n",
    "        between the factual and counterfactual worlds.\n",
    "\n",
    "    :param antecedents: A mapping from antecedent names to interventions or to constraints.\n",
    "    :param witnesses: A mapping from witness names to interventions or to constraints.\n",
    "    :param consequents: A mapping from consequent names to factor functions or to constraints.\n",
    "    \"\"\"\n",
    "\n",
    "    # intervention_names will disentangle antecedent interventions\n",
    "    # from potential interventions on antecedent nodes upstream\n",
    "    # and allow for programmatic indexing into counterfactual worlds\n",
    "    # within consequent eq_neq\n",
    "    \n",
    "\n",
    "    intervention_names = {f\"{antecedent}{intervention_postfix}\" for antecedent in antecedents.keys()}\n",
    "\n",
    "    if antecedents and isinstance(\n",
    "        next(iter(antecedents.values())),\n",
    "        constraints.Constraint,\n",
    "    ):\n",
    "        \n",
    "        antecedents_supports = {a: s for a, s in antecedents.items()}\n",
    "\n",
    "        antecedents = {\n",
    "            a: (\n",
    "                random_intervention(s, name=f\"{antecedent_prefix}_proposal_{a}\"),\n",
    "                sufficiency_intervention(intervention_names),\n",
    "            )\n",
    "            for a, s in antecedents_supports.items()\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        \n",
    "        antecedents_supports = {a: constraints.boolean for a in antecedents.keys()}\n",
    "        # TODO generalize to non-scalar antecedents\n",
    "        # comment: how about extracting supports? \n",
    "        \n",
    "        antecedents = {\n",
    "            a: (\n",
    "                antecedents[a], \n",
    "                sufficiency_intervention(intervention_names)\n",
    "                )\n",
    "             \n",
    "            for a, s in antecedents_supports.items()\n",
    "        }\n",
    "\n",
    "\n",
    "    if witnesses and isinstance(\n",
    "        next(iter(witnesses.values())),\n",
    "        constraints.Constraint,\n",
    "    ):\n",
    "        witnesses = {\n",
    "            w: undo_split(s, antecedents=list(antecedents.keys()))\n",
    "            for w, s in witnesses.items()\n",
    "        }\n",
    "\n",
    "    if consequents and isinstance(\n",
    "        next(iter(consequents.values())),\n",
    "        constraints.Constraint,\n",
    "    ):\n",
    "\n",
    "        consequents_eq = {\n",
    "            c: consequent_eq(\n",
    "                support=s,\n",
    "                antecedents=list(antecedents.keys()),\n",
    "                scale=consequent_scale, #TODO allow for different scales for neq and eq\n",
    "            )\n",
    "            for c, s in consequents.items()\n",
    "        \n",
    "        }\n",
    "\n",
    "        consequents_neq = {\n",
    "            c: consequent_neq(\n",
    "                support=s,\n",
    "                antecedents=list(antecedents.keys()),\n",
    "                scale=consequent_scale, #TODO allow for different scales for neq and eq\n",
    "            )\n",
    "            for c, s in consequents.items()\n",
    "        }\n",
    "\n",
    "\n",
    "        consequents_eq_neq = {\n",
    "            c: consequent_eq_neq(\n",
    "                support=s,\n",
    "                antecedents=list(antecedents.keys()),\n",
    "                intervention_names=intervention_names,\n",
    "                scale=consequent_scale, #TODO allow for different scales for neq and eq\n",
    "            )\n",
    "            for c, s in consequents.items()\n",
    "        }\n",
    "\n",
    "    if len(consequents_neq) == 0:\n",
    "        raise ValueError(\"must have at least one consequent\")\n",
    "\n",
    "    if len(antecedents) == 0:\n",
    "        raise ValueError(\"must have at least one antecedent\")\n",
    "\n",
    "    if set(consequents_neq.keys()) & set(antecedents.keys()):\n",
    "        raise ValueError(\"consequents and possible antecedents must be disjoint\")\n",
    "\n",
    "    if set(consequents_neq.keys()) & set(witnesses.keys()):\n",
    "        raise ValueError(\"consequents and possible witnesses must be disjoint\")\n",
    "\n",
    "    antecedent_handler = SplitSubsets(\n",
    "        supports=antecedents_supports,\n",
    "        actions=antecedents,\n",
    "        intervention_postfix = intervention_postfix,\n",
    "        bias=antecedent_bias,\n",
    "        prefix=antecedent_prefix,\n",
    "    )\n",
    "\n",
    "    witness_handler: Preemptions = Preemptions(\n",
    "        actions=witnesses, bias=witness_bias, prefix=witness_prefix\n",
    "    )\n",
    "\n",
    "    consequent_eq_handler = Factors(factors=consequents_eq, prefix=f\"{consequent_prefix}_eq_\")\n",
    "\n",
    "    consequent_neq_handler = Factors(factors=consequents_neq, prefix=f\"{consequent_prefix}_neq_\")\n",
    "\n",
    "    consequent_eq_neq_handler = Factors(factors=consequents_eq_neq, prefix=f\"{consequent_prefix}_eq_neq_\")\n",
    "\n",
    "\n",
    "    with antecedent_handler, witness_handler, consequent_neq_handler, consequent_eq_handler, consequent_eq_neq_handler:\n",
    "        yield\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiWorldCounterfactual() as ffd_ns_mwc:  # needed to keep track of multiple scenarios\n",
    "    with SearchForNS(antecedents = antecedents,\n",
    "    antecedent_bias = -0.5, \n",
    "                              witnesses = witnesses,\n",
    "                              consequents = consequents,\n",
    "                              consequent_scale= 1e-8):\n",
    "        with condition(data = observations):\n",
    "                with pyro.poutine.trace() as ffd_ns_tr:\n",
    "                    ff_disjunctive()\n",
    "\n",
    "ffd_ns_tr.trace.compute_log_prob() \n",
    "ffd_ns_nd = ffd_ns_tr.trace.nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eqneq tensor([[[[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]]]])\n",
      "neq tensor([[[[[-inf]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[0.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-inf]]]]])\n",
      "eq tensor([[[[[  0.0000]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-18.4207]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[  0.0000]]]]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with ffd_ns_mwc:\n",
    "    match_observed = gather(ffd_ns_nd['match_dropped']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {0}}))\n",
    "    match_intervened = gather(ffd_ns_nd['match_dropped']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {1}}))\n",
    "    match_sufficiency = gather(ffd_ns_nd['match_dropped']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {2}}))\n",
    "\n",
    "    neq_log_prob_observed = gather(ffd_ns_nd['__consequent__neq_forest_fire']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {0}}))\n",
    "    neq_log_prob_intervened = gather(ffd_ns_nd['__consequent__neq_forest_fire']['log_prob'],\n",
    "                IndexSet(**{'match_dropped': {1}}))\n",
    "    neq_log_prob_sufficiency = gather(ffd_ns_nd['__consequent__neq_forest_fire']['log_prob'],\n",
    "                IndexSet(**{'match_dropped': {2}}))\n",
    "\n",
    "\n",
    "assert torch.equal(match_observed, match_sufficiency)\n",
    "assert torch.equal(neq_log_prob_observed, neq_log_prob_sufficiency)\n",
    "assert neq_log_prob_observed < 1e-4\n",
    "assert neq_log_prob_intervened == 0 \n",
    "\n",
    "assert not torch.equal(match_intervened, match_observed)\n",
    "\n",
    "print(\"eqneq\", ffd_ns_nd['__consequent__eq_neq_forest_fire']['log_prob'])\n",
    "\n",
    "\n",
    "\n",
    "print(\"neq\", ffd_ns_nd['__consequent__neq_forest_fire']['log_prob'])\n",
    "\n",
    "print(\"eq\", ffd_ns_nd['__consequent__eq_forest_fire']['log_prob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexSet({'match_dropped': {0, 1, 2}})\n"
     ]
    }
   ],
   "source": [
    "with mwc:\n",
    "    print(indices_of(ffd_ns_nd['__consequent__neq_forest_fire']['log_prob']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
