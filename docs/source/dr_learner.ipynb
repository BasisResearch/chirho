{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional, List, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from chirho.indexed.handlers import IndexPlatesMessenger\n",
    "from chirho.observational.handlers.soft_conditioning import IndexCutModule, cut\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaLinearCATE(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward():\n",
    "        intercept = pyro.sample(\"intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"theta\", dist.Normal(0., 1.).expand((self.p, )).to_event(1))\n",
    "        tau = pyro.sample(\"tau\", dist.Normal(0., 1.))\n",
    "        return lambda x, a: intercept + x @ weights + a * tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDRLearner(pyro.nn.PyroModule):\n",
    "    \"\"\"\n",
    "    AbstractDRLearner is a base class for all DR learners. It is based on the following paper:\n",
    "    https://arxiv.org/abs/2004.14497.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, p: int, outcome_noise_scale: float = 1., pseudo_noise_scale: float = 1.\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.outcome_noise_scale = outcome_noise_scale\n",
    "        self.pseudo_noise_scale = pseudo_noise_scale\n",
    "    \n",
    "    def _get_module_param(self, param, module_ix):\n",
    "        if len(param.shape) > 1:\n",
    "            return param[module_ix].squeeze()\n",
    "        return param\n",
    "    \n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def pseudo_outcome(\n",
    "        self, D: Dict, propensity_model: Callable, outcome_model: Callable, module_ix: int\n",
    "    ) -> torch.Tensor:\n",
    "        X, A, Y = D[\"X\"], D[\"A\"], D[\"Y\"]\n",
    "        N = X.shape[0]\n",
    "        propensity_scores = propensity_model(X, module_ix)\n",
    "        probs = torch.special.expit(propensity_scores)\n",
    "        outcome_0 = outcome_model(X, torch.zeros(N), module_ix)\n",
    "        outcome_1 = outcome_model(X, torch.ones(N), module_ix)\n",
    "        outcome_A = outcome_model(X, A, module_ix)\n",
    "        traditional_term = outcome_1 - outcome_0  # vanilla cate estimate\n",
    "        correction_term = (Y - outcome_A) * (A - probs) / (probs * (1 - probs))  # double robustness correction\n",
    "        return traditional_term + correction_term\n",
    "    \n",
    "    def forward(self, D1: Dict, D2: Dict, **kwargs):\n",
    "        propensity_score = self.sample_propensity_model(**kwargs)\n",
    "        outcome_model = self.sample_outcome_model(**kwargs)\n",
    "        cate_model = self.sample_cate(**kwargs)\n",
    "\n",
    "        # Treatment conditioning\n",
    "        X1, A1, Y1 = D1['X'], D1['A'], D1['Y']\n",
    "        with pyro.plate(\"treatment\", X1.shape[0]):\n",
    "            pyro.sample(\"A\", dist.Bernoulli(logits=propensity_score(X1, 0)), obs=A1)\n",
    "    \n",
    "        # Outcome conditioning\n",
    "        with pyro.plate(\"outcome\", X1.shape[0]):\n",
    "            pyro.sample(\"Y\", dist.Normal(outcome_model(X1, A1, 0), self.outcome_noise_scale), obs=Y1)\n",
    "        \n",
    "        # pseudo outcome conditioning\n",
    "        X2 = D2['X']\n",
    "        Y_pseudo = self.pseudo_outcome(D2, propensity_score, outcome_model, 1)\n",
    "        with pyro.plate(\"pseudo_outcome\", X2.shape[0]):\n",
    "            # Hack to get correct conditional dependence structure rendered\n",
    "            # pyro.sample(\"Y_pseudo_residuals\", dist.Normal(Y_pseudo - cate_model(X2, 1), self.pseudo_noise_scale), obs=torch.zeros(X2.shape[0]))\n",
    "            pyro.sample(\"Y_pseudo_residuals\", dist.Normal(cate_model(X2, 1), self.pseudo_noise_scale), obs=Y_pseudo)\n",
    "\n",
    "\n",
    "class LinearDRLearner(AbstractDRLearner):\n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"propensity_intercept\", dist.Normal(0., 1./math.sqrt(self.p)))\n",
    "        weights = pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"outcome_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"outcome_weights\", dist.Normal(0., 1.).expand((self.p, )).to_event(1))\n",
    "        tau = pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "        return lambda x, a, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix) + a * self._get_module_param(tau, module_ix)\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        intercept = pyro.sample(\"cate_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"cate_weights\", dist.Normal(0., 1.).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"1118pt\" height=\"155pt\"\n",
       " viewBox=\"0.00 0.00 1118.09 155.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 151)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-151 1114.09,-151 1114.09,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_treatment</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134.29,-8 134.29,-83 204.29,-83 204.29,-8 134.29,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"170.29\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">treatment</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_outcome</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"838.29,-8 838.29,-83 908.29,-83 908.29,-8 838.29,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"876.29\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">outcome</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_pseudo_outcome</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"461.29,-8 461.29,-83 641.29,-83 641.29,-8 461.29,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"586.79\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">pseudo_outcome</text>\n",
       "</g>\n",
       "<!-- propensity_intercept -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>propensity_intercept</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"259.29\" cy=\"-129\" rx=\"83.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"259.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">propensity_intercept</text>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"169.29\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"169.29\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- propensity_intercept&#45;&gt;A -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>propensity_intercept&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M237.96,-111.41C224.88,-101.23 208.06,-88.15 194.34,-77.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.3,-74.57 186.26,-71.2 192.01,-80.1 196.3,-74.57\"/>\n",
       "</g>\n",
       "<!-- Y_pseudo_residuals -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>Y_pseudo_residuals</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"551.29\" cy=\"-57\" rx=\"81.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"551.29\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y_pseudo_residuals</text>\n",
       "</g>\n",
       "<!-- propensity_intercept&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>propensity_intercept&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.85,-115.16C362.35,-103.3 435.92,-85.66 488.24,-73.12\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.17,-76.49 498.08,-70.76 487.53,-69.69 489.17,-76.49\"/>\n",
       "</g>\n",
       "<!-- propensity_weights -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>propensity_weights</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"79.29\" cy=\"-129\" rx=\"79.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">propensity_weights</text>\n",
       "</g>\n",
       "<!-- propensity_weights&#45;&gt;A -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>propensity_weights&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M100.62,-111.41C113.71,-101.23 130.53,-88.15 144.25,-77.48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"146.58,-80.1 152.33,-71.2 142.28,-74.57 146.58,-80.1\"/>\n",
       "</g>\n",
       "<!-- propensity_weights&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>propensity_weights&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136.49,-116.49C146.73,-114.56 157.31,-112.65 167.29,-111 271.98,-93.68 393.38,-77.59 471.19,-67.78\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"471.92,-71.22 481.4,-66.5 471.04,-64.27 471.92,-71.22\"/>\n",
       "</g>\n",
       "<!-- outcome_intercept -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>outcome_intercept</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"705.29\" cy=\"-129\" rx=\"76.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"705.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">outcome_intercept</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"873.29\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.29\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- outcome_intercept&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>outcome_intercept&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M741.33,-112.98C771,-100.62 812.67,-83.26 841.2,-71.37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"842.82,-74.49 850.71,-67.41 840.13,-68.03 842.82,-74.49\"/>\n",
       "</g>\n",
       "<!-- outcome_intercept&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>outcome_intercept&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M671.5,-112.64C648.91,-102.37 618.98,-88.77 594.64,-77.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"595.83,-74.4 585.28,-73.45 592.93,-80.77 595.83,-74.4\"/>\n",
       "</g>\n",
       "<!-- outcome_weights -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>outcome_weights</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"873.29\" cy=\"-129\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"873.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">outcome_weights</text>\n",
       "</g>\n",
       "<!-- outcome_weights&#45;&gt;Y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>outcome_weights&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M873.29,-110.7C873.29,-102.98 873.29,-93.71 873.29,-85.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"876.79,-85.1 873.29,-75.1 869.79,-85.1 876.79,-85.1\"/>\n",
       "</g>\n",
       "<!-- outcome_weights&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>outcome_weights&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M820.12,-116.44C764.57,-104.36 677.31,-85.4 617.37,-72.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"617.76,-68.87 607.25,-70.16 616.28,-75.71 617.76,-68.87\"/>\n",
       "</g>\n",
       "<!-- treatment_weight -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>treatment_weight</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1037.29\" cy=\"-129\" rx=\"72.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1037.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">treatment_weight</text>\n",
       "</g>\n",
       "<!-- treatment_weight&#45;&gt;Y -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>treatment_weight&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M1002.51,-113.15C973.58,-100.81 932.75,-83.38 904.77,-71.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"906.02,-68.17 895.45,-67.46 903.28,-74.6 906.02,-68.17\"/>\n",
       "</g>\n",
       "<!-- treatment_weight&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>treatment_weight&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M984.47,-116.45C974.79,-114.5 964.75,-112.6 955.29,-111 844.14,-92.21 714.83,-76.28 633.03,-66.93\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"633.38,-63.44 623.05,-65.79 632.59,-70.4 633.38,-63.44\"/>\n",
       "</g>\n",
       "<!-- cate_intercept -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>cate_intercept</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"551.29\" cy=\"-129\" rx=\"59.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"551.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">cate_intercept</text>\n",
       "</g>\n",
       "<!-- cate_intercept&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>cate_intercept&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M551.29,-110.7C551.29,-102.98 551.29,-93.71 551.29,-85.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"554.79,-85.1 551.29,-75.1 547.79,-85.1 554.79,-85.1\"/>\n",
       "</g>\n",
       "<!-- cate_weights -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>cate_weights</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"417.29\" cy=\"-129\" rx=\"56.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"417.29\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">cate_weights</text>\n",
       "</g>\n",
       "<!-- cate_weights&#45;&gt;Y_pseudo_residuals -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>cate_weights&#45;&gt;Y_pseudo_residuals</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M445.38,-113.33C464.64,-103.27 490.45,-89.78 511.77,-78.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"513.66,-81.61 520.91,-73.87 510.42,-75.4 513.66,-81.61\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x108f2ffd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Made up data for testing\n",
    "D1 = {\n",
    "    \"X\": torch.tensor([[1., 0., 3.], [0., 1., -1.], [1., 1., 4.]]),\n",
    "    \"A\": torch.tensor([1., 0., 1.]),\n",
    "    \"Y\": torch.tensor([4., 2., -1.])\n",
    "}\n",
    "\n",
    "D2 = {\n",
    "    \"X\": torch.tensor([[.5, .2, -3.], [3., 2., 1.], [0.6, 1., -1.]]),\n",
    "    \"A\": torch.tensor([0., 1., 1.]),\n",
    "    \"Y\": torch.tensor([2., -1, 2.])\n",
    "}\n",
    "\n",
    "model = LinearDRLearner(3)\n",
    "pyro.render_model(model, model_args=(D1, D2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 70.8813\n"
     ]
    }
   ],
   "source": [
    "module_one_vars = [\n",
    "    \"propensity_intercept\", \"propensity_weights\", \n",
    "    \"outcome_intercept\", \"outcome_weights\", \n",
    "    \"treatment_weight\", \"A\", \"Y\"\n",
    "]\n",
    "\n",
    "def run_svi_inference(\n",
    "    model: Callable, vi_family: Optional[Callable] = None, guide: Optional[Callable] = None, n_steps=100, verbose=True, **model_kwargs\n",
    "):\n",
    "    if guide is None:\n",
    "        guide = vi_family(model)\n",
    "    adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
    "    # Do gradient steps\n",
    "    pyro.clear_param_store()\n",
    "    for step in range(1, n_steps + 1):\n",
    "        loss = svi.step(**model_kwargs)\n",
    "        if (step % 1000 == 0) or (step == 1) & verbose:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step, loss))\n",
    "\n",
    "    return guide\n",
    "\n",
    "def make_cut_model_single(p, module_one_vars):\n",
    "    model =  LinearDRLearner(p)\n",
    "    def cut_model_single(*args, **kwargs):\n",
    "        with IndexPlatesMessenger(), IndexCutModule(module_one_vars):\n",
    "            model(*args, **kwargs)\n",
    "    return cut_model_single\n",
    "\n",
    "p = 3\n",
    "cut_model_single = make_cut_model_single(p, module_one_vars)\n",
    "cut_guide_single = run_svi_inference(cut_model_single, vi_family=AutoNormal, n_steps=1, D1=D1, D2=D2)\n",
    "cut_samples_single = Predictive(cut_model_single, guide=cut_guide_single, num_samples=1)(D1=D1, D2=D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data as in Figure 4b but use Gaussian responses instead of Bernoulli\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        N = X.shape[0]\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        with pyro.plate(\"obs\", N):\n",
    "            A = pyro.sample(\"A\", dist.Bernoulli(logits=X @ propensity_weights))\n",
    "            Y = pyro.sample(\"Y\", dist.Normal(X @ outcome_weights, 1.))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, p: int, alpha: int, beta: int):\n",
    "        super().__init__(p)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "\n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(4 * self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha:] = 0.\n",
    "        return propensity_weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(4 * self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 10\n",
    "beta = 10\n",
    "N_train = 1000\n",
    "N_test = 500\n",
    "benchmark_model = BenchmarkLinearModel(p, alpha, beta)\n",
    "X_train = dist.Normal(0., 1.).expand((N_train, p)).to_event(1).sample()\n",
    "X_test = dist.Normal(0., 1.).expand((N_test, p)).to_event(1).sample()\n",
    "\n",
    "with pyro.poutine.trace() as training_data:\n",
    "    benchmark_model(X_train)\n",
    "\n",
    "with pyro.poutine.trace() as testing_data:\n",
    "    benchmark_model(X_test)\n",
    "\n",
    "A_train = training_data.trace.nodes[\"A\"][\"value\"]\n",
    "Y_train = training_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_train = {\"X\": X_train, \"A\": A_train, \"Y\": Y_train}\n",
    "\n",
    "A_test = testing_data.trace.nodes[\"A\"][\"value\"]\n",
    "Y_test = testing_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_test = {\"X\": X_test, \"A\": A_test, \"Y\": Y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 6780.2487\n",
      "[iteration 1000] loss: 2511.0466\n",
      "[iteration 2000] loss: 2497.8009\n",
      "[iteration 3000] loss: 2442.7480\n",
      "[iteration 4000] loss: 2543.6138\n",
      "[iteration 5000] loss: 2546.7113\n"
     ]
    }
   ],
   "source": [
    "plug_in_model = HighDimLinearModel(p)\n",
    "plug_in_model_conditioned = pyro.poutine.condition(plug_in_model, data=D_train)\n",
    "plug_in_guide = run_svi_inference(plug_in_model_conditioned, vi_family=AutoNormal, n_steps=5000, X=X_train)\n",
    "plug_in_samples_test = Predictive(plug_in_model_conditioned, guide=plug_in_guide, num_samples=1000)(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/basis/lib/python3.10/site-packages/pyro/util.py:365: UserWarning: Found plate statements in guide but not model: {'__index_plate_____cut_plate'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 1438531.0016\n",
      "[iteration 1000] loss: 16667.6311\n",
      "[iteration 2000] loss: 81659.1838\n",
      "[iteration 3000] loss: 23661.5204\n",
      "[iteration 4000] loss: 15388.4630\n",
      "[iteration 5000] loss: 22804.4260\n"
     ]
    }
   ],
   "source": [
    "D1 = {\"X\": X_train[:N_train//2], \"A\": A_train[:N_train//2], \"Y\": Y_train[:N_train//2]}\n",
    "D2 = {\"X\": X_train[N_train//2:], \"A\": A_train[N_train//2:], \"Y\": Y_train[N_train//2:]}\n",
    "\n",
    "cut_model_single = make_cut_model_single(p, module_one_vars)\n",
    "cut_guide_single = run_svi_inference(cut_model_single, vi_family=AutoNormal, n_steps=5000, D1=D1, D2=D2)\n",
    "cut_samples_single = Predictive(cut_model_single, guide=cut_guide_single, num_samples=1000)(D1=D1, D2=D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(704.3757)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cut_guide_single.median()['cate_weights'][1] ** 2).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
