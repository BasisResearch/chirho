{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Optional, Tuple, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from chirho.indexed.handlers import IndexPlatesMessenger\n",
    "from chirho.observational.handlers.soft_conditioning import IndexCutModule\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class AbstractDRLearner(pyro.nn.PyroModule):\n",
    "    \"\"\"\n",
    "    AbstractDRLearner is a base class for all DR learners. It is based on the following paper:\n",
    "    https://arxiv.org/abs/2004.14497.\n",
    "    \"\"\"\n",
    "    def __init__(self, p: int, link_fn: Callable = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def _get_module_param(self, param, module_ix):\n",
    "        if len(param.shape) > 1:\n",
    "            return param[module_ix].squeeze()\n",
    "        return param\n",
    "    \n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def pseudo_outcome(\n",
    "        self, D: Dict, propensity_model: Callable, outcome_model: Callable, module_ix: int\n",
    "    ) -> torch.Tensor:\n",
    "        X, A, Y = D[\"X\"], D[\"A\"], D[\"Y\"]\n",
    "        N = X.shape[0]\n",
    "        propensity_scores = propensity_model(X, module_ix)\n",
    "        probs = torch.special.expit(propensity_scores)\n",
    "        outcome_0 = outcome_model(X, torch.zeros(N), module_ix)\n",
    "        outcome_1 = outcome_model(X, torch.ones(N), module_ix)\n",
    "        outcome_A = outcome_model(X, A, module_ix)\n",
    "        traditional_term = outcome_1 - outcome_0  # vanilla cate estimate\n",
    "        correction_term = (Y - outcome_A) * (A - probs) / (probs * (1 - probs))  # double robustness correction\n",
    "        return traditional_term + correction_term\n",
    "    \n",
    "    def forward(self, D1: Dict, D2: Dict, **kwargs):\n",
    "        propensity_score = self.sample_propensity_model(**kwargs)\n",
    "        outcome_model = self.sample_outcome_model(**kwargs)\n",
    "        cate_model = self.sample_cate(**kwargs)\n",
    "\n",
    "        # Treatment conditioning\n",
    "        X1, A1, Y1 = D1['X'], D1['A'], D1['Y']\n",
    "        with pyro.plate(\"treatment\", X1.shape[0]):\n",
    "            pyro.sample(\"A\", dist.Bernoulli(logits=propensity_score(X1, 0)), obs=A1)\n",
    "    \n",
    "        # Outcome conditioning\n",
    "        with pyro.plate(\"outcome\", X1.shape[0]):\n",
    "            pyro.sample(\"Y\", self.link_fn(outcome_model(X1, A1, 0)), obs=Y1)\n",
    "        \n",
    "        # pseudo outcome conditioning\n",
    "        X2 = D2['X']\n",
    "        Y_pseudo = self.pseudo_outcome(D2, propensity_score, outcome_model, 1)\n",
    "        with pyro.plate(\"pseudo_outcome\", X2.shape[0]):\n",
    "            # Hack to get correct conditional dependence structure rendered\n",
    "            # pyro.sample(\"Y_pseudo_residuals\", dist.Normal(Y_pseudo - cate_model(X2, 1), self.pseudo_noise_scale), obs=torch.zeros(X2.shape[0]))\n",
    "            pyro.sample(\"Y_pseudo_residuals\", self.link_fn(cate_model(X2, 1)), obs=Y_pseudo)\n",
    "\n",
    "\n",
    "class LinearDRLearner(AbstractDRLearner):\n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"propensity_intercept\", dist.Normal(0., 1./math.sqrt(self.p)))\n",
    "        weights = pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"outcome_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"outcome_weights\", dist.Normal(0., 1.).expand((self.p, )).to_event(1))\n",
    "        tau = pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "        return lambda x, a, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix) + a * self._get_module_param(tau, module_ix)\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        intercept = pyro.sample(\"cate_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"cate_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)\n",
    "    \n",
    "\n",
    "class ATEDRLearner(LinearDRLearner):\n",
    "    def sample_cate(self) -> Callable:\n",
    "        intercept = pyro.sample(\"cate_intercept\", dist.Normal(0., 1.))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_one_vars = [\n",
    "    \"propensity_intercept\", \"propensity_weights\", \n",
    "    \"outcome_intercept\", \"outcome_weights\", \n",
    "    \"treatment_weight\", \"A\", \"Y\"\n",
    "]\n",
    "\n",
    "def run_svi_inference(\n",
    "    model: Callable, vi_family: Optional[Callable] = None, guide: Optional[Callable] = None, n_steps=100, verbose=True, **model_kwargs\n",
    "):\n",
    "    if guide is None:\n",
    "        guide = vi_family(model)\n",
    "    adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "    svi = SVI(model, guide, adam, loss=Trace_ELBO())\n",
    "    # Do gradient steps\n",
    "    pyro.clear_param_store()\n",
    "    for step in range(1, n_steps + 1):\n",
    "        loss = svi.step(**model_kwargs)\n",
    "        if (step % 1000 == 0) or (step == 1) & verbose:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step, loss))\n",
    "\n",
    "    return guide\n",
    "\n",
    "def make_cut_model_single(p, module_one_vars, estimator=LinearDRLearner):\n",
    "    model =  estimator(p)\n",
    "    def cut_model_single(*args, **kwargs):\n",
    "        with IndexPlatesMessenger(), IndexCutModule(module_one_vars):\n",
    "            model(*args, **kwargs)\n",
    "    return cut_model_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data as in Figure 4b of https://arxiv.org/abs/2004.14497\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int, link_fn: Callable = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        N = X.shape[0]\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        with pyro.plate(\"obs\", N):\n",
    "            pyro.sample(\"Y\", self.link_fn(X @ outcome_weights + A * tau))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, p: int, link_fn: Callable, alpha: int, beta: int):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 200\n",
    "N_test = 500\n",
    "benchmark_model = BenchmarkLinearModel(p, bernoulli_link, alpha, beta)\n",
    "true_propensity_weights = 1 / math.sqrt(4 * alpha) * torch.ones(p)\n",
    "true_propensity_weights[alpha:] = 0.\n",
    "X_train = dist.Normal(0., 1.).expand((N_train, p)).to_event(1).sample()\n",
    "A_train = dist.Bernoulli(logits=X_train @ true_propensity_weights).sample()\n",
    "X_test = dist.Normal(0., 1.).expand((N_test, p)).to_event(1).sample()\n",
    "A_test = dist.Bernoulli(logits=X_test @ true_propensity_weights).sample()\n",
    "\n",
    "with pyro.poutine.trace() as training_data:\n",
    "    benchmark_model(X_train, A_train)\n",
    "\n",
    "with pyro.poutine.trace() as testing_data:\n",
    "    benchmark_model(X_test, A_test)\n",
    "\n",
    "Y_train = training_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_train = {\"X\": X_train, \"A\": A_train, \"Y\": Y_train}\n",
    "\n",
    "Y_test = testing_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_test = {\"X\": X_test, \"A\": A_test, \"Y\": Y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 918.1769\n",
      "[iteration 1000] loss: 234.1480\n",
      "[iteration 2000] loss: 207.4526\n",
      "[iteration 3000] loss: 223.0624\n",
      "[iteration 4000] loss: 233.1212\n",
      "[iteration 5000] loss: 220.0307\n"
     ]
    }
   ],
   "source": [
    "plug_in_model = HighDimLinearModel(p, link_fn=bernoulli_link)\n",
    "plug_in_model_conditioned = pyro.poutine.condition(plug_in_model, data=D_train)\n",
    "plug_in_guide = run_svi_inference(plug_in_model_conditioned, vi_family=AutoNormal, n_steps=5000, X=X_train, A=A_train)\n",
    "plug_in_samples_test = Predictive(plug_in_model_conditioned, guide=plug_in_guide, num_samples=1000)(X=X_test, A=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 3170179.0189\n",
      "[iteration 1000] loss: 167.1916\n",
      "[iteration 2000] loss: -1085.0101\n",
      "[iteration 3000] loss: -1124.6604\n",
      "[iteration 4000] loss: -2508.5250\n",
      "[iteration 5000] loss: 1232.6446\n"
     ]
    }
   ],
   "source": [
    "D1 = {\"X\": X_train[:N_train//2], \"A\": A_train[:N_train//2], \"Y\": Y_train[:N_train//2]}\n",
    "D2 = {\"X\": X_train[N_train//2:], \"A\": A_train[N_train//2:], \"Y\": Y_train[N_train//2:]}\n",
    "\n",
    "cut_model_single = make_cut_model_single(p, module_one_vars)\n",
    "cut_guide_single = run_svi_inference(cut_model_single, vi_family=AutoNormal, n_steps=5000, D1=D1, D2=D2)\n",
    "cut_samples_single = Predictive(cut_model_single, guide=cut_guide_single, num_samples=1000)(D1=D1, D2=D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 2205034.1592\n",
      "[iteration 1000] loss: 5550.8041\n",
      "[iteration 2000] loss: -805.7264\n",
      "[iteration 3000] loss: 2321.4295\n",
      "[iteration 4000] loss: -334.3918\n",
      "[iteration 5000] loss: -2048.3615\n"
     ]
    }
   ],
   "source": [
    "cut_ate_model_single = make_cut_model_single(p, module_one_vars, estimator=ATEDRLearner)\n",
    "cut_ate_guide_single = run_svi_inference(cut_ate_model_single, vi_family=AutoNormal, n_steps=5000, D1=D1, D2=D2)\n",
    "cut_ate_samples_single = Predictive(cut_ate_model_single, guide=cut_ate_guide_single, num_samples=1000)(D1=D1, D2=D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6366)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cut_guide_single.median()['cate_weights'][1] ** 2).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.1711]]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_guide_single.median()['cate_intercept'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.2055]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut_ate_guide_single.median()['cate_intercept'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2651)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plug_in_guide.median()['treatment_weight']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
