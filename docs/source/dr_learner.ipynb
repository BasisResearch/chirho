{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from chirho.indexed.handlers import IndexPlatesMessenger\n",
    "from chirho.observational.handlers.soft_conditioning import IndexCutModule\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly robust estimation with Chirho\n",
    "\n",
    "In this notebook, we implement a Bayesian analogue of the DR-Learner estimator in Kennedy (2022). The DR-Learner estimator is a doubly robust estimator for the conditional average treatment effect (CATE). It works by regressing a \"psuedo outcome\" on treatment, where the \"psuedo outcome\" is contructed by approximating the outcome and propensity score functions. The DR-Learner estimator is doubly robust in the sense that it is consistent if either the outcome or propensity score models are correctly specified. Moreoever, as long as the outcome and propensity score models can be estimated at $O(N^{-1/4})$ rates, the DR-Learner estimator can estimate CATE at the *parametric* $O(N^{-1/2})$ fast rate.\n",
    "\n",
    "Due to the two-stage nature of the DR-Learner estimator, we use Chirho's `IndexCutModule` to naturally construct a Bayesian analogue of a two-stage estimator using cut posteriors. Specifically, we use the single-stage variational inference procedure proposed in ADD REF to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class AbstractDRLearner(pyro.nn.PyroModule):\n",
    "    \"\"\"\n",
    "    AbstractDRLearner is a base class for all DR learners. It is based on the following paper:\n",
    "    https://arxiv.org/abs/2004.14497.\n",
    "    \"\"\"\n",
    "    def __init__(self, p: int, link_fn: Callable = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def _get_module_param(self, param, module_ix):\n",
    "        if len(param.shape) > 1:\n",
    "            return param[module_ix].squeeze()\n",
    "        return param\n",
    "    \n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def pseudo_outcome(\n",
    "        self, D: Dict, propensity_model: Callable, outcome_model: Callable, module_ix: int\n",
    "    ) -> torch.Tensor:\n",
    "        X, A, Y = D[\"X\"], D[\"A\"], D[\"Y\"]\n",
    "        N = X.shape[0]\n",
    "        propensity_scores = propensity_model(X, module_ix)\n",
    "        probs = torch.special.expit(propensity_scores)\n",
    "        outcome_0 = outcome_model(X, torch.zeros(N), module_ix)\n",
    "        outcome_1 = outcome_model(X, torch.ones(N), module_ix)\n",
    "        outcome_A = outcome_model(X, A, module_ix)\n",
    "        traditional_term = outcome_1 - outcome_0  # vanilla cate estimate\n",
    "        correction_term = (Y - outcome_A) * (A - probs) / (probs * (1 - probs))  # double robustness correction\n",
    "        return traditional_term + correction_term\n",
    "    \n",
    "    def forward(self, D1: Dict, D2: Dict, **kwargs):\n",
    "        propensity_score = self.sample_propensity_model(**kwargs)\n",
    "        outcome_model = self.sample_outcome_model(**kwargs)\n",
    "        cate_model = self.sample_cate(**kwargs)\n",
    "\n",
    "        # Treatment conditioning\n",
    "        X1, A1, Y1 = D1['X'], D1['A'], D1['Y']\n",
    "        with pyro.plate(\"treatment\", X1.shape[0]):\n",
    "            pyro.sample(\"A\", dist.Bernoulli(logits=propensity_score(X1, 0)), obs=A1)\n",
    "    \n",
    "        # Outcome conditioning\n",
    "        with pyro.plate(\"outcome\", X1.shape[0]):\n",
    "            pyro.sample(\"Y\", self.link_fn(outcome_model(X1, A1, 0)), obs=Y1)\n",
    "        \n",
    "        # pseudo outcome conditioning\n",
    "        X2 = D2['X']\n",
    "        Y_pseudo = self.pseudo_outcome(D2, propensity_score, outcome_model, 1)\n",
    "        with pyro.plate(\"pseudo_outcome\", X2.shape[0]):\n",
    "            # Hack to get correct conditional dependence structure rendered\n",
    "            # pyro.sample(\"Y_pseudo_residuals\", dist.Normal(Y_pseudo - cate_model(X2, 1), self.pseudo_noise_scale), obs=torch.zeros(X2.shape[0]))\n",
    "            pyro.sample(\"Y_pseudo_residuals\", self.link_fn(cate_model(X2, 1)), obs=Y_pseudo)\n",
    "\n",
    "\n",
    "class LinearDRLearner(AbstractDRLearner):\n",
    "    def sample_propensity_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"propensity_intercept\", dist.Normal(0., 1./math.sqrt(self.p)))\n",
    "        weights = pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)\n",
    "\n",
    "    def sample_outcome_model(self) -> Callable:\n",
    "        intercept = pyro.sample(\"outcome_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"outcome_weights\", dist.Normal(0., 1.).expand((self.p, )).to_event(1))\n",
    "        tau = pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "        return lambda x, a, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix) + a * self._get_module_param(tau, module_ix)\n",
    "\n",
    "    def sample_cate(self) -> Callable:\n",
    "        intercept = pyro.sample(\"cate_intercept\", dist.Normal(0., 1.))\n",
    "        weights = pyro.sample(\"cate_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix) + x @ self._get_module_param(weights, module_ix)\n",
    "    \n",
    "\n",
    "class ATEDRLearner(LinearDRLearner):\n",
    "    def sample_cate(self) -> Callable:\n",
    "        intercept = pyro.sample(\"cate_intercept\", dist.Normal(0., 1.))\n",
    "        return lambda x, module_ix: self._get_module_param(intercept, module_ix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synthetic data generation from a high-dimensional generalized linear model\n",
    "\n",
    "We use the classes below to generate synthetic data from a high-dimensional generalized linear model. Further, we will use this class to implement the standard outcome-regression approach to estimate CATE. That is, we regress $Y$ on $X$ and $T$ to obtain an estimate of $E[Y | X, A=1] - E[Y | X, A=0]$. This approach is called the \"plug-in\" approach in Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int, link_fn: Callable = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        N = X.shape[0]\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        with pyro.plate(\"obs\", N):\n",
    "            pyro.sample(\"Y\", self.link_fn(X @ outcome_weights + A * tau))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, p: int, link_fn: Callable, alpha: int, beta: int):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(0.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we generate synthetic data as in Figure 4b of Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 200\n",
    "N_test = 500\n",
    "benchmark_model = BenchmarkLinearModel(p, bernoulli_link, alpha, beta)\n",
    "true_propensity_weights = 1 / math.sqrt(4 * alpha) * torch.ones(p)\n",
    "true_propensity_weights[alpha:] = 0.\n",
    "X_train = dist.Normal(0., 1.).expand((N_train, p)).to_event(1).sample()\n",
    "A_train = dist.Bernoulli(logits=X_train @ true_propensity_weights).sample()\n",
    "X_test = dist.Normal(0., 1.).expand((N_test, p)).to_event(1).sample()\n",
    "A_test = dist.Bernoulli(logits=X_test @ true_propensity_weights).sample()\n",
    "\n",
    "with pyro.poutine.trace() as training_data:\n",
    "    benchmark_model(X_train, A_train)\n",
    "\n",
    "with pyro.poutine.trace() as testing_data:\n",
    "    benchmark_model(X_test, A_test)\n",
    "\n",
    "Y_train = training_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_train = {\"X\": X_train, \"A\": A_train, \"Y\": Y_train}\n",
    "\n",
    "Y_test = testing_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_test = {\"X\": X_test, \"A\": A_test, \"Y\": Y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to run SVI. (Generally, Pyro users like to have more control over the training process!)\n",
    "def run_svi_inference(model, n_steps=100, verbose=True, lr=.03, vi_family=AutoNormal, guide=None, **model_kwargs):\n",
    "    if guide is None:\n",
    "        guide = vi_family(model)\n",
    "    elbo = pyro.infer.Trace_ELBO()(model, guide)\n",
    "    # initialize parameters\n",
    "    elbo(**model_kwargs)\n",
    "    adam = torch.optim.Adam(elbo.parameters(), lr=lr)\n",
    "    # Do gradient steps\n",
    "    for step in range(1, n_steps + 1):\n",
    "        adam.zero_grad()\n",
    "        loss = elbo(**model_kwargs)\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        if (step % 1000 == 0) or (step == 1) & verbose:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (step, loss))\n",
    "    return guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 908.8467\n",
      "[iteration 1000] loss: 228.1193\n",
      "[iteration 2000] loss: 197.7543\n",
      "[iteration 3000] loss: 234.1455\n",
      "[iteration 4000] loss: 224.1709\n",
      "[iteration 5000] loss: 283.9059\n"
     ]
    }
   ],
   "source": [
    "plug_in_model = HighDimLinearModel(p, link_fn=bernoulli_link)\n",
    "plug_in_model_conditioned = pyro.poutine.condition(plug_in_model, data=D_train)\n",
    "plug_in_guide = run_svi_inference(plug_in_model_conditioned, vi_family=AutoNormal, n_steps=5000, X=X_train, A=A_train)\n",
    "plug_in_samples_test = Predictive(plug_in_model_conditioned, guide=plug_in_guide, num_samples=1000)(X=X_test, A=A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/basis/lib/python3.10/site-packages/pyro/util.py:365: UserWarning: Found plate statements in guide but not model: {'__index_plate_____cut_plate'}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 349285.8125\n",
      "[iteration 1000] loss: -1693.5898\n",
      "[iteration 2000] loss: 1343.1140\n",
      "[iteration 3000] loss: 3310.6719\n",
      "[iteration 4000] loss: -2768.5151\n",
      "[iteration 5000] loss: 5615.3228\n"
     ]
    }
   ],
   "source": [
    "D1 = {\"X\": X_train[:N_train//2], \"A\": A_train[:N_train//2], \"Y\": Y_train[:N_train//2]}\n",
    "D2 = {\"X\": X_train[N_train//2:], \"A\": A_train[N_train//2:], \"Y\": Y_train[N_train//2:]}\n",
    "\n",
    "module_one_vars = [\n",
    "    \"propensity_intercept\", \"propensity_weights\", \n",
    "    \"outcome_intercept\", \"outcome_weights\", \n",
    "    \"treatment_weight\", \"A\", \"Y\"\n",
    "]\n",
    "\n",
    "def make_cut_model_single(p, module_one_vars, estimator=LinearDRLearner):\n",
    "    model =  estimator(p)\n",
    "    def cut_model_single(*args, **kwargs):\n",
    "        with IndexPlatesMessenger(), IndexCutModule(module_one_vars):\n",
    "            model(*args, **kwargs)\n",
    "    return cut_model_single\n",
    "\n",
    "cut_model_single = make_cut_model_single(p, module_one_vars)\n",
    "cut_guide_single = run_svi_inference(cut_model_single, vi_family=AutoNormal, n_steps=5000, D1=D1, D2=D2)\n",
    "cut_samples_single = Predictive(cut_model_single, guide=cut_guide_single, num_samples=1000)(D1=D1, D2=D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 204065.4062\n",
      "[iteration 1000] loss: 1151.2887\n",
      "[iteration 2000] loss: 241.4312\n",
      "[iteration 3000] loss: 549.9827\n",
      "[iteration 4000] loss: 349.6018\n",
      "[iteration 5000] loss: -1269.1072\n"
     ]
    }
   ],
   "source": [
    "cut_ate_model_single = make_cut_model_single(p, module_one_vars, estimator=ATEDRLearner)\n",
    "cut_ate_guide_single = run_svi_inference(cut_ate_model_single, vi_family=AutoNormal, n_steps=5000, D1=D1, D2=D2)\n",
    "cut_ate_samples_single = Predictive(cut_ate_model_single, guide=cut_ate_guide_single, num_samples=1000)(D1=D1, D2=D2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at how well each method estimates average treatment effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True ATE: 0.0\n",
      "Estimated Treatment Effect (Plug-In): 0.204\n",
      "Estimated Treatment Effect (CATE DR-Learner): -0.073\n",
      "Estimated Treatment Effect (ATE DR-Learner): 0.029\n"
     ]
    }
   ],
   "source": [
    "print(\"True ATE: 0.0\")\n",
    "print(f\"Estimated Treatment Effect (Plug-In): {round(plug_in_guide.median()['treatment_weight'].item(), 3)}\")\n",
    "print(f\"Estimated Treatment Effect (CATE DR-Learner): {round(cut_guide_single.median()['cate_intercept'][1].item(), 3)}\")\n",
    "print(f\"Estimated Treatment Effect (ATE DR-Learner): {round(cut_ate_guide_single.median()['cate_intercept'][1].item(), 3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
