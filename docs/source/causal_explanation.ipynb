{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import contextlib\n",
    "import collections\n",
    "from typing import Callable, Iterable, TypeVar, Mapping, List, Dict\n",
    "\n",
    "import random\n",
    "\n",
    "import pyro\n",
    "import torch  # noqa: F401\n",
    "\n",
    "from chirho.counterfactual.handlers.selection import get_factual_indices\n",
    "from chirho.indexed.ops import IndexSet, cond, gather, indices_of, scatter\n",
    "\n",
    "S = TypeVar(\"S\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "import pyro\n",
    "import chirho\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from chirho.counterfactual.handlers.counterfactual import (MultiWorldCounterfactual,\n",
    "        Preemptions)\n",
    "from chirho.counterfactual.handlers.explanation import (\n",
    "    SearchForCause,\n",
    "    consequent_differs,\n",
    "    random_intervention,\n",
    "    undo_split,\n",
    "    uniform_proposal,\n",
    ")\n",
    "from chirho.counterfactual.ops import preempt, split\n",
    "from chirho.indexed.ops import IndexSet, gather, indices_of\n",
    "from chirho.observational.handlers.condition import Factors, condition\n",
    "from chirho.interventional.ops import Intervention, intervene\n",
    "from chirho.interventional.handlers import do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def ExplainCauses(\n",
    "    antecedents: Mapping[str, Intervention[T]]\n",
    "    | Mapping[str, pyro.distributions.constraints.Constraint],\n",
    "    witnesses: Mapping[str, Intervention[T]] | Iterable[str],\n",
    "    consequents: Mapping[str, Callable[[T], float | torch.Tensor]]\n",
    "    | Iterable[str],\n",
    "    *,\n",
    "    antecedent_bias: float = 0.0,\n",
    "    witness_bias: float = 0.0,\n",
    "    consequent_eps: float = -1e8,\n",
    "    antecedent_prefix: str = \"__antecedent_\",\n",
    "    witness_prefix: str = \"__witness_\",\n",
    "    consequent_prefix: str = \"__consequent_\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Effect handler for causal explanation.\n",
    "\n",
    "    :param antecedents: A mapping from antecedent names to interventions.\n",
    "    :param witnesses: A mapping from witness names to interventions.\n",
    "    :param consequents: A mapping from consequent names to factor functions.\n",
    "    \"\"\"\n",
    "    if isinstance(\n",
    "        next(iter(antecedents.values())),\n",
    "        pyro.distributions.constraints.Constraint,\n",
    "    ):\n",
    "        antecedents = {\n",
    "            a: random_intervention(s, name=f\"{antecedent_prefix}_proposal_{a}\")\n",
    "            for a, s in antecedents.items()\n",
    "        }\n",
    "\n",
    "    if not isinstance(witnesses, collections.abc.Mapping):\n",
    "        witnesses = {\n",
    "            w: undo_split(antecedents=list(antecedents.keys()))\n",
    "            for w in witnesses\n",
    "        }\n",
    "\n",
    "    if not isinstance(consequents, collections.abc.Mapping):\n",
    "        consequents = {\n",
    "            c: consequent_differs(\n",
    "                antecedents=list(antecedents.keys()), eps=consequent_eps\n",
    "            )\n",
    "            for c in consequents\n",
    "        }\n",
    "\n",
    "    if len(consequents) == 0:\n",
    "        raise ValueError(\"must have at least one consequent\")\n",
    "\n",
    "    if len(antecedents) == 0:\n",
    "        raise ValueError(\"must have at least one antecedent\")\n",
    "\n",
    "    if set(consequents.keys()) & set(antecedents.keys()):\n",
    "        raise ValueError(\n",
    "            \"consequents and possible antecedents must be disjoint\"\n",
    "        )\n",
    "\n",
    "    if set(consequents.keys()) & set(witnesses.keys()):\n",
    "        raise ValueError(\"consequents and possible witnesses must be disjoint\")\n",
    "\n",
    "    antecedent_handler = SearchForCause(\n",
    "        actions=antecedents, bias=antecedent_bias, prefix=antecedent_prefix\n",
    "    )\n",
    "    witness_handler = Preemptions(\n",
    "        actions=witnesses, bias=witness_bias, prefix=witness_prefix\n",
    "    )\n",
    "    consequent_handler = Factors(factors=consequents, prefix=consequent_prefix)\n",
    "\n",
    "    with antecedent_handler, witness_handler, consequent_handler:\n",
    "        with pyro.poutine.trace() as logging_tr:\n",
    "            yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_observed(value, antecedents, witnesses):\n",
    "    \n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "        \n",
    "    _indices = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(value, event_dim=0)\n",
    "        ]\n",
    "    _int_can = gather(\n",
    "    value, IndexSet(**{i: {0} for i in _indices}), event_dim=0,)\n",
    "    return _int_can\n",
    "\n",
    "def gather_intervened(value, antecedents, witnesses):\n",
    "    \n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "        \n",
    "        \n",
    "    _indices = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(value, event_dim=0)\n",
    "        ]\n",
    "    _int_can = gather(\n",
    "    value, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\n",
    "    return _int_can\n",
    "\n",
    "\n",
    "def get_table(trace, mwc, antecedents, witnesses, consequents):\n",
    "\n",
    "    values_table = {}\n",
    "    nodes = trace.trace.nodes\n",
    "    \n",
    "\n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "\n",
    "    with mwc:\n",
    "\n",
    "        for antecedent_str in antecedents_list:\n",
    "                \n",
    "            obs_ant = gather_observed(nodes[antecedent_str][\"value\"], antecedents_list, witnesses)\n",
    "            int_ant = gather_intervened(nodes[antecedent_str][\"value\"], antecedents_list, witnesses)\n",
    "\n",
    "            values_table[f\"{antecedent_str}_obs\"] = obs_ant.squeeze().tolist()\n",
    "            values_table[f\"{antecedent_str}_int\"] = int_ant.squeeze().tolist()\n",
    "            \n",
    "            apr_ant = nodes[f\"__antecedent_{antecedent_str}\"][\"value\"]\n",
    "            values_table[f\"apr_{antecedent_str}\"] = apr_ant.squeeze().tolist()\n",
    "            \n",
    "            values_table[f\"apr_{antecedent_str}_lp\"] = nodes[f\"__antecedent_{antecedent_str}\"][\"fn\"].log_prob(apr_ant)\n",
    "\n",
    "        if witnesses:\n",
    "            for candidate in witnesses:\n",
    "                obs_candidate = gather_observed(nodes[candidate][\"value\"], antecedents_list, witnesses)\n",
    "                int_candidate = gather_intervened(nodes[candidate][\"value\"], antecedents_list, witnesses)\n",
    "                values_table[f\"{candidate}_obs\"] = obs_candidate.squeeze().tolist()\n",
    "                values_table[f\"{candidate}_int\"] = int_candidate.squeeze().tolist()\n",
    "\n",
    "                wpr_con = nodes[f\"__witness_{candidate}\"][\"value\"]\n",
    "                values_table[f\"wpr_{candidate}\"] = wpr_con.squeeze().tolist()\n",
    "            \n",
    "\n",
    "\n",
    "        for consequent in consequents:\n",
    "            \n",
    "            obs_consequent = gather_observed(nodes[consequent][\"value\"], antecedents_list, witnesses)\n",
    "            int_consequent = gather_intervened(nodes[consequent][\"value\"], antecedents_list, witnesses)\n",
    "            con_lp = nodes[f\"__consequent_{consequent}\"]['fn'].log_prob(torch.tensor(1)) #TODO: this feels like a hack\n",
    "            _indices_lp = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(con_lp)]\n",
    "            int_con_lp = gather(con_lp, IndexSet(**{i: {1} for i in _indices_lp}), event_dim=0,)      \n",
    "\n",
    "\n",
    "            values_table[f\"{consequent}_obs\"] = obs_consequent.squeeze().tolist()   \n",
    "            values_table[f\"{consequent}_int\"] = int_consequent.squeeze().tolist()\n",
    "            values_table[f\"{consequent}_lp\"] = int_con_lp.squeeze().tolist()   \n",
    "\n",
    "    values_df = pd.DataFrame(values_table)\n",
    "\n",
    "    values_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    summands = [col for col in values_df.columns if col.endswith('lp')]\n",
    "    values_df[\"sum_log_prob\"] =  values_df[summands].sum(axis = 1)\n",
    "    values_df.sort_values(by = \"sum_log_prob\", ascending = False, inplace = True)\n",
    "\n",
    "    return values_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reduces the actual causality check to checking a property of the resulting sums of log probabilities\n",
    "# for the antecedent preemption and the consequent differs nodes\n",
    "\n",
    "def ac_check(trace, mwc, antecedents, witnesses, consequents):\n",
    "\n",
    "     table = get_table(trace, mwc, antecedents, witnesses, consequents)\n",
    "     \n",
    "     if (list(table['sum_log_prob'])[0]<= -1e8):\n",
    "          print(\"No resulting difference to the consequent in the sample.\")\n",
    "          return\n",
    "     \n",
    "     winners = table[table['sum_log_prob'] == table['sum_log_prob'].max()]\n",
    "     \n",
    "\n",
    "     ac_flags = []\n",
    "     for index, row in winners.iterrows():\n",
    "          active_antecedents = []\n",
    "          for antecedent in antecedents:\n",
    "               if row[f\"apr_{antecedent}\"] == 0:\n",
    "                    active_antecedents.append(antecedent)\n",
    "\n",
    "          ac_flags.append(set(active_antecedents) == set(antecedents))\n",
    "\n",
    "     if not any(ac_flags):\n",
    "          print(\"The antecedent set is not minimal.\")\n",
    "     else:\n",
    "          print(\"The antecedent set is an actual cause.\")\n",
    "\n",
    "     return any(ac_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_sets(set_list):\n",
    "    inclusion_minimal = []\n",
    "    for s1 in set_list:\n",
    "        is_minimal = True\n",
    "        for s2 in set_list:\n",
    "            if s1 != s2 and s2.issubset(s1):\n",
    "                is_minimal = False\n",
    "        if is_minimal:\n",
    "            inclusion_minimal.append(s1)\n",
    "    return inclusion_minimal\n",
    "\n",
    "\n",
    "def tensorize_dictionary(dictionary):\n",
    "    return {k: torch.as_tensor(v) for k, v in dictionary.items()}\n",
    "\n",
    "def boolean_constraints_from_list(list):\n",
    "    return {k: pyro.distributions.constraints.boolean for k in list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sufficient_causality_checkA(table, considered_antecedent_setting, causal_candidates):\n",
    "\n",
    "    sufficiency_table = table.copy()\n",
    "        \n",
    "    for antecedent_str in considered_antecedent_setting.keys():\n",
    "        sufficiency_table = sufficiency_table[sufficiency_table[f\"{antecedent_str}_obs\"] == considered_antecedent_setting[antecedent_str]]\n",
    "    \n",
    "    sufficiency_table = sufficiency_table[sufficiency_table['sum_log_prob'] > -1e8]\n",
    "    \n",
    "    # we need to check set inclusion minimality of cause sets\n",
    "    # as there might be inclusion minimal sets that are not log-prob-sum minimal\n",
    "    # just because they have more elements\n",
    "    candidate_sets = []\n",
    "    for i, row in sufficiency_table.iterrows():\n",
    "        candidate_set = set()\n",
    "        for node in causal_candidates:\n",
    "            if row[f\"{node}_int\"] != row[f\"{node}_obs\"]:\n",
    "                candidate_set.add(node)\n",
    "        candidate_sets.append(candidate_set)\n",
    "        \n",
    "        actual_cause_sets = minimal_sets(candidate_sets)\n",
    "        \n",
    "        frozensets = [frozenset(s) for s in actual_cause_sets]\n",
    "        unique_actual_cause_sets = [set(f) for f in set(frozensets)]\n",
    "        \n",
    "    sufficiency_flag = any(key in ac_set for key in considered_antecedent_setting.keys() for ac_set in actual_cause_sets)\n",
    "\n",
    "    return  unique_actual_cause_sets, sufficiency_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest fire example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match_dropped': tensor(0.),\n",
       " 'lightning': tensor(0.),\n",
       " 'forest_fire': tensor(0.)}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ff_conjunctive():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "    match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                       u_match_dropped, event_dim=0)\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "    forest_fire = pyro.deterministic(\"forest_fire\", match_dropped.bool() & lightning.bool(),\n",
    "                                     event_dim=0).float()\n",
    "\n",
    "    return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n",
    "    \n",
    "def ff_disjunctive():\n",
    "        u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "        u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "        match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                        u_match_dropped.bool(), event_dim=0)\n",
    "        lightning = pyro.deterministic(\"lightning\", u_lightning.bool(), event_dim=0)\n",
    "        forest_fire = pyro.deterministic(\"forest_fire\", (match_dropped.bool() | lightning.bool()).bool(), event_dim=0).bool()\n",
    "\n",
    "        return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n",
    "\n",
    "\n",
    "ff_conjunctive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7.1.2. from the book\n",
    "\n",
    "# all contexts available, no settings excluded by what \n",
    "# the agent knows about the world\n",
    "# in the conjunctive model, the joint nodes are an explanation of forest fire\n",
    "\n",
    "# these are explanation candidates\n",
    "antecedents = {\"match_dropped\": 1.0, \"lightning\": 1.0}\n",
    "consequents = [\"forest_fire\"]\n",
    "consequents_observed = tensorize_dictionary({\"forest_fire\": 1.0})\n",
    "all_nodes = [\"match_dropped\", \"lightning\", \"forest_fire\"]\n",
    "causal_candidates = [node for node in all_nodes if node not in consequents]\n",
    "causal_candidate_constraints = boolean_constraints_from_list(causal_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with MultiWorldCounterfactual() as mwc:\n",
    "    with ExplainCauses(antecedents = causal_candidate_constraints, \n",
    "                      witnesses = causal_candidates, consequents = consequents):\n",
    "            with condition(data = {\"forest_fire\": torch.tensor(True)}):\n",
    "                with pyro.plate(\"sample\", 100):\n",
    "                    with pyro.poutine.trace() as tr:\n",
    "                        ff_conjunctive()\n",
    "\n",
    "ff_conjunctive_table =  get_table(tr, mwc, causal_candidates, causal_candidates, consequents)\n",
    "\n",
    "condition_e1a = sufficient_causality_checkA(ff_conjunctive_table, antecedents, causal_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# now condition 1b \n",
    "# P(consequents | do(antecedents)) = 1\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:\n",
    "      #  with pyro.plate(\"sample\", 100):\n",
    "    with do( actions = antecedents):\n",
    "        with pyro.plate(\"samples\", 100):\n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    ff_conjunctive()\n",
    "\n",
    "outcome_df = pd.DataFrame()\n",
    "\n",
    "with mwc:\n",
    "    for consequent in consequents:\n",
    "        \n",
    "        value = tr.trace.nodes[consequent][\"value\"]\n",
    "        _indices = [\n",
    "                i for i in list(antecedents.keys()) if i in indices_of(value, event_dim=0)\n",
    "            ]\n",
    "        _int_con = gather(\n",
    "        value, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\n",
    "        outcome_df[consequent] = _int_con.squeeze().tolist()\n",
    "    \n",
    "condition_e1b = ((outcome_df) == True).all().all()\n",
    "\n",
    "print(condition_e1a and condition_e1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# condition_3: P(antecedents & consequent) > 0\n",
    "# condition_4: P(antecedents < 1) \n",
    "    \n",
    "with pyro.plate(\"samples\", 100):\n",
    "        with pyro.poutine.trace() as tr:\n",
    "            ff_conjunctive()\n",
    "            \n",
    "\n",
    "reqs = {**antecedents, **consequents_observed}\n",
    "\n",
    "reqs_outcome = pd.DataFrame()\n",
    "\n",
    "for req in reqs:\n",
    "    reqs_outcome[req] = tr.trace.nodes[req][\"value\"]\n",
    "       \n",
    "condition_e3 = (reqs_outcome == 1.0).all(axis=1).any()\n",
    "condition_e4 = (reqs_outcome.iloc[:, :2] == 0.0).any(axis=1).any()\n",
    "\n",
    "print(condition_e3)\n",
    "print(condition_e4)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
