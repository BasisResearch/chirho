{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import contextlib\n",
    "import collections\n",
    "from typing import Callable, Iterable, TypeVar, Mapping, List, Dict\n",
    "\n",
    "\n",
    "from itertools import chain, combinations\n",
    "\n",
    "import random\n",
    "\n",
    "import pyro\n",
    "import torch  # noqa: F401\n",
    "\n",
    "from chirho.counterfactual.handlers.selection import get_factual_indices\n",
    "from chirho.indexed.ops import IndexSet, cond, gather, indices_of, scatter\n",
    "\n",
    "S = TypeVar(\"S\")\n",
    "T = TypeVar(\"T\")\n",
    "\n",
    "import pyro\n",
    "import chirho\n",
    "import pyro.distributions as dist\n",
    "import pyro.infer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from chirho.counterfactual.handlers.counterfactual import (MultiWorldCounterfactual,\n",
    "        Preemptions)\n",
    "from chirho.counterfactual.handlers.explanation import (\n",
    "    SearchForCause,\n",
    "    consequent_differs,\n",
    "    random_intervention,\n",
    "    undo_split,\n",
    "    uniform_proposal,\n",
    ")\n",
    "from chirho.counterfactual.ops import preempt, split\n",
    "from chirho.indexed.ops import IndexSet, gather, indices_of\n",
    "from chirho.observational.handlers.condition import Factors, condition\n",
    "from chirho.interventional.ops import Intervention, intervene\n",
    "from chirho.interventional.handlers import do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def ExplainCauses(\n",
    "    antecedents: Mapping[str, Intervention[T]]\n",
    "    | Mapping[str, pyro.distributions.constraints.Constraint],\n",
    "    witnesses: Mapping[str, Intervention[T]] | Iterable[str],\n",
    "    consequents: Mapping[str, Callable[[T], float | torch.Tensor]]\n",
    "    | Iterable[str],\n",
    "    *,\n",
    "    antecedent_bias: float = 0.0,\n",
    "    witness_bias: float = 0.0,\n",
    "    consequent_eps: float = -1e8,\n",
    "    antecedent_prefix: str = \"__antecedent_\",\n",
    "    witness_prefix: str = \"__witness_\",\n",
    "    consequent_prefix: str = \"__consequent_\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Effect handler for causal explanation.\n",
    "\n",
    "    :param antecedents: A mapping from antecedent names to interventions.\n",
    "    :param witnesses: A mapping from witness names to interventions.\n",
    "    :param consequents: A mapping from consequent names to factor functions.\n",
    "    \"\"\"\n",
    "    if isinstance(\n",
    "        next(iter(antecedents.values())),\n",
    "        pyro.distributions.constraints.Constraint,\n",
    "    ):\n",
    "        antecedents = {\n",
    "            a: random_intervention(s, name=f\"{antecedent_prefix}_proposal_{a}\")\n",
    "            for a, s in antecedents.items()\n",
    "        }\n",
    "\n",
    "    if not isinstance(witnesses, collections.abc.Mapping):\n",
    "        witnesses = {\n",
    "            w: undo_split(antecedents=list(antecedents.keys()))\n",
    "            for w in witnesses\n",
    "        }\n",
    "\n",
    "    if not isinstance(consequents, collections.abc.Mapping):\n",
    "        consequents = {\n",
    "            c: consequent_differs(\n",
    "                antecedents=list(antecedents.keys()), eps=consequent_eps\n",
    "            )\n",
    "            for c in consequents\n",
    "        }\n",
    "\n",
    "    if len(consequents) == 0:\n",
    "        raise ValueError(\"must have at least one consequent\")\n",
    "\n",
    "    if len(antecedents) == 0:\n",
    "        raise ValueError(\"must have at least one antecedent\")\n",
    "\n",
    "    if set(consequents.keys()) & set(antecedents.keys()):\n",
    "        raise ValueError(\n",
    "            \"consequents and possible antecedents must be disjoint\"\n",
    "        )\n",
    "\n",
    "    if set(consequents.keys()) & set(witnesses.keys()):\n",
    "        raise ValueError(\"consequents and possible witnesses must be disjoint\")\n",
    "\n",
    "    antecedent_handler = SearchForCause(\n",
    "        actions=antecedents, bias=antecedent_bias, prefix=antecedent_prefix\n",
    "    )\n",
    "    witness_handler = Preemptions(\n",
    "        actions=witnesses, bias=witness_bias, prefix=witness_prefix\n",
    "    )\n",
    "    consequent_handler = Factors(factors=consequents, prefix=consequent_prefix)\n",
    "\n",
    "    with antecedent_handler, witness_handler, consequent_handler:\n",
    "            yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_observed(value, antecedents, witnesses):\n",
    "    \n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "        \n",
    "    _indices = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(value, event_dim=0)\n",
    "        ]\n",
    "    _int_can = gather(\n",
    "    value, IndexSet(**{i: {0} for i in _indices}), event_dim=0,)\n",
    "    return _int_can\n",
    "\n",
    "def gather_intervened(value, antecedents, witnesses):\n",
    "    \n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "        \n",
    "        \n",
    "    _indices = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(value, event_dim=0)\n",
    "        ]\n",
    "    _int_can = gather(\n",
    "    value, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\n",
    "    return _int_can\n",
    "\n",
    "\n",
    "def get_table(trace, mwc, antecedents, witnesses, consequents):\n",
    "\n",
    "    values_table = {}\n",
    "    nodes = trace.trace.nodes\n",
    "    \n",
    "\n",
    "    if isinstance(antecedents, dict):\n",
    "        antecedents_list = list(antecedents.keys())\n",
    "    else:\n",
    "        antecedents_list = antecedents\n",
    "\n",
    "    with mwc:\n",
    "\n",
    "        for antecedent_str in antecedents_list:\n",
    "                \n",
    "            obs_ant = gather_observed(nodes[antecedent_str][\"value\"], antecedents_list, witnesses)\n",
    "            int_ant = gather_intervened(nodes[antecedent_str][\"value\"], antecedents_list, witnesses)\n",
    "\n",
    "            values_table[f\"{antecedent_str}_obs\"] = obs_ant.squeeze().tolist()\n",
    "            values_table[f\"{antecedent_str}_int\"] = int_ant.squeeze().tolist()\n",
    "            \n",
    "            apr_ant = nodes[f\"__antecedent_{antecedent_str}\"][\"value\"]\n",
    "            values_table[f\"apr_{antecedent_str}\"] = apr_ant.squeeze().tolist()\n",
    "            \n",
    "            values_table[f\"apr_{antecedent_str}_lp\"] = nodes[f\"__antecedent_{antecedent_str}\"][\"fn\"].log_prob(apr_ant)\n",
    "\n",
    "        if witnesses:\n",
    "            for candidate in witnesses:\n",
    "                obs_candidate = gather_observed(nodes[candidate][\"value\"], antecedents_list, witnesses)\n",
    "                int_candidate = gather_intervened(nodes[candidate][\"value\"], antecedents_list, witnesses)\n",
    "                values_table[f\"{candidate}_obs\"] = obs_candidate.squeeze().tolist()\n",
    "                values_table[f\"{candidate}_int\"] = int_candidate.squeeze().tolist()\n",
    "\n",
    "                wpr_con = nodes[f\"__witness_{candidate}\"][\"value\"]\n",
    "                values_table[f\"wpr_{candidate}\"] = wpr_con.squeeze().tolist()\n",
    "            \n",
    "\n",
    "\n",
    "        for consequent in consequents:\n",
    "            \n",
    "            obs_consequent = gather_observed(nodes[consequent][\"value\"], antecedents_list, witnesses)\n",
    "            int_consequent = gather_intervened(nodes[consequent][\"value\"], antecedents_list, witnesses)\n",
    "            con_lp = nodes[f\"__consequent_{consequent}\"]['fn'].log_prob(torch.tensor(1)) #TODO: this feels like a hack\n",
    "            _indices_lp = [\n",
    "            i for i in antecedents_list + witnesses if i in indices_of(con_lp)]\n",
    "            int_con_lp = gather(con_lp, IndexSet(**{i: {1} for i in _indices_lp}), event_dim=0,)      \n",
    "\n",
    "\n",
    "            values_table[f\"{consequent}_obs\"] = obs_consequent.squeeze().tolist()   \n",
    "            values_table[f\"{consequent}_int\"] = int_consequent.squeeze().tolist()\n",
    "            values_table[f\"{consequent}_lp\"] = int_con_lp.squeeze().tolist()   \n",
    "\n",
    "    values_df = pd.DataFrame(values_table)\n",
    "\n",
    "    values_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    summands = [col for col in values_df.columns if col.endswith('lp')]\n",
    "    values_df[\"sum_log_prob\"] =  values_df[summands].sum(axis = 1)\n",
    "    values_df.sort_values(by = \"sum_log_prob\", ascending = False, inplace = True)\n",
    "\n",
    "    return values_df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this reduces the actual causality check to checking a property of the resulting sums of log probabilities\n",
    "# for the antecedent preemption and the consequent differs nodes\n",
    "\n",
    "def ac_check(trace, mwc, antecedents, witnesses, consequents):\n",
    "\n",
    "     table = get_table(trace, mwc, antecedents, witnesses, consequents)\n",
    "     \n",
    "     if (list(table['sum_log_prob'])[0]<= -1e8):\n",
    "          print(\"No resulting difference to the consequent in the sample.\")\n",
    "          return\n",
    "     \n",
    "     winners = table[table['sum_log_prob'] == table['sum_log_prob'].max()]\n",
    "     \n",
    "\n",
    "     ac_flags = []\n",
    "     for index, row in winners.iterrows():\n",
    "          active_antecedents = []\n",
    "          for antecedent in antecedents:\n",
    "               if row[f\"apr_{antecedent}\"] == 0:\n",
    "                    active_antecedents.append(antecedent)\n",
    "\n",
    "          ac_flags.append(set(active_antecedents) == set(antecedents))\n",
    "\n",
    "     if not any(ac_flags):\n",
    "          print(\"The antecedent set is not minimal.\")\n",
    "     else:\n",
    "          print(\"The antecedent set is an actual cause.\")\n",
    "\n",
    "     return any(ac_flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_sets(set_list):\n",
    "    inclusion_minimal = []\n",
    "    for s1 in set_list:\n",
    "        is_minimal = True\n",
    "        for s2 in set_list:\n",
    "            if s1 != s2 and s2.issubset(s1):\n",
    "                is_minimal = False\n",
    "        if is_minimal:\n",
    "            inclusion_minimal.append(s1)\n",
    "    return inclusion_minimal\n",
    "\n",
    "\n",
    "def tensorize_dictionary(dictionary):\n",
    "    return {k: torch.as_tensor(v) for k, v in dictionary.items()}\n",
    "\n",
    "def boolean_constraints_from_list(list):\n",
    "    return {k: pyro.distributions.constraints.boolean for k in list}\n",
    "\n",
    "def powerset(dict):\n",
    "    subdicts = []\n",
    "    keys = list(dict.keys())\n",
    "    key_tuples =  list(chain.from_iterable(combinations(keys, r) for r in range(len(keys)+1)))[1:-1]\n",
    "    for tuple in key_tuples:\n",
    "        subdicts.append({k: dict[k] for k in tuple})\n",
    "    return subdicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sufficient_causality_checkA(output_dict, antecedents = None, witnesses = None, consequents = None):\n",
    "\n",
    "    if antecedents is None:\n",
    "        antecedents = output_dict['antecedents']\n",
    "    if consequents is None:\n",
    "        consequents = list(output_dict['consequents_observed'].keys())\n",
    "    \n",
    "    endogenous_nodes = output_dict['endogenous_nodes']\n",
    "    causal_candidates = output_dict['causal_candidates']\n",
    "\n",
    "    if witnesses is None:\n",
    "        witnesses = [node for node in endogenous_nodes if (\n",
    "                                          node not in antecedents.keys() and \n",
    "                                          node not in consequents)]\n",
    "\n",
    "    table = get_table(output_dict['tr_sufficiency_A'],\n",
    "                      output_dict['mwc_sufficiency_A'],\n",
    "                      antecedents, witnesses, \n",
    "                      consequents)\n",
    "\n",
    "    # a bit hacky, but adding antecedents to conditioning\n",
    "    # within the first batch of handlers\n",
    "    # led to tensor broadcasting issues\n",
    "    for antecedent_str in antecedents.keys():\n",
    "        table = table[table[f\"{antecedent_str}_obs\"] == antecedents[antecedent_str]]\n",
    "    \n",
    "    table = table[table['sum_log_prob'] > -1e8]\n",
    "    \n",
    "    # we need to check set inclusion minimality of cause sets\n",
    "    # as there might be inclusion minimal sets that are not log-prob-sum minimal\n",
    "    # just because they have a higher cardinality\n",
    "    candidate_sets = []\n",
    "    for i, row in table.iterrows():\n",
    "        candidate_set = set()\n",
    "        for node in causal_candidates:\n",
    "            if row[f\"{node}_int\"] != row[f\"{node}_obs\"]:\n",
    "                candidate_set.add(node)\n",
    "        candidate_sets.append(candidate_set)\n",
    "        \n",
    "        actual_cause_sets = minimal_sets(candidate_sets)\n",
    "        \n",
    "        frozensets = [frozenset(s) for s in actual_cause_sets]\n",
    "        unique_actual_cause_sets = [set(f) for f in set(frozensets)]\n",
    "        \n",
    "    sufficiency_flag = any(key in ac_set for key in antecedents.keys() for ac_set in actual_cause_sets)\n",
    "\n",
    "    return  unique_actual_cause_sets, sufficiency_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forest fire example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_conjunctive():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "    match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                        u_match_dropped, event_dim=0)\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "    forest_fire = pyro.deterministic(\"forest_fire\", (match_dropped.bool() & lightning.bool()),\n",
    "                                      event_dim=0)\n",
    "\n",
    "    # return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "    #         \"forest_fire\": forest_fire}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecedents = {\"match_dropped\": 1.0, \"lightning\": 1.0}\n",
    "runs_n = 10 \n",
    "model = ff_conjunctive\n",
    "\n",
    "antecedent_candidates = powerset(antecedents)\n",
    "candidate_traces = []\n",
    "for antecedent_candidate in antecedent_candidates:\n",
    "    antecedent_candidate = tensorize_dictionary(antecedent_candidate)\n",
    "    with do( actions = antecedent_candidate):\n",
    "        with pyro.plate(\"samples\", runs_n):\n",
    "            with pyro.poutine.trace() as candidate_trace:\n",
    "                model()\n",
    "    candidate_traces.append(candidate_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_disjunctive():\n",
    "        u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "        u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "        match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                        u_match_dropped, event_dim=0)\n",
    "        lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "        forest_fire = pyro.deterministic(\"forest_fire\", (match_dropped.bool() | lightning.bool()).bool(), event_dim=0)\n",
    "\n",
    "        return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7.1.2. from the book\n",
    "\n",
    "# all contexts available, no settings excluded by what \n",
    "# the agent knows about the world\n",
    "# in the conjunctive model, the joint nodes are an explanation of forest fire\n",
    "\n",
    "# these are explanation candidates\n",
    "# antecedents = {\"match_dropped\": 1.0, \"lightning\": 1.0}\n",
    "# consequents = [\"forest_fire\"]\n",
    "# consequents_observed = tensorize_dictionary({\"forest_fire\": 1.0})\n",
    "# all_nodes = [\"match_dropped\", \"lightning\", \"forest_fire\"]\n",
    "# causal_candidates = [node for node in all_nodes if node not in consequents]\n",
    "# causal_candidate_constraints = boolean_constraints_from_list(causal_candidates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with MultiWorldCounterfactual() as mwc:\n",
    "#     with ExplainCauses(antecedents = causal_candidate_constraints, \n",
    "#                       witnesses = causal_candidates, consequents = consequents):\n",
    "#             with condition(data = {\"forest_fire\": torch.tensor(True)}):\n",
    "#                 with pyro.plate(\"sample\", 100):\n",
    "#                     with pyro.poutine.trace() as tr:\n",
    "#                         ff_conjunctive()\n",
    "\n",
    "# ff_conjunctive_table =  get_table(tr, mwc, causal_candidates, causal_candidates, consequents)\n",
    "\n",
    "#condition_e1a = sufficient_causality_checkA(ff_conjunctive_table, antecedents, causal_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now condition 1b \n",
    "# # P(consequents | do(antecedents)) = 1\n",
    "\n",
    "# with MultiWorldCounterfactual() as mwc:\n",
    "#       #  with pyro.plate(\"sample\", 100):\n",
    "#     with do( actions = antecedents):\n",
    "#         with pyro.plate(\"samples\", 100):\n",
    "#                 with pyro.poutine.trace() as tr:\n",
    "#                     ff_conjunctive()\n",
    "\n",
    "# outcome_df = pd.DataFrame()\n",
    "\n",
    "# with mwc:\n",
    "#     for consequent in consequents:\n",
    "        \n",
    "#         value = tr.trace.nodes[consequent][\"value\"]\n",
    "#         _indices = [\n",
    "#                 i for i in list(antecedents.keys()) if i in indices_of(value, event_dim=0)\n",
    "#             ]\n",
    "#         _int_con = gather(\n",
    "#         value, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\n",
    "#         outcome_df[consequent] = _int_con.squeeze().tolist()\n",
    "    \n",
    "# condition_e1b = ((outcome_df) == True).all().all()\n",
    "\n",
    "# #print(condition_e1a and condition_e1b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # condition_3: P(antecedents & consequent) > 0\n",
    "# # condition_4: P(antecedents < 1) \n",
    "    \n",
    "# with pyro.plate(\"samples\", 100):\n",
    "#         with pyro.poutine.trace() as tr:\n",
    "#             ff_conjunctive()\n",
    "            \n",
    "\n",
    "# reqs = {**antecedents, **consequents_observed}\n",
    "\n",
    "# reqs_outcome = pd.DataFrame()\n",
    "\n",
    "# for req in reqs:\n",
    "#     reqs_outcome[req] = tr.trace.nodes[req][\"value\"]\n",
    "       \n",
    "# condition_e3 = (reqs_outcome == 1.0).all(axis=1).any()\n",
    "# condition_e4 = (reqs_outcome.iloc[:, :2] == 0.0).any(axis=1).any()\n",
    "\n",
    "# #print(condition_e1a[1], condition_e1b, condition_e3, condition_e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sufficient_causality_checkB(output_dict, mwc = None, trace = None, antecedents = None, consequents = None):\n",
    "    \n",
    "    if antecedents is None:\n",
    "        antecedents = output_dict['antecedents']\n",
    "    if consequents is None:\n",
    "        consequents = list(output_dict['consequents_observed'].keys())\n",
    "    \n",
    "    if trace is None:    \n",
    "        trace = output_dict[\"tr_sufficiency_B\"]    \n",
    "    \n",
    "    if mwc is None:\n",
    "        mwc = output_dict[\"mwc_sufficiency_B\"]\n",
    "    \n",
    "    outcome_df = pd.DataFrame()\n",
    "    with mwc:\n",
    "        for consequent in consequents:\n",
    "            value = trace.trace.nodes[consequent][\"value\"]\n",
    "            _indices = [\n",
    "                    i for i in list(antecedents.keys()) if i in indices_of(value, event_dim=0)\n",
    "                ]\n",
    "            _int_con = gather(\n",
    "            value, IndexSet(**{i: {1} for i in _indices}), event_dim=0,)\n",
    "            outcome_df[consequent] = _int_con.squeeze().tolist()\n",
    "        \n",
    "    return ((outcome_df) == True).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibility_and_nontriviality_check(output_dict):\n",
    "    \n",
    "    trace = output_dict[\"tr_priors\"]\n",
    "    antecedents = output_dict['antecedents']\n",
    "    consequents_observed = output_dict['consequents_observed']\n",
    "    \n",
    "    reqs = {**antecedents, **consequents_observed}\n",
    "\n",
    "    reqs_outcome = pd.DataFrame()\n",
    "\n",
    "    for req in reqs:\n",
    "        reqs_outcome[req] = trace.trace.nodes[req][\"value\"]\n",
    "           \n",
    "    possibility = (reqs_outcome == 1.0).all(axis=1).any()\n",
    "    nontriviality = (reqs_outcome.iloc[:, :2] == 0.0).any(axis=1).any()\n",
    "    \n",
    "    return possibility, nontriviality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# antecedents = {\"match_dropped\": 1.0, \"lightning\": 1.0}\n",
    "# consequents = [\"forest_fire\"]\n",
    "# consequents_observed = tensorize_dictionary({\"forest_fire\": 1.0})\n",
    "# all_nodes = [\"match_dropped\", \"lightning\", \"forest_fire\"]\n",
    "# causal_candidates = [node for node in all_nodes if node not in consequents]\n",
    "# causal_candidate_constraints = boolean_constraints_from_list(causal_candidates)\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def Explanation_Evaluation( \n",
    "        model: Callable,\n",
    "        antecedents: Dict[str, torch.Tensor],\n",
    "        consequents_observed: Dict[str, torch.Tensor],\n",
    "        endogenous_nodes: List[str],\n",
    "        runs_n: int = 100,):\n",
    "\n",
    "        consequents = list(consequents_observed.keys())\n",
    "        consequents_observed = tensorize_dictionary(consequents_observed)\n",
    "        causal_candidates = [node for node in endogenous_nodes if node not in consequents_observed.keys()]\n",
    "        # this needs to be replaced if nodes are not boolean\n",
    "        causal_candidate_constraints = boolean_constraints_from_list(causal_candidates)\n",
    "\n",
    "        # needed to check if always a part of the antecedent set is a part of an actual cause\n",
    "        with MultiWorldCounterfactual() as mwc_sufficiency_A:\n",
    "            with ExplainCauses(antecedents = causal_candidate_constraints, \n",
    "                      witnesses = causal_candidates, consequents = consequents):\n",
    "                with condition(data = {\"forest_fire\": torch.tensor(True)}):\n",
    "                    with pyro.plate(\"sample\", runs_n):\n",
    "                        with pyro.poutine.trace() as tr_sufficiency_A:\n",
    "                            model()\n",
    "                            \n",
    "        # needed to check P(consequents | do(antecedents)) = 1                    \n",
    "        with MultiWorldCounterfactual() as mwc_sufficiency_B:\n",
    "            with do( actions = antecedents):\n",
    "                with pyro.plate(\"samples\", runs_n):\n",
    "                    with pyro.poutine.trace() as tr_sufficiency_B:\n",
    "                        model()\n",
    "        \n",
    "        # # needed to check minimality:\n",
    "        antecedent_candidates = powerset(antecedents)\n",
    "        candidate_mwc = []\n",
    "        candidate_traces = []\n",
    "        for antecedent_candidate in antecedent_candidates:\n",
    "            antecedent_candidate = tensorize_dictionary(antecedent_candidate)\n",
    "            with MultiWorldCounterfactual() as mwc_candidate:\n",
    "                with do( actions = antecedent_candidate):\n",
    "                    with pyro.plate(\"samples\", runs_n):\n",
    "                        with pyro.poutine.trace() as candidate_trace:\n",
    "                            model()\n",
    "            candidate_mwc.append(mwc_candidate)\n",
    "            candidate_traces.append(candidate_trace)\n",
    "        \n",
    "                        \n",
    "        # will be used to check\n",
    "        # - possibility: P(antecedents & consequent) > 0\n",
    "        # - nontriviality: P(antecedents < 1) \n",
    "        with pyro.plate(\"samples\", runs_n):\n",
    "            with pyro.poutine.trace() as tr_priors:\n",
    "                model()\n",
    "        \n",
    "        \n",
    "                            \n",
    "        yield {\"mwc_sufficiency_A\": mwc_sufficiency_A, \"tr_sufficiency_A\": tr_sufficiency_A,\n",
    "               \"mwc_sufficiency_B\": mwc_sufficiency_B, \"tr_sufficiency_B\": tr_sufficiency_B,\n",
    "               \"tr_priors\": tr_priors,  \n",
    "               \"mwc_candidate\": candidate_mwc, \n",
    "               \"tr_candidate\": candidate_traces,\n",
    "               \"antecedents\": antecedents, \n",
    "               \"consequents_observed\": consequents_observed, \"endogenous_nodes\": endogenous_nodes,\n",
    "               \"causal_candidates\": causal_candidates}\n",
    "        \n",
    "           \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_ff_con_handler =  Explanation_Evaluation(\n",
    "    model = ff_conjunctive,\n",
    "    antecedents={\"match_dropped\": 1.0, \"lightning\": 1.0},\n",
    "    consequents_observed={\"forest_fire\": torch.tensor(True)},\n",
    "    endogenous_nodes=[\"match_dropped\", \"lightning\", \"forest_fire\"],\n",
    ")\n",
    "    \n",
    "with exp_ff_con_handler as con_ff:\n",
    "    ff_conjunctive()\n",
    "    \n",
    "\n",
    "exp_ff_dis_handler =  Explanation_Evaluation(\n",
    "    model = ff_disjunctive,\n",
    "    antecedents={\"match_dropped\": 1.0, \"lightning\": 1.0},\n",
    "    consequents_observed={\"forest_fire\": torch.tensor(True)},\n",
    "    endogenous_nodes=[\"match_dropped\", \"lightning\", \"forest_fire\"],\n",
    ")\n",
    "    \n",
    "    \n",
    "with exp_ff_dis_handler as dis_ff:\n",
    "    ff_disjunctive()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'match_dropped': 1.0}\n",
      "1\n",
      "{'lightning': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'minimality': True,\n",
       " 'a_checks': [False, False],\n",
       " 'b_checks': [False, False],\n",
       " 'antecedent_candidates': [{'match_dropped': 1.0}, {'lightning': 1.0}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minimal_sufficiency_check(output_dict):\n",
    "    antecedent_candidates = powerset(output_dict['antecedents'])\n",
    "    \n",
    "    a_checks = []\n",
    "    b_checks = []\n",
    "    for i, antecedent_candidate in enumerate(antecedent_candidates):\n",
    "        print(i)\n",
    "        print(antecedent_candidate)\n",
    "        a_checks.append(sufficient_causality_checkA(\n",
    "            output_dict,antecedents = antecedent_candidate,            \n",
    "                witnesses = [node for node in output_dict['endogenous_nodes'] if (\n",
    "                            node not in antecedent_candidate.keys() and \n",
    "                            node not in output_dict['consequents_observed'].keys())])[1]\n",
    "        )\n",
    "    \n",
    "        b_checks.append( sufficient_causality_checkB(output_dict, \n",
    "                                                     mwc = output_dict[\"mwc_candidate\"][i],\n",
    "                                                     trace = output_dict[\"tr_candidate\"][i],\n",
    "                                            antecedents = antecedent_candidate)\n",
    "            )\n",
    "        \n",
    "    minimality = not any(a and b for a,b in zip(a_checks, b_checks))       \n",
    "    \n",
    "    return {\"minimality\": minimality, \"a_checks\": a_checks, \"b_checks\": b_checks, \"antecedent_candidates\": antecedent_candidates}\n",
    "\n",
    "minimal_sufficiency_check(con_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def explanation_check(output_object):\n",
    "    \n",
    "    sufficiencyA = sufficient_causality_checkA(output_object)[1]\n",
    "    sufficiencyB = sufficient_causality_checkB(output_object)\n",
    "    #minimal_sufficiency = minimal_sufficiency_check(output_object)['minimality']\n",
    "    possibility, nontriviality = possibility_and_nontriviality_check(output_object)\n",
    "    \n",
    "    return all([sufficiencyA, sufficiencyB, minimal_sufficiency, possibility, nontriviality])\n",
    "\n",
    "\n",
    "\n",
    "explanation_check(con_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# But none individually is (see e.g. match_dropped)\n",
    "\n",
    "exp_ff_con_separate_handler =  Explanation_Evaluation(\n",
    "    model = ff_conjunctive,\n",
    "    antecedents={\"match_dropped\": 1.0},\n",
    "    consequents_observed={\"forest_fire\": torch.tensor(True)},\n",
    "    endogenous_nodes=[\"match_dropped\", \"lightning\", \"forest_fire\"],\n",
    ")\n",
    "     \n",
    "with exp_ff_con_separate_handler as con_ff_separate:\n",
    "    ff_conjunctive()\n",
    "    \n",
    "explanation_check(con_ff_separate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The situation is the opposite for the disjunctive model\n",
    "# the joint nodes are not an explanation of ff\n",
    "# each of the individual nodes is an explanation of ff\n",
    "exp_ff_dis_handler =  Explanation_Evaluation(\n",
    "    model = ff_disjunctive,\n",
    "    antecedents={\"match_dropped\": 1.0, \"lightning\": 1.0},\n",
    "    consequents_observed={\"forest_fire\": torch.tensor(True)},\n",
    "    endogenous_nodes=[\"match_dropped\", \"lightning\", \"forest_fire\"],\n",
    ")\n",
    "    \n",
    "    \n",
    "with exp_ff_dis_handler as dis_ff:\n",
    "    ff_disjunctive()\n",
    "    \n",
    "#minimal_sufficiency_check(dis_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'explanation_check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/rafal/UGPOP/projectsUGPOP/chirho/docs/source/causal_explanation.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/rafal/UGPOP/projectsUGPOP/chirho/docs/source/causal_explanation.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m explanation_check(dis_ff)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'explanation_check' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "explanation_check(dis_ff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
