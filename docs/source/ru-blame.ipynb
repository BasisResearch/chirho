{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blame and actual causality"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preceding notebooks**\n",
    "\n",
    "- [Actual Causality: the modified Halpern-Pearl definition]() TODO add link\n",
    "\n",
    "- [Responsibility and actual causality]() TODO add link\n",
    "\n",
    "**Summary**\n",
    "\n",
    "We use the causal-model based explication of responsibility by J. Halpern (*Actual Causality, MIT Press, 2016), explained in a previous notebook to follow Halpern's explication of blame in terms of responsibility."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline:**\n",
    "\n",
    "[Intuitions](##intuitions)\n",
    "    \n",
    "[Formalization](#formalization)\n",
    "\n",
    "[Implementation](#implementation)\n",
    "\n",
    "[Examples](#examples)\n",
    "\n",
    "- [Comments on example selection](#comments-on-example-selection)\n",
    "  \n",
    "- [Voting](#voting)\n",
    "\n",
    "- [Stone-throwing](#stone-throwing)\n",
    "\n",
    "- [Firing squad](#firing-squad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blame assigned to agents, on this approach, depends on their epistemic state - if an agent had no reason to believe that performing a given action would lead to a given outcome, they should not be blamed for this action even if this outcome came about. (They might be blamed for not performing their epistemic duties in properly finding out whether the outcome would come about, but that is another action that the agent might still be blamed for). Blame, on this approach, is the agent's expected responsibility for a given outcome, relative to the agent's probability distribution over possible causal models (including states of the exogenous variables)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say the epistemic state of an agent is represented as $\\langle K, \\Pr\\rangle$, where $K$ is a set of causal settings, and $Pr$ is a probability distribution over $K$, representing the agent's uncertainty. Then the degree of blame of $X=x$ for $\\varphi$ relative to $\\langle K, \\Pr\\rangle$ is $\\sum_{\\langle M, \\vec{u}\\rangle \\in K} dr\\left( \\langle M, \\vec{u}\\rangle, X=x, \\varphi\\right)Pr\\left(\\langle M, \\vec{u}\\rangle\\right)$, where $dr$ is the degree of responsibility of $X=x$ for $\\varphi$ in $\\langle M, \\vec{u}\\rangle$, as defined in a previous notebook TODO add link"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we simply re-use the implementations of actual causality and responsibility from the previous notebooks. All that remains is the calculation of expected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from typing import Dict, List, Optional,  Union, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import random\n",
    "\n",
    "from causal_pyro.indexed.ops import IndexSet, gather, indices_of, scatter\n",
    "from causal_pyro.interventional.handlers import do\n",
    "from causal_pyro.counterfactual.handlers import MultiWorldCounterfactual, Preemptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalpernPearlModifiedApproximate:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Callable,\n",
    "        antecedents: Union[Dict[str, torch.Tensor], List[str]],\n",
    "        outcome: str,\n",
    "        witness_candidates: List[str],\n",
    "        observations: Optional[Dict[str, torch.Tensor]],\n",
    "        sample_size: int = 100,\n",
    "        event_dim: int = 0\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.antecedents = antecedents\n",
    "        self.outcome = outcome\n",
    "        self.witness_candidates = witness_candidates\n",
    "        self.nodes = antecedents + [outcome] + witness_candidates\n",
    "        self.observations = observations\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.antecedents_dict = (\n",
    "            self.antecedents if isinstance(self.antecedents, dict)\n",
    "            else self.revert_antecedents(self.antecedents)\n",
    "        )\n",
    "    \n",
    "        self.preemptions = {candidate: functools.partial(self.preempt_with_factual,\n",
    "                                             antecedents = self.antecedents) for \n",
    "                                             candidate in self.witness_candidates}\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def revert_antecedents(antecedents: List[str]) -> Dict[str, Callable[[torch.Tensor], torch.Tensor]]:\n",
    "        return {antecedent: (lambda v: 1 - v) for antecedent in antecedents}\n",
    "\n",
    "    @staticmethod   \n",
    "    def preempt_with_factual(value: torch.Tensor, *,\n",
    "                          antecedents: List[str] = None, event_dim: int = 0):\n",
    "    \n",
    "        if antecedents is None:\n",
    "            antecedents = []\n",
    "\n",
    "        antecedents = [a for a in antecedents if a in indices_of(value, event_dim=event_dim)]\n",
    "\n",
    "        factual_value = gather(value, IndexSet(**{antecedent: {0} for antecedent in antecedents}),\n",
    "                                event_dim=event_dim)\n",
    "            \n",
    "        return scatter({\n",
    "            IndexSet(**{antecedent: {0} for antecedent in antecedents}): factual_value,\n",
    "            IndexSet(**{antecedent: {1} for antecedent in antecedents}): factual_value,\n",
    "        }, event_dim=event_dim)\n",
    "        \n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with pyro.poutine.trace() as trace:\n",
    "            with MultiWorldCounterfactual():\n",
    "                with do(actions=self.antecedents_dict):\n",
    "                    with Preemptions(actions = self.preemptions):\n",
    "                        with pyro.condition(data={k: torch.as_tensor(v) for k, v in self.observations.items()}):\n",
    "                            with pyro.plate(\"plate\", self.sample_size):\n",
    "                                self.consequent = self.model()[self.outcome]\n",
    "                                self.intervened_consequent = gather(self.consequent, IndexSet(**{ant: {1} for ant in self.antecedents}))\n",
    "                                self.observed_consequent = gather(self.consequent, IndexSet(**{ant: {0} for ant in self.antecedents}))\n",
    "                                self.consequent_differs = self.intervened_consequent != self.observed_consequent   \n",
    "                                pyro.factor(\"consequent_differs\", torch.where(self.consequent_differs, torch.tensor(0.0), torch.tensor(-1e8)))\n",
    "                            \n",
    "        self.trace = trace.trace\n",
    "        self.nodes_trace = {node: self.trace.nodes[node]['value'] for node in self.nodes}\n",
    "        \n",
    "        \n",
    "         # slightly hacky solution for odd witness candidate sets\n",
    "        if  isinstance(self.consequent_differs.squeeze().tolist(), bool):\n",
    "            self.existential_but_for = self.consequent_differs.squeeze()\n",
    "        else:\n",
    "            #if (len(self.consequent_differs.squeeze().tolist() )>1):\n",
    "            self.existential_but_for = any(self.consequent_differs.squeeze().tolist()                )  \n",
    "\n",
    "\n",
    "        witness_dict = dict()\n",
    "        if self.witness_candidates:\n",
    "            witness_keys = [\"__split_\" + candidate for candidate in self.witness_candidates]\n",
    "            witness_dict = {key: self.trace.nodes[key]['value']  for key in witness_keys}\n",
    "            \n",
    "\n",
    "        witness_dict['observed'] = self.observed_consequent.squeeze()\n",
    "        witness_dict['intervened'] = self.intervened_consequent.squeeze()\n",
    "        witness_dict['consequent_differs'] = self.consequent_differs.squeeze()\n",
    "\n",
    "        # slightly hacky as above\n",
    "        self.witness_df = pd.DataFrame(witness_dict) if self.witness_candidates else witness_dict\n",
    "\n",
    "        if self.witness_candidates:\n",
    "            self.witness_df['witness_size'] = self.witness_df[witness_keys].sum(axis = 1)\n",
    "            satisfactory = self.witness_df[self.witness_df['consequent_differs'] == True]\n",
    "            \n",
    "        self.minimal_witness_size = satisfactory['witness_size'].min() if self.witness_candidates else 0\n",
    "        self.responsibility_internal = 1/(len(self.antecedents) + self.minimal_witness_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalpernPearlResponsibilityApproximate:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Callable,\n",
    "        nodes: List,\n",
    "        antecedent: str,\n",
    "        outcome: str,\n",
    "        observations: Dict[str, torch.Tensor], \n",
    "        runs_n: int \n",
    "    ):\n",
    "        self.model = model\n",
    "        self.nodes = nodes\n",
    "        self.antecedent = antecedent\n",
    "        self.outcome = outcome\n",
    "        self.observations = observations\n",
    "        self.runs_n = runs_n\n",
    "        \n",
    "        self.minimal_antecedents_cache = []\n",
    "        self.antecedent_sizes = []\n",
    "        self.existential_but_fors = []\n",
    "        self.minimal_witness_sizes = []\n",
    "        self.responsibilities = []\n",
    "        self.HPMs = []\n",
    "\n",
    "    def __call__(self):\n",
    "        \n",
    "        for step in range(1,self.runs_n):\n",
    "\n",
    "            nodes = self.nodes\n",
    "            if self.outcome in nodes:\n",
    "                nodes.remove(self.outcome) \n",
    "            \n",
    "            companion_size = random.randint(0,len(nodes))\n",
    "            companion_candidates = random.sample(self.nodes, companion_size)\n",
    "            witness_candidates = [node for node in self.nodes if \n",
    "                                node != self.antecedent and \n",
    "                                node != self.outcome and \n",
    "                                    node not in companion_candidates]\n",
    "\n",
    "            HPM = HalpernPearlModifiedApproximate(\n",
    "                model = self.model,\n",
    "                antecedents = companion_candidates,\n",
    "                outcome = self.outcome,\n",
    "                witness_candidates = witness_candidates,\n",
    "                observations = self.observations,\n",
    "                sample_size = 1000)\n",
    "            \n",
    "            HPM()\n",
    "\n",
    "            self.HPMs.append(HPM)\n",
    "\n",
    "\n",
    "            if  HPM.existential_but_for:\n",
    "\n",
    "\n",
    "                subset_in_cache = any([s.issubset(set(HPM.antecedents)) for s in self.minimal_antecedents_cache])\n",
    "                if not subset_in_cache:\n",
    "                    self.minimal_antecedents_cache.append(set(HPM.antecedents))\n",
    "\n",
    "                    if self.antecedent in HPM.antecedents:\n",
    "                        self.antecedent_sizes.append(len(HPM.antecedents))\n",
    "                        self.existential_but_fors.append(HPM.existential_but_for)\n",
    "                        self.minimal_witness_sizes.append(HPM.minimal_witness_size)\n",
    "                        self.responsibilities.append(HPM.responsibility_internal)\n",
    "\n",
    "\n",
    "        self.denumerators = [x + y for x, y in zip(self.antecedent_sizes, self.minimal_witness_sizes)]\n",
    "\n",
    "        self.responsibilityDF = pd.DataFrame(\n",
    "            {\"existential_but_for\": [bool(value) for value in self.existential_but_fors],\n",
    "                \"antecedent_size\": self.antecedent_sizes, \n",
    "                \"minimal_witness_size\": self.minimal_witness_sizes,\n",
    "                \"denumerator\": self.denumerators,\n",
    "                \"responsibility\": self.responsibilities\n",
    "            }\n",
    "            )\n",
    "        if len(self.responsibilityDF['existential_but_for']) == 0:\n",
    "            self.responsibility = 0\n",
    "        else:\n",
    "            min_denumerator = min(self.responsibilityDF['denumerator'])\n",
    "            self.responsibility = 1/min_denumerator\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on example selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **firing squad:**  in this case we illustrate that while responsibility will be solely with the single marksman who has the live bullet (see a previous notebook on responsibility), blame assignments will be proportionate to the probabilities. \n",
    "  \n",
    "- **voters:** while Halpern only considered responsibility for an outcome if one votes (and suggests that blame can be calculated), we go somewhat further. We investigate the blame for an outcome assigned to a voter who decides not to vote. We also use this example to illustrate that the definition of responsibility in terms of node-counting is naive, as the same situation can easily be represented using models with different node counts, which will have impact on responsibility and blame calculations.\n",
    "\n",
    "- **pollution:** Halpern describes expected responsibility calculations for four scenarios involving two companies dumping pollutants into a river. We go further and show how approximate blame calculations in a continuous case (involving uncertainties about the amounts of pollutant dumped by the companies and about the threshold sufficient for the fish to die)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firing squad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already discuss responsibility assignment in this scenario in the responsibility notebook. There is a firing squad consisting of five excellent marksmen. Only one of them has a live bullet in his rifle and the rest have blanks. They shoot and the prisoner dies.The marksmen shoot at the prisoner and he dies. The only cause of the prisoner’s death is the marksman with the live bullet. We have already seen that marksman has degree of responsibility 1 for the death and all the others have degree of responsibility 0. Now we will see that if the marksmen completely do not know which of them has the live bullet, blame is nevertheless equally distributed between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def firing_squad_model():\n",
    "    probs = pyro.sample(\"probs\", dist.Dirichlet(torch.ones(5)))\n",
    "\n",
    "    who_has_bullet = pyro.sample(\"who_has_bullet\", dist.OneHotCategorical(probs))\n",
    "\n",
    "    mark0 = pyro.deterministic(\"mark0\", torch.tensor([who[0] for who in who_has_bullet]), event_dim=0)\n",
    "    mark1 = pyro.deterministic(\"mark1\", torch.tensor([who[1] for who in who_has_bullet]), event_dim=0)\n",
    "    mark2 = pyro.deterministic(\"mark2\", torch.tensor([who[2] for who in who_has_bullet]), event_dim=0)\n",
    "    mark3 = pyro.deterministic(\"mark3\", torch.tensor([who[3] for who in who_has_bullet]), event_dim=0)\n",
    "    mark4 = pyro.deterministic(\"mark4\", torch.tensor([who[4] for who in who_has_bullet]), event_dim=0)\n",
    "\n",
    "    dead = pyro.deterministic(\"dead\", mark0 + mark1 + mark2 + mark3 + \n",
    "                                mark4  > 0)\n",
    "    \n",
    "    return {\"probs\": probs,\n",
    "            \"mark0\": mark0,\n",
    "            \"mark1\": mark1,\n",
    "            \"mark2\": mark2,\n",
    "\n",
    "            \"mark3\": mark3,\n",
    "            \"mark4\": mark4, \n",
    "            \"dead\": dead}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2000)\n"
     ]
    }
   ],
   "source": [
    "# assign weights to models\n",
    "# and take the weighted average\n",
    "\n",
    "pyro.set_rng_seed(102)\n",
    "\n",
    "marksman0_responsibilities = []\n",
    "for who in range(0,5):\n",
    "    probs = torch.zeros(5)\n",
    "    probs[who] = 1.0\n",
    "\n",
    "\n",
    "    responsibility_marksmen_HPR = HalpernPearlResponsibilityApproximate(\n",
    "        model = firing_squad_model,\n",
    "        nodes = [\"mark\" + str(i) for i in range(0,5)],\n",
    "        antecedent = \"mark0\", outcome = \"dead\",\n",
    "        observations = {\"probs\": probs,},\n",
    "                        runs_n=50)\n",
    "\n",
    "    responsibility_marksmen_HPR()\n",
    "    marksman0_responsibilities.append(responsibility_marksmen_HPR.responsibility)\n",
    "\n",
    "weights = torch.full((5,), .2)\n",
    "\n",
    "# if your pr(mark0) = i, \n",
    "# the blame in this case is i as well \n",
    "\n",
    "print(torch.dot(torch.tensor(marksman0_responsibilities), weights))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The general phenomenon in the voting setup is this: if $a$ voted for and $b$ against, $a>b$, a voter voting for has degree of responsibility $\\frac{1}{\\lceil (a-b)/2\\rceil}$. So much is in Halpern. Now let's go beyond this: what if you decide whether to vote and consider your blame if your candidate looses and you didn't vote? This is still a somewhat simplistic setup: eight other people are about to vote, each outcome among them you consider equally likely, and you consider the blame associated with you skipping the voting if your candidate fails. In the first model, we represent the number of other voters who voted the same way you would have voted by a single node; in the second model, we have a separate node for each voter. We will see that this shift in representation method results in quite a difference in blame assignment. This indicates that counting nodes is a rather crude method of responsibility- or blame-splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_to_vote_model():\n",
    "    u_a = pyro.sample(\"u_a\", dist.Categorical(logits=torch.ones(8)))\n",
    "    u_skip = pyro.sample(\"u_skip\", dist.Bernoulli(0.5))\n",
    "\n",
    "    a = pyro.deterministic(\"a\", u_a, event_dim=0)\n",
    "    skip = pyro.deterministic(\"skip\", u_skip, event_dim=0)\n",
    "\n",
    "    result = pyro.deterministic(\"result\", \n",
    "                            (a + 1 - skip) > torch.ceil(9 - skip)/2)\n",
    "\n",
    "    return {\"a\": a, \"against\": 8-a, \"voted\": 1 - skip, \"result\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your skipping is not an actual cause if a=1\n",
    "\n",
    "skipping1HPM = HalpernPearlModifiedApproximate(\n",
    "    model = choose_to_vote_model,\n",
    "    antecedents = [\"skip\"],\n",
    "    outcome = \"result\",\n",
    "    witness_candidates = [\"a\"],\n",
    "    observations = dict(u_a=1.),\n",
    "    sample_size = 100)\n",
    "\n",
    "skipping1HPM()\n",
    "\n",
    "skipping1HPM.existential_but_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but it is, if a = 4\n",
    "\n",
    "skipping4HPM = HalpernPearlModifiedApproximate(\n",
    "    model = choose_to_vote_model,\n",
    "    antecedents = [\"skip\"],\n",
    "    outcome = \"result\",\n",
    "    witness_candidates = [\"a\"],\n",
    "    observations = dict(u_a=4.),\n",
    "    sample_size = 100)\n",
    "\n",
    "skipping4HPM()\n",
    "\n",
    "skipping4HPM.existential_but_for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with this model responsibilities follow the same pattern\n",
    "\n",
    "skipping1HPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = choose_to_vote_model,\n",
    "    nodes = [\"a\", \"skip\"],\n",
    "    antecedent = \"skip\", outcome = \"result\",\n",
    "    observations = dict(u_a=1.), \n",
    "    runs_n=50\n",
    "    )\n",
    "\n",
    "skipping1HPR()\n",
    "skipping1HPR.responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.set_rng_seed(102)\n",
    "skipping4HPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = choose_to_vote_model,\n",
    "    nodes = [\"a\", \"skip\"],\n",
    "    antecedent = \"skip\", outcome = \"result\",\n",
    "    observations = dict(u_a=4.), \n",
    "    runs_n=50\n",
    "    )\n",
    "\n",
    "skipping4HPR()\n",
    "skipping4HPR.responsibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but now suppose we work with a different representation\n",
    "# in which each voter has their own node\n",
    "\n",
    "def choose_to_vote_wide_model():\n",
    "    u_vote0 = pyro.sample(\"u_vote0\", dist.Bernoulli(0.6))\n",
    "    u_vote1 = pyro.sample(\"u_vote1\", dist.Bernoulli(0.6))\n",
    "    u_vote2 = pyro.sample(\"u_vote2\", dist.Bernoulli(0.6))\n",
    "    u_vote3 = pyro.sample(\"u_vote3\", dist.Bernoulli(0.6))\n",
    "    u_vote4 = pyro.sample(\"u_vote4\", dist.Bernoulli(0.6))\n",
    "    u_vote5 = pyro.sample(\"u_vote5\", dist.Bernoulli(0.6))\n",
    "    u_vote6 = pyro.sample(\"u_vote6\", dist.Bernoulli(0.6))\n",
    "    u_vote7 = pyro.sample(\"u_vote7\", dist.Bernoulli(0.6))\n",
    "    u_skip = pyro.sample(\"u_skip\", dist.Bernoulli(0.5))\n",
    "\n",
    "    vote0 = pyro.deterministic(\"vote0\", u_vote0, event_dim=0)\n",
    "    vote1 = pyro.deterministic(\"vote1\", u_vote1, event_dim=0)\n",
    "    vote2 = pyro.deterministic(\"vote2\", u_vote2, event_dim=0)\n",
    "    vote3 = pyro.deterministic(\"vote3\", u_vote3, event_dim=0)\n",
    "    vote4 = pyro.deterministic(\"vote4\", u_vote4, event_dim=0)\n",
    "    vote5 = pyro.deterministic(\"vote5\", u_vote5, event_dim=0)\n",
    "    vote6 = pyro.deterministic(\"vote6\", u_vote6, event_dim=0)\n",
    "    vote7 = pyro.deterministic(\"vote7\", u_vote7, event_dim=0)\n",
    "    skip = pyro.deterministic(\"skip\", u_skip, event_dim=0)\n",
    "\n",
    "\n",
    "    result = pyro.deterministic(\"result\", vote0 + vote1 + vote2 + vote3 + \n",
    "                                 vote4 + vote5 + vote6 + vote7 + 1 - skip > 4)\n",
    "    return {\"result\": result.float()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if a = 1, skipping is still not an actual cause\n",
    "\n",
    "skipping_wide1HPM = HalpernPearlModifiedApproximate(\n",
    "    model = choose_to_vote_wide_model,\n",
    "    antecedents = [\"skip\"],\n",
    "    outcome = \"result\",\n",
    "    witness_candidates = [f\"vote{i}\" for i in range(0,8)],\n",
    "    observations = dict(u_vote0=1., u_vote1=0., u_vote2=0.,\n",
    "                        u_vote3=0., u_vote4=0., u_vote5=0,\n",
    "                        u_vote6=0., u_vote7=0.),\n",
    "    sample_size = 1000)\n",
    "\n",
    "skipping_wide1HPM()\n",
    "skipping_wide1HPM.existential_but_for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if a = 4, skipping still is an actual cause\n",
    "\n",
    "skipping_wide4HPM = HalpernPearlModifiedApproximate(\n",
    "    model = choose_to_vote_wide_model,\n",
    "    antecedents = [\"skip\"],\n",
    "    outcome = \"result\",\n",
    "    witness_candidates = [f\"vote{i}\" for i in range(0,8)],\n",
    "    observations = dict(u_vote0=1., u_vote1=1., u_vote2=1.,\n",
    "                        u_vote3=1., u_vote4=0., u_vote5=0,\n",
    "                        u_vote6=0., u_vote7=0.),\n",
    "    sample_size = 1000)\n",
    "\n",
    "skipping_wide4HPM()\n",
    "skipping_wide4HPM.existential_but_for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if a = 1 your responsibility is now 1/4\n",
    "# because if you voted for\n",
    "# and three other voters changed their mind,\n",
    "# the outcome would be different\n",
    "\n",
    "skipping_wide1HPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = choose_to_vote_wide_model,\n",
    "    nodes = [f\"vote{i}\" for i in range(0,8,)] + [\"skip\"],\n",
    "    antecedent = \"skip\", outcome = \"result\",\n",
    "    observations =  dict(u_vote0=1., u_vote1=0., u_vote2=0.,\n",
    "                        u_vote3=0., u_vote4=0., u_vote5=0,\n",
    "                        u_vote6=0., u_vote7=0.), \n",
    "    runs_n=500\n",
    "    )\n",
    "\n",
    "skipping_wide1HPR()\n",
    "skipping_wide1HPR.responsibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above illustrates one reason why what Halpern called the naive definition of responsibility used in Halpern is properly so named: it is highly sensitive to modeling choices and needs to be improved. This is a story for another occasion. Meawhile, let's assume that the wide model is the intended one and see what can be said about blame for failure before the voting happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1.0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a>4 blame is zero as your candidate wins\n",
    "# let's use the first model\n",
    "\n",
    "pyro.set_rng_seed(1231)\n",
    "\n",
    "skipping_responsibilities = []\n",
    "\n",
    "for a in range(0,5):\n",
    "\n",
    "    observations = dict(u_a=a)\n",
    "\n",
    "    skipping_stepHPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = choose_to_vote_model,\n",
    "    nodes = [\"a\", \"skip\"],\n",
    "    antecedent = \"skip\", outcome = \"result\",\n",
    "    observations =  observations, \n",
    "    runs_n=100\n",
    "    )\n",
    "    skipping_stepHPR()\n",
    "\n",
    "    skipping_responsibilities.append(skipping_stepHPR.responsibility)\n",
    "\n",
    "skipping_responsibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2, 0.25, 0.3333333333333333, 0.3333333333333333, 1.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for a>4 blame is zero as your candidate wins\n",
    "\n",
    "pyro.set_rng_seed(1231)\n",
    "\n",
    "skipping_responsibilities_wide = []\n",
    "\n",
    "for a in range(0,5):\n",
    "\n",
    "    observations = {\"u_vote\"+str(i): 1. if i < a else 0. for i in range(0,8)}\n",
    "\n",
    "    skipping_wide_stepHPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = choose_to_vote_wide_model,\n",
    "    nodes = [f\"vote{i}\" for i in range(0,8,)] + [\"skip\"],\n",
    "    antecedent = \"skip\", outcome = \"result\",\n",
    "    observations =  observations, \n",
    "    runs_n=100\n",
    "    )\n",
    "    skipping_wide_stepHPR()\n",
    "\n",
    "    skipping_responsibilities_wide.append(skipping_wide_stepHPR.responsibility)\n",
    "\n",
    "skipping_responsibilities_wide\n",
    "# [0.2, 0.25, 0.3333333333333333, 0.3333333333333333, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blame in the first model: 0.125 blame in the second model: 0.26458333333333334\n"
     ]
    }
   ],
   "source": [
    "# assuming each value of a has probability 1/8\n",
    "# the expected value of responsibility for failure \n",
    "# if you skip the voting\n",
    "\n",
    "blame = sum([1/8 * resp for resp in skipping_responsibilities])\n",
    "# 1/8\n",
    "\n",
    "blame_wide = sum([1/8 * resp for resp in skipping_responsibilities_wide])\n",
    "# approx. 0.264\n",
    "\n",
    "print(\"blame in the first model:\", blame, \"blame in the second model:\", blame_wide)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pollution\n",
    "\n",
    "In the original example, company A dumps either 0 or 100 kilograms of pollutant, company B dumps either 60 or 0 kilograms, and biologists determine that k kilograms of pollutant sufﬁce for the ﬁsh to die. \n",
    "One problematic case is was when k = 80 and both companies dump pollutants. In this case, only A is a cause of the ﬁsh dying according to the modiﬁed HP deﬁnition. We'll go a bit further: we consider a situation in which the amount that companies have to potentially dump and the threshold come from normal distributions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_A = 100\n",
    "amount_B = 60\n",
    "\n",
    "def pollution_model():\n",
    "    u_A_dumps = pyro.sample(\"u_A_dumps\", dist.Bernoulli(0.5))\n",
    "    u_B_dumps = pyro.sample(\"u_B_dumps\", dist.Bernoulli(0.5))\n",
    "    u_k = pyro.sample(\"u_k\", dist.Normal(80., 10.))\n",
    "\n",
    "    A_dumps = pyro.deterministic(\"A_dumps\", u_A_dumps, event_dim=0)\n",
    "    B_dumps = pyro.deterministic(\"B_dumps\", u_B_dumps, event_dim=0)\n",
    "    k = pyro.deterministic(\"k\", u_k, event_dim=0)\n",
    "\n",
    "    total_pollution = pyro.deterministic(\"total_pollution\",\n",
    "                                          A_dumps * amount_A + B_dumps * amount_B)\n",
    "    dead = pyro.deterministic(\"dead\", total_pollution > k)\n",
    "\n",
    "    return {\"total_pollution\": total_pollution, \"k\": k, \"dead\": dead}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False\n"
     ]
    }
   ],
   "source": [
    "# A an actual cause in the original scenario\n",
    "# B not an actual cause in the original scenario\n",
    "# set threshold to 80\n",
    "\n",
    "\n",
    "pollution_strict_A_HPM = HalpernPearlModifiedApproximate(\n",
    "    model = pollution_model,\n",
    "    antecedents = [\"A_dumps\"],\n",
    "    outcome = \"dead\",\n",
    "    witness_candidates = [\"B_dumps\"],\n",
    "    observations = dict(u_A_dumps=1., u_B_dumps=1., u_k=80.),\n",
    "    sample_size = 5)\n",
    "\n",
    "pollution_strict_B_HPM = HalpernPearlModifiedApproximate(\n",
    "    model = pollution_model,\n",
    "    antecedents = [\"B_dumps\"],\n",
    "    outcome = \"dead\",\n",
    "    witness_candidates = [\"A_dumps\"],\n",
    "    observations = dict(u_A_dumps=1., u_B_dumps=1., u_k=80.),\n",
    "    sample_size = 5)\n",
    "\n",
    "pollution_strict_A_HPM()\n",
    "pollution_strict_B_HPM()\n",
    "\n",
    "print(\n",
    "pollution_strict_A_HPM.existential_but_for,\n",
    "pollution_strict_B_HPM.existential_but_for\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, $B$ is not even a part of an actual cause, as the only antecedent set containing it that satisfies the existential but-for condition is the one containing both `A_dumps` and `B_dumps`, but already the singleton containing `A_dumps` is an actual cause, so the two-element set fails to satisfy the minimality condition. So the responsibility of $B$ in this case is 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how uncertainty about $A$, $B$, and $k$ might propagate and allow for an estimation of actual causality probability and of blame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_causality = []\n",
    "B_causality = []\n",
    "A_responsibility = []\n",
    "B_responsibility = []\n",
    "\n",
    "steps = 200\n",
    "\n",
    "\n",
    "for run in range(0,steps):\n",
    "\n",
    "    amount_A = pyro.sample(\"amount_A\", dist.Normal(100., 25.))\n",
    "    amount_B = pyro.sample(\"amount_B\", dist.Normal(60., 25.))\n",
    "\n",
    "\n",
    "    def pollution_model():\n",
    "        u_A_dumps = pyro.sample(\"u_A_dumps\", dist.Bernoulli(0.5))\n",
    "        u_B_dumps = pyro.sample(\"u_B_dumps\", dist.Bernoulli(0.5))\n",
    "        u_k = pyro.sample(\"u_k\", dist.Normal(80., 10.))\n",
    "\n",
    "        A_dumps = pyro.deterministic(\"A_dumps\", u_A_dumps, event_dim=0)\n",
    "        B_dumps = pyro.deterministic(\"B_dumps\", u_B_dumps, event_dim=0)\n",
    "        k = pyro.deterministic(\"k\", u_k, event_dim=0)\n",
    "\n",
    "        total_pollution = pyro.deterministic(\"total_pollution\",\n",
    "                                            A_dumps * amount_A + B_dumps * amount_B)\n",
    "        dead = pyro.deterministic(\"dead\", total_pollution > k)\n",
    "\n",
    "        return {\"total_pollution\": total_pollution, \"k\": k, \"dead\": dead}\n",
    "\n",
    "\n",
    "    pollution_A_HPM = HalpernPearlModifiedApproximate(\n",
    "        model = pollution_model,\n",
    "        antecedents = [\"A_dumps\"],\n",
    "        outcome = \"dead\",\n",
    "        witness_candidates = [\"B_dumps\"],\n",
    "        observations = dict(u_A_dumps=1., u_B_dumps=1.),\n",
    "        sample_size = 5)\n",
    "\n",
    "    pollution_B_HPM = HalpernPearlModifiedApproximate(\n",
    "        model = pollution_model,\n",
    "        antecedents = [\"B_dumps\"],\n",
    "        outcome = \"dead\",\n",
    "        witness_candidates = [\"A_dumps\"],\n",
    "        observations = dict(u_A_dumps=1., u_B_dumps=1.),\n",
    "        sample_size = 5)\n",
    "\n",
    "\n",
    "    pollution_A_HPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = pollution_model,\n",
    "    nodes = [\"A_dumps\", \"B_dumps\"],\n",
    "    antecedent = \"A_dumps\", outcome = \"dead\",\n",
    "    observations =  dict(u_A_dumps=1., u_B_dumps=1.), \n",
    "    runs_n=100\n",
    "    )\n",
    "\n",
    "    pollution_B_HPR = HalpernPearlResponsibilityApproximate(\n",
    "    model = pollution_model,\n",
    "    nodes = [\"A_dumps\", \"B_dumps\"],\n",
    "    antecedent = \"B_dumps\", outcome = \"dead\",\n",
    "    observations =  dict(u_A_dumps=1., u_B_dumps=1.), \n",
    "    runs_n=100\n",
    "    )\n",
    "\n",
    "\n",
    "    pollution_A_HPM()\n",
    "    pollution_B_HPM()\n",
    "\n",
    "    pollution_A_HPR()\n",
    "    pollution_B_HPR()\n",
    "\n",
    "    A_causality.append(pollution_A_HPM.existential_but_for)\n",
    "    B_causality.append(pollution_B_HPM.existential_but_for)\n",
    "    A_responsibility.append(pollution_A_HPR.responsibility)\n",
    "    B_responsibility.append(pollution_B_HPR.responsibility)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.mean( A_causality)\n",
    "np.mean( B_causality)\n",
    "\n",
    "comb = A_causality + B_causality\n",
    "data = {\"company\": [\"A\"]*steps + [\"B\"]*steps, \"value\": comb}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.countplot(data=df, x='value', hue='company')\n",
    "\n",
    "ax.set_title('Pr(A):' +  str(np.mean( A_causality)) +\n",
    "                \" Pr(B):\" + str(np.mean( B_causality))\n",
    "                 )\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# despite the fact that uncertainties are expressed\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# in terms of normal distributions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# the range of responsibility values is quite narrow\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# because of the crude nature of the calculations\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# which involve division by a the number of nodes\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m A_r_mean \u001b[39m=\u001b[39m mean(A_responsibility)\n\u001b[1;32m      8\u001b[0m B_r_mean \u001b[39m=\u001b[39m mean(B_responsibility)\n\u001b[1;32m     10\u001b[0m categories \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(A_responsibility \u001b[39m+\u001b[39m B_responsibility)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "# despite the fact that uncertainties are expressed\n",
    "# in terms of normal distributions\n",
    "# the range of responsibility values is quite narrow\n",
    "# because of the crude nature of the calculations\n",
    "# which involve division by a the number of nodes\n",
    "\n",
    "A_r_mean = mean(A_responsibility)\n",
    "B_r_mean = mean(B_responsibility)\n",
    "\n",
    "categories = np.unique(A_responsibility + B_responsibility)\n",
    "A_counts = [A_responsibility.count(cat) for cat in categories]\n",
    "B_counts = [B_responsibility.count(cat) for cat in categories]\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "bar_positions_A = np.arange(len(categories))\n",
    "bar_positions_B = bar_positions_A + bar_width\n",
    "\n",
    "plt.bar(bar_positions_A, A_counts, width=bar_width, label='Company A')\n",
    "plt.bar(bar_positions_B, B_counts, width=bar_width, label='Company B')\n",
    "\n",
    "plt.xlabel('Responsibility score')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Company blame distributions (means: A=' + str(A_r_mean) + ', B=' + str(B_r_mean) + ')')\n",
    "\n",
    "plt.xticks(bar_positions_A + bar_width / 2, categories)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
