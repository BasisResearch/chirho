{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable reasoning with ChiRho (categorical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **Explainable Reasoning with ChiRho** package aims to provide a systematic, unified approach to causal explanation computations in terms of different probabilistic queries over expanded causal models that are constructed from a single generic program transformation applied to an arbitrary causal model represented as a ChiRho program. The approach of reducing causal queries to probabilistic computations on transformed causal models is the foundational idea behind all of ChiRho. The key strategy underlying \"causal explanation\" queries is their use of auxiliary variables representing uncertainty over which interventions or preemptions to apply, implicitly inducing a search space over counterfactuals.\n",
    "\n",
    "The goal of this notebook is to illustrate how the package can be used to provide approximate method of answering a range of causal explanation queries with respect to models in which the key role is played by categorical variables. Continuous variables are in the focus of another notebook.\n",
    "\n",
    "In [another notebook](https://basisresearch.github.io/chirho/actual_causality.html) we illustrate how the module allows for a faithful reconstration of a specific notion of local explanation that inspired some of the conceptual moves underlying the current implementation  (the so-called Halpern-Pearl modified definition of actual causality [(J. Halpern, MIT Press, 2016)](https://mitpress.mit.edu/9780262537131/actual-causality/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "[Introduction and motivations](#intuitions-and-motivations)\n",
    "    \n",
    "- [The but-for condition](#the-but-for-condtition)\n",
    "\n",
    "- [Witness nodes and context-sensitivity](#witness-nodes-and-context-sensitivity)\n",
    "\n",
    "[Simplified actual causality](#simplified-actual-causality)\n",
    "\n",
    "[Probability of causation](#probability-of-causation)\n",
    "\n",
    "[Causal explanation](#causal-explanation)\n",
    "\n",
    "[Responsibility attribution](#responsibility-attribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.counterfactual.handlers.counterfactual import MultiWorldCounterfactual\n",
    "from chirho.explainable.handlers import SearchForExplanation\n",
    "                                            \n",
    "from chirho.indexed.ops import (IndexSet, gather, indices_of) \n",
    "from chirho.interventional.handlers import do\n",
    "\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "runs_n = 5 if smoke_test else 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a model of a very simplistic situation, in which a forest fire can be cause by exactly one of two causes: a match being dropped (`match_dropped`), or a lightning (`lightning`), and either of these factors on its own is already deterministcally sufficient for the `forest_fire` to occur. In general, you think a match being dropped is more likely than a lightning (we use fairly large probabilities for the same of example transparency). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'match_dropped': tensor(0.),\n",
       " 'lightning': tensor(1.),\n",
       " 'forest_fire': tensor(1.)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ff_disjunctive():\n",
    "        match_dropped = pyro.sample(\"match_dropped\", dist.Bernoulli(0.7)) # notice uneven probs here\n",
    "        lightning = pyro.sample(\"lightning\", dist.Bernoulli(0.4))\n",
    "\n",
    "        forest_fire = pyro.deterministic(\"forest_fire\", torch.max(match_dropped, lightning), event_dim=0)\n",
    "\n",
    "        return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n",
    "\n",
    "# each run is stochastic\n",
    "ff_disjunctive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose in this particular case you know a forest fire has occured, a match has been dropped, but no lightning occured. This further assumption can be introduced by conditioning on these observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'match_dropped': tensor(1.), 'lightning': tensor(0.), 'forest_fire': tensor(1.)}\n"
     ]
    }
   ],
   "source": [
    "observations = {\"match_dropped\": torch.tensor(1.), \n",
    "                \"lightning\": torch.tensor(0.),\n",
    "                \"forest_fire\": torch.tensor(1.)}\n",
    "\n",
    "with condition(data = observations):\n",
    "    with pyro.poutine.trace() as tr:\n",
    "        ff_disjunctive()\n",
    "\n",
    "# now it is determined how things play out\n",
    "print({key: tr.trace.nodes[key][\"value\"] for key in tr.trace.nodes.keys()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a particular context like this, when you know what happened, or at least know the values of some of the variables at play, you might be interested in using the model to answer a range of causal-explanation related questions.\n",
    "\n",
    "- Did the match being dropped actually cause the fire?\n",
    "- Suppose you only know that the forest fire occured, what are the likely explanations? How likely are they?\n",
    "- Suppose you know both factors occured, to what extent should they be deemed responsible for this outcome?\n",
    "\n",
    "Let's see how these can be addressed using ChiRho. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The but-for condition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial intuition that you might have is that in that situation, `match_dropped` is a cause of `forest_fire`, because had the match not been dropped, the forest fire would not have occurred, ro, in other words, there would be no forest fire but for the match being dropped. The `Search for Explanation` handler can be used to test for this condition, and gives the expected result in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['__antecedent_match_dropped', 'match_dropped', 'lightning', 'forest_fire_factual', 'forest_fire_counterfactual', '__consequent_forest_fire', 'forest_fire'])\n"
     ]
    }
   ],
   "source": [
    "antecedents = {\"match_dropped\": 0.0}\n",
    "witnesses = {}\n",
    "consequents = {\"forest_fire\": constraints.boolean}\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:  # needed to keep track of multiple scenarios\n",
    "    with SearchForExplanation(antecedents = antecedents, \n",
    "                              witnesses = witnesses, # no witnesses, ignore for now\n",
    "                              consequents = consequents,\n",
    "                              consequent_scale= 1e-8):\n",
    "        with condition(data = observations):\n",
    "            with pyro.plate(\"sample\", 10): # run a few times\n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    ff_disjunctive()\n",
    "\n",
    "print(tr.trace.nodes.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we used `SearchForExplanation` to investigate what would have happened to the consequent(s) if we intervened on the antecedent(s) as specified, and we run the model a few times as now the run contains stochastic elements. The trace now contains more information.\n",
    "\n",
    "1.  We now randomly intervened (for now, with uniform distribution) on `match_dropped` as specified in `antecedents`. `'__antecedent_match_dropped` now contains information on whether intervention has been preempted (that is, it has value `0` if the intervention wasn't blocked in a given run, and 1 if the intervention was blocked). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "antecedents = {\"match_dropped\": 0.0, 'lightning': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__antecedent_match_dropped: tensor([0, 0, 0, 1, 1, 0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "nd = (tr.trace.nodes)\n",
    "print(\"__antecedent_match_dropped:\", nd[\"__antecedent_match_dropped\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might think that randomly prempting the intervention from happening is unnecessary complication, and in this particular case it in fact is. The functionality is however useful in general for searching for possible antecedent sets. For this simple example we could supress this by shifting the uniform preemption probability down by .5. We'll just shift it down by .1 to be able to illustrate a point about log probabilities later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__antecedent_match_dropped: tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "with MultiWorldCounterfactual() as mwc:  \n",
    "    with SearchForExplanation(antecedents = antecedents, \n",
    "                              antecedent_bias= -.1, # we drop the probability of preemption\n",
    "                              witnesses = witnesses, \n",
    "                              consequents = consequents,\n",
    "                              consequent_scale= 1e-8):\n",
    "        with condition(data = observations):\n",
    "            with pyro.plate(\"sample\", 10): \n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    ff_disjunctive()\n",
    "\n",
    "tr.trace.compute_log_prob()\n",
    "nd = (tr.trace.nodes)\n",
    "\n",
    "print(\"__antecedent_match_dropped:\", nd[\"__antecedent_match_dropped\"][\"value\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `match_dropped` and `forest_fire` now contain values for the factual and the counterfactual scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IndexSet({'match_dropped': {0, 1}})\n",
      "Antecedent Factual:\n",
      " tensor([[[[[1., 1., 1., 1., 1.]]]]])\n",
      "Antecedent Counterfactual:\n",
      " tensor([[[[[0., 0., 0., 0., 0.]]]]])\n",
      "Consequent Factual:\n",
      " tensor([[[[[1., 1., 1., 1., 1.]]]]])\n",
      "Consequent Counterfactual:\n",
      " tensor([[[[[0., 0., 0., 0., 0.]]]]])\n"
     ]
    }
   ],
   "source": [
    "with mwc: # use the same mwc context to keep track of what's what\n",
    "    print( indices_of(nd[\"match_dropped\"][\"value\"]))  # each potential upstream intervention extends the indices\n",
    "    # use the indices to pick the right values\n",
    "    antecedent_factual = gather(nd[\"match_dropped\"][\"value\"], IndexSet(**{'match_dropped': {0}}))\n",
    "    antecedent_counterfactual = gather(nd[\"match_dropped\"][\"value\"], IndexSet(**{'match_dropped': {1}}))\n",
    "\n",
    "    consequent_factual = gather(nd[\"forest_fire\"][\"value\"], IndexSet(**{'match_dropped': {0}}))\n",
    "    consequent_counterfactual = gather(nd[\"forest_fire\"][\"value\"], IndexSet(**{'match_dropped': {1}}))\n",
    "\n",
    "\n",
    "print(\"Antecedent Factual:\\n\", antecedent_factual)\n",
    "print(\"Antecedent Counterfactual:\\n\", antecedent_counterfactual)\n",
    "print(\"Consequent Factual:\\n\", consequent_factual)\n",
    "print(\"Consequent Counterfactual:\\n\", consequent_counterfactual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. While we already see that here the answer is positive, the conterfactual value of the consequent would be different, to handle more general cases, `__consequent_forest_fire` records the score assigned to whether the factual and counterfactual values of the consequent differ. We used `consequent_scale= 1e-8` which in the binary case results in `log_prob = 0` for cases in which there is a difference and to `-inf` in cases in which there isn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-inf, 0., 0., 0., 0., 0., -inf, -inf, 0., 0.]]]]])\n"
     ]
    }
   ],
   "source": [
    "with mwc: \n",
    "    print(gather(nd['__consequent_forest_fire']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {1}})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with the but-for analysis of causality, though, is that it misdiagnoses causal factors in cases that involve overdetermination. If, for instance, we ask the same question in the context in which both a match has been dropped and a lightning took place, the answer will be negative, as strictly speaking preventing the match from being dropped wouldn't have prevented the forest fire. This is a misdiagnosis, as we still would like to think that the match being dropped played a causal role.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf]]]]])\n"
     ]
    }
   ],
   "source": [
    "observations = {\"match_dropped\": torch.tensor(1.),\n",
    "                \"lightning\": torch.tensor(1.),  # we changed this line \n",
    "                \"forest_fire\": torch.tensor(1.)}\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:  \n",
    "    with SearchForExplanation(antecedents = antecedents, \n",
    "                              witnesses = witnesses, \n",
    "                              consequents = consequents,\n",
    "                              consequent_scale= 1e-8):\n",
    "        with condition(data = observations):\n",
    "            with pyro.plate(\"sample\", 10):\n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    ff_disjunctive()\n",
    "\n",
    "tr.trace.compute_log_prob()\n",
    "nd = tr.trace.nodes\n",
    "with mwc: \n",
    "    print(gather(nd['__consequent_forest_fire']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {1}})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Witness nodes and context-sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these intuitions in the forest fire example can be perhaps salaved by considering a two-membered antecedent set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]]]]])\n"
     ]
    }
   ],
   "source": [
    "antecedents = {\"match_dropped\": 0.0, 'lightning': 0.0}\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:  \n",
    "    with SearchForExplanation(antecedents = antecedents, \n",
    "                              antecedent_bias= -.5, # enforce execution of the intevention\n",
    "                              witnesses = witnesses, \n",
    "                              consequents = consequents,\n",
    "                              consequent_scale= 1e-8):\n",
    "        with condition(data = observations):\n",
    "            with pyro.plate(\"sample\", 10):\n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    ff_disjunctive()\n",
    "\n",
    "tr.trace.compute_log_prob()\n",
    "nd = tr.trace.nodes\n",
    "with mwc: \n",
    "    print(gather(nd['__consequent_forest_fire']['log_prob'], \n",
    "                IndexSet(**{'match_dropped': {1}, \"lightning\": {1}})))  \n",
    "                # note we needed to add the index for lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already suggests a more complicated picture, as it turns out we need to pay attention to membership in larger antecedent sets that would make a difference (that is one reason why we need stochasticity in antecedent candidate preemption: to search for such subsets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But even then, the but-for analysis does not pay sufficient attention to the granularity of cause sets and to actual contexts. There are assymetric cases where the efficiency of one cause prevents the efficiency of another, in which our causal attributions should also be assymetric. \n",
    "\n",
    "A simple example involves bottle shattering. Suppose Sally and Bob throw a stone at a bottle, Sally does so a bit earlier than Bob. Suppose both are perfectly accurate and the bottle shatters if hit. Sally hits, the bottle \n",
    "shatters, but Bob fails to hit, because the bottle isn’t there anymore. \n",
    "\n",
    "Sally's throw does not satisfy the but-for condition: if she didn't throw the stone the bottle would still have shattered. Of course, the combined event of Sally throwing a stone and Bob throwing a stone is a but-for cause of the bottle shattering, so in some sense we can use the but-for clause to identify the whole set as a cause. But this doesn't capture the clear assymetry involved here. Intuitively, Sally’s throw is the (actual) cause of the bottle shattering in a sense in which Bob's throw isn't.  Sally’s throw actually caused the bottle to shatter and Bob's didn't, partially because Bob’s stone actually failed to hit it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive solution to the problem, inspired by the  Pearl-Halpern definition of actual causality (which we discuss in [another notebook](https://basisresearch.github.io/chirho/actual_causality.html)) is to say that in answering actual causality queries, we need to consider what happens when part of the actual context is kept fixed. For instance, in the bottle shattering example, given the observed fact that Bob’s stone didn’t hit, in the counterfactual world in which we keep this observed fact fixed, if Sally did not throw the stone, the bottle in fact would not have shattered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, `SearchForCauses` not only allows for stochastic preemption of interventions (to approximate search through possible antecedent sets), but also stochastic witness-preeption of those nodes that are considered part of the context (these don't need to exclude each other). In a witness-preemption we ensure that the counterfactual value is identical to the factual one (and by applying it randomly to wintess node candidates we approximate search through all possible context sets). Let's define the model and apply the handler before we go through what the trace now contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stones_model():        \n",
    "    prob_sally_throws = pyro.sample(\"prob_sally_throws\", dist.Beta(1, 1))\n",
    "    prob_bill_throws = pyro.sample(\"prob_bill_throws\", dist.Beta(1, 1))\n",
    "    prob_sally_hits = pyro.sample(\"prob_sally_hits\", dist.Beta(1, 1))\n",
    "    prob_bill_hits = pyro.sample(\"prob_bill_hits\", dist.Beta(1, 1))\n",
    "    prob_bottle_shatters_if_sally = pyro.sample(\"prob_bottle_shatters_if_sally\", dist.Beta(1, 1))\n",
    "    prob_bottle_shatters_if_bill = pyro.sample(\"prob_bottle_shatters_if_bill\", dist.Beta(1, 1))\n",
    "\n",
    "    sally_throws = pyro.sample(\"sally_throws\", dist.Bernoulli(prob_sally_throws))\n",
    "    bill_throws = pyro.sample(\"bill_throws\", dist.Bernoulli(prob_bill_throws))\n",
    "\n",
    "\n",
    "    new_shp = torch.where(sally_throws == 1,prob_sally_hits, 0.0)\n",
    "\n",
    "    sally_hits = pyro.sample(\"sally_hits\",dist.Bernoulli(new_shp))\n",
    "\n",
    "    new_bhp = torch.where(\n",
    "        bill_throws.bool() & (~sally_hits.bool()),\n",
    "        prob_bill_hits,\n",
    "        torch.tensor(0.0),\n",
    "    )\n",
    "\n",
    "    bill_hits = pyro.sample(\"bill_hits\", dist.Bernoulli(new_bhp))\n",
    "\n",
    "    new_bsp = torch.where(bill_hits.bool(), prob_bottle_shatters_if_bill,\n",
    "            torch.where(sally_hits.bool(),prob_bottle_shatters_if_sally,torch.tensor(0.0),),)\n",
    "\n",
    "    bottle_shatters = pyro.sample(\"bottle_shatters\", dist.Bernoulli(new_bsp))\n",
    "\n",
    "    return {\"sally_throws\": sally_throws, \"bill_throws\": bill_throws,  \"sally_hits\": sally_hits,\n",
    "            \"bill_hits\": bill_hits,  \"bottle_shatters\": bottle_shatters,}\n",
    "\n",
    "stones_model.nodes = [\"sally_throws\",\"bill_throws\", \"sally_hits\", \"bill_hits\",\"bottle_shatters\",]\n",
    "\n",
    "def tensorize_observations(observations):\n",
    "    return {k: torch.as_tensor(v) for k, v in observations.items()}\n",
    "\n",
    "# for now, we assume the mechanisms are deterministic\n",
    "# and that both sally and bill throw stones\n",
    "observations = {\"prob_sally_throws\": 1.0, \n",
    "                \"prob_bill_throws\": 1.0,\n",
    "                \"prob_sally_hits\": 1.0,\n",
    "                \"prob_bill_hits\": 1.0,\n",
    "                \"prob_bottle_shatters_if_sally\": 1.0,\n",
    "                \"prob_bottle_shatters_if_bill\": 1.0,\n",
    "                \"sally_throws\": 1.0, \"bill_throws\": 1.0}\n",
    "\n",
    "observations_tensorized = tensorize_observations(observations)\n",
    "\n",
    "# instead of directly specifying an alternative scenario for the antecedent\n",
    "# we can pass a constraint and at each run\n",
    "# an intervened value is proposed automatically\n",
    "# by sampling from an appropriate distribution\n",
    "\n",
    "antecedents = {\"sally_hits\": constraints.boolean}\n",
    "antencedent_bias = 0.1\n",
    "witnesses = {\"bill_throws\": constraints.boolean, \"bill_hits\": constraints.boolean}\n",
    "consequents = {\"bottle_shatters\": constraints.boolean}\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:\n",
    "    with SearchForExplanation(antecedents = antecedents, \n",
    "                       witnesses = witnesses, consequents = consequents,\n",
    "                       consequent_scale= 1e-8):\n",
    "        with condition(data = observations_tensorized):\n",
    "            with pyro.plate(\"sample\", 100):\n",
    "                with pyro.poutine.trace() as tr:\n",
    "                    stones_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified actual causality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of causation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsibility attribution\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
