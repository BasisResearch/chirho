{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Deep structural causal model counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union, TypeVar\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.nn import PyroModule, PyroSample, PyroParam\n",
    "from pyro.contrib.autoname import scope\n",
    "from pyro.poutine import condition, reparam\n",
    "\n",
    "import causal_pyro\n",
    "from causal_pyro.query.do_messenger import do\n",
    "from causal_pyro.counterfactual.handlers import Factual, MultiWorldCounterfactual, TwinWorldCounterfactual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Normalizing flows and counterfactuals\n",
    "\n",
    "Much of the causal inference literature has focused on relatively simple\n",
    "causal models with low dimensional data. In order to perform\n",
    "counterfactual reasoning in more complex domains with high dimensional\n",
    "data, Palowski et al. [@pawlowski2020deep] introduced *deep structural\n",
    "causal models* (Deep SCMs): SCMs with neural networks as the functional\n",
    "mechanisms between variables.\n",
    "\n",
    "Specifically, the neural networks are\n",
    "*normalizing flows*. A normalizing flow transforms a base probability\n",
    "distribution (often a simple distribution, such as a multivariate\n",
    "Gaussian) through a sequence of invertible transformations into a more\n",
    "complex distribution (such as a distribution over images). When used\n",
    "within a Deep SCM, the flow's base distribution is an exogenous noise\n",
    "variable, and its output is an endogenous variable.\n",
    "\n",
    "A salient property\n",
    "of normalizing flows is that computing the likelihood of data can be\n",
    "done both exactly and efficiently, and hence training a flow to model a\n",
    "data distribution through maximum likelihood is straightforward. In\n",
    "addition, the inverse of a normalizing flow can also typically be\n",
    "efficiently computed, which renders the abduction step of a\n",
    "counterfactual---inferring the posterior over exogenous variables given\n",
    "evidence---trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deep structural causal model\n",
    "\n",
    "The following code models morphological transformations of MNIST,\n",
    "defining a causal generative model over digits that contains endogenous\n",
    "variables to control the width $t$ and intensity $i$ of the stroke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSCM(PyroModule):\n",
    "\n",
    "    def forward(self):\n",
    "        U = pyro.sample(\"U\", dist.Normal(0, 1))\n",
    "        T = pyro.sample(\"T\", ...)\n",
    "        I = pyro.sample(\"I\", ...)  # I(T)\n",
    "        X = self.f_X(U, I, T)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query: counterfactual data generation\n",
    "\n",
    "Next we ask a *counterfactual* question: given an observed digit $X$, what\n",
    "would the digit have been had $t$ been $t + 1$?\n",
    "\n",
    "To compute this quantity we would normally:\n",
    "   1. invert the model to find latent exogenous noise $u$\n",
    "   2. construct an intervened model\n",
    "   3. re-simulate the forward model on the $u$ [@pearl2011algorithmization].  \n",
    "\n",
    "However, we can equivalently\n",
    "represent this process with inference in a single, expanded\n",
    "probabilistic program containing two copies of every deterministic\n",
    "statement (a so-called \\\"twin network\\\" representation of\n",
    "counterfactuals, first described in Chapter 7 of [@pearl] and extended\n",
    "to the PPL setting in [@tavares_2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_scm_query(model: DeepSCM):\n",
    "    return do(actions={\"T\": 1})(\n",
    "        condition(data={\"X\": x_obs})(\n",
    "            TwinWorldCounterfactual(dim=-1)(\n",
    "                reparam(config=pyro.infer.reparam.AutoReparam())(\n",
    "                    model))))\n",
    "\n",
    "cf_model = deep_scm_query(DeepSCM())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all counterfactuals, this estimand is not identified in general\n",
    "without further assumptions: learning parameters $\\theta$ that match\n",
    "observed data does not guarantee that the counterfactual distribution\n",
    "will match that of the true causal model. \n",
    "\n",
    "However, as discussed in the\n",
    "original paper [@pawlowski2020deep] in the context of modeling MRI\n",
    "images, there are a number of valid practical reasons one might wish to\n",
    "compute it anyway, such as explanation or expert evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Inductive Biases for Identifiable Counterfactuals\n",
    "\n",
    "Consider two types of counterfactuals; one of the form *\\\"Given $X$ was\n",
    "observed as $x$, what would $Y$ have been had $X$ been $x'$\\\"* and\n",
    "another of the form *\\\"Given $X$ was observed as $x$ and $Y$ was\n",
    "observed as $y$, what would $Y$ have been had $X$ been $x'$?\\\"*.\n",
    "\n",
    "The former type is potentially identifiable without an SCM using\n",
    "observational or interventional data [@pearl; @richardson2013single].\n",
    "*Effect of treatment on the treated* is a useful example of the former\n",
    "that is widely useful.\n",
    "\n",
    "The second type of counterfactual is called a\n",
    "\"twin-world counterfactual\\\" because it predicts $Y$ in a world where we\n",
    "*do* $X=x'$ conditional on information from a world where $X=x$ already\n",
    "caused $Y=y$ [@pearl].\n",
    "\n",
    "Structural counterfactuals enable interesting\n",
    "counterfactual quantities such as probability of necessity and\n",
    "sufficiency, and have interesting applications in generative\n",
    "explanations as well as quantifying regret, blame, and responsibility in\n",
    "decision theory and agent modeling.\n",
    "\n",
    "However, inferring twin-world counterfactual counterfactuals require an explicit SCM.\n",
    "Further, if the SCM is misspecified, it can produce incorrect counterfactual inferences\n",
    "even if it is a perfect statistical fit for observational and\n",
    "interventional data.\n",
    "\n",
    "How do we do select the \"right\\\" surrogate SCM?\n",
    "\n",
    "### Reparameterization Tricks\n",
    "\n",
    "A tempting approach is to simply convert a directed generative model\n",
    "into an SCM using \"reparameterization tricks\\\"\n",
    "[@kingma2015variational; @jang2016categorical], methods that shunt\n",
    "randomness to *exogenous* variables to facilitate back-propagation\n",
    "through *endogenous* variables.\n",
    "\n",
    "The problem is that in general a causal generative model can yield\n",
    "different SCMs depending on how it is reparameterized, and the different\n",
    "SCMs might yield different counterfactual inferences. To illustrate,\n",
    "consider the following causal generative model.\n",
    "\n",
    "    def cgmodel():\n",
    "      $x_1 \\sim$ Bernoulli(0.5)\n",
    "      $x_2 \\sim$ Bernoulli(0.5)\n",
    "      $y \\sim$ Categorical$(p_{x_1, x_2}=g(x_1, x_2))$\n",
    "\n",
    "Suppose we wished to \"reparameterize\\\" this into an SCM. To accomplish\n",
    "this, we shunt the randomness in the Bernoulli and categorical\n",
    "distributions to *exogenous* variables $N_{X_1}$, $N_{X_2}$, and $N_{Y}$\n",
    "through deterministic transformations $f_{X_1}$, $f_{X_2}$, and $f_{Y}$.\n",
    "\n",
    "    def true_dgp():\n",
    "      $n_{x_1} \\sim \\text{Bernoulli}(0.5)$\n",
    "      $n_{x_2} \\sim \\text{Bernoulli}(0.5)$\n",
    "      $n_y \\sim \\text{UniformDiscrete}([0, 1, 2])$\n",
    "      $x_1 = f_{X_1}(n_{x_1}})$\n",
    "      $x_2 = f_{X_2}(n_{x_2}})$\n",
    "      $y = f_{Y}(x_1, x_2, n_y)$\n",
    "\n",
    "In this case, $f_{Y,a}$ and $f_{Y,b}$ are two different alternatives for\n",
    "$f_Y$ above that would each yield the same observational and\n",
    "interventional distributions:\n",
    "\n",
    "    def $f_{Y,a}(x_1, x_2, n_y)$:\n",
    "        if ($x_1$ != $x_2$):\n",
    "            if($n_y$ == 0):\n",
    "                return $x_1$\n",
    "            else:\n",
    "                return $x_2$\n",
    "        else:\n",
    "            return $n_y$\n",
    "\n",
    "    def $f_{Y,b}(x_1, x_2, n_y)$:\n",
    "        if ($x_1$ != $x_2$):\n",
    "            if($n_y$ == 0):\n",
    "                return $x_1$\n",
    "            else:\n",
    "                return $x_2$\n",
    "        else:\n",
    "            return $2-n_y$\n",
    "\n",
    "However, they would produce different counterfactual inferences. Suppose\n",
    "we conditioned scmmodel on the observation\n",
    "$\\{X_1 = 1, X_2 = 0, Y = 0 \\}$ and we are interested in the\n",
    "counterfactual query \"what would $Y$ have been if $X_1$ had been 0?\" In\n",
    "this degenerate case, we would infer a point value $N_Y = 0$. When we\n",
    "re-execute the model after setting both $N_Y$ and $X_1$ to 0, $f_{Y,a}$\n",
    "would yield 0, and $f_{Y,b}$ would yield 2.\n",
    "\n",
    "### Monotonicity as Inductive Bias {#sec:monotonicity}\n",
    "\n",
    "One solution is to limit ourselves to reparameterizations where key\n",
    "counterfactual queries are identifiable from observational and\n",
    "interventional data. [@pearl] named this constraint *monotonicity* and\n",
    "defined it for the binary outcome case. [@oberst2019counterfactual]\n",
    "extended the definition to categorical variables and showed that the\n",
    "Gumbel-softmax reparameterization trick ([@jang2016categorical])\n",
    "produced a monotonic SCM. [@ness2019integrating] extended the definition\n",
    "to binomial and Poisson outcomes and provided a probabilistic\n",
    "programming implementation.\n",
    "\n",
    "In machine learning, we often talk about the inductive bias of a model,\n",
    "such as how convolutional neural networks with max-pooling favor\n",
    "translation invariance. In contrast to inductive biases implicit in\n",
    "architecture, a strength of the probabilistic programming community is\n",
    "that we favor explicit inductive biases, i.e. constraining inference\n",
    "with domain knowledge built into the model.\n",
    "\n",
    "Monotonicity is an example of an explicit inductive bias. To illustrate,\n",
    "suppose Anne has the flu but still goes to work. Jon is exposed to Anne\n",
    "($X_1 = 1$) and and a few days later, Jon got the flu ($Y = 1)$. Jon is\n",
    "may or may not have had exposure to the flu on the bus ($X_2$), which is\n",
    "unknown. Given knowledge that Jon was exposed to Anne and he got the\n",
    "flu, what are the chances he wouldn't have gotten the flu if Anne had\n",
    "stayed home ($P(Y_{X_1=0}=0|X_1=1, Y=1)$)?\n",
    "\n",
    "Given sufficient data, we could build a good probabilistic model of\n",
    "$P(X_1, X_2, Y)$. Theoretically we know that if we were to apply a\n",
    "monotonic reparameterization (specifically with respect to $X_1$ and\n",
    "$Y$) to an SCM then we could use that model to infer the above\n",
    "counterfactual. How would we know if monotonicity is a valid\n",
    "counterfactual inductive bias in this case?\n",
    "\n",
    "We can answer with a simple thought experiment. Is it conceivable that\n",
    "some strange group of coworkers could have the flu, but then *be cured\n",
    "by* exposure to Anne? That would be a case of non-monotonicity. In this\n",
    "case that is implausible, and thus monotonicity is a safe assumption.\n",
    "\n",
    "Suppose, however, that $X_1$ were email promotion and $Y$ were sales,\n",
    "and we were interested in if John would have bought a product if he\n",
    "hadn't seen an email promotion. In our thought experiment we would ask\n",
    "it is plausible for some people who intended to buy a product to be\n",
    "annoyed enough by an email promotion that they then decided not to buy.\n",
    "In that case, monotonicity would not be a safe assumption, since\n",
    "non-monotonicity is plausible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
