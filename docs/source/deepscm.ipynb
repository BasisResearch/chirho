{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Deep structural causal model counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb off\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple, Union, TypeVar\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "import torch\n",
    "import skimage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pyro\n",
    "import pyro.infer\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.transforms as Transforms\n",
    "from pyro.nn import PyroParam, PyroSample, PyroModule\n",
    "\n",
    "import causal_pyro\n",
    "from causal_pyro.query.do_messenger import do\n",
    "from causal_pyro.counterfactual.handlers import Factual, MultiWorldCounterfactual\n",
    "from causal_pyro.reparam.soft_conditioning import TransformInferReparam\n",
    "\n",
    "pyro.clear_param_store()\n",
    "pyro.settings.set(module_local_params=True)\n",
    "pyro.set_rng_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Normalizing flows and counterfactuals\n",
    "\n",
    "Much of the causal inference literature has focused on relatively simple\n",
    "causal models with low dimensional data. In order to perform\n",
    "counterfactual reasoning in more complex domains with high dimensional\n",
    "data, Palowski et al. [@pawlowski2020deep] introduced *deep structural\n",
    "causal models* (Deep SCMs): SCMs with neural networks as the functional\n",
    "mechanisms between variables.\n",
    "\n",
    "Specifically, the neural networks are\n",
    "*normalizing flows*. A normalizing flow transforms a base probability\n",
    "distribution (often a simple distribution, such as a multivariate\n",
    "Gaussian) through a sequence of invertible transformations into a more\n",
    "complex distribution (such as a distribution over images). When used\n",
    "within a Deep SCM, the flow's base distribution is an exogenous noise\n",
    "variable, and its output is an endogenous variable.\n",
    "\n",
    "A salient property\n",
    "of normalizing flows is that computing the likelihood of data can be\n",
    "done both exactly and efficiently, and hence training a flow to model a\n",
    "data distribution through maximum likelihood is straightforward. In\n",
    "addition, the inverse of a normalizing flow can also typically be\n",
    "efficiently computed, which renders the abduction step of a\n",
    "counterfactual---inferring the posterior over exogenous variables given\n",
    "evidence---trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Morpho-MNIST\n",
    "\n",
    "We consider a synthetic dataset based on MNIST, where the image of each digit ($X$) depends on stroke thickness ($T$) and brightness ($I$) of the image and the thickness depends on brightness as well.\n",
    "\n",
    "We assume we know full causal structure (i.e., there are no unconfounded variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_idx(path: str) -> np.ndarray:\n",
    "    with (gzip.open(path, 'rb') if path.endswith('.gz') else open(path, 'rb')) as f:\n",
    "        idx_dtype, ndim = struct.unpack('BBBB', f.read(4))[2:]\n",
    "        shape = struct.unpack('>' + 'I' * ndim, f.read(4 * ndim))\n",
    "        buffer_length = int(np.prod(shape))\n",
    "        return np.frombuffer(f.read(buffer_length), dtype=np.uint8).reshape(shape).astype(np.float32)\n",
    "    \n",
    "DATA_PATH = os.path.join(os.getcwd(), \"../datasets/morphomnist/\")\n",
    "DIGIT = 5\n",
    "\n",
    "raw_labels = load_idx(os.path.join(DATA_PATH, \"train-labels-idx1-ubyte.gz\"))\n",
    "raw_images = load_idx(os.path.join(DATA_PATH, \"train-images-idx3-ubyte.gz\"))\n",
    "raw_metrics = pd.read_csv(os.path.join(DATA_PATH, \"train-morpho.csv\"), index_col= 'index')\n",
    "raw_thickness = np.array(raw_metrics[\"thickness\"])[..., None]\n",
    "raw_intensity = np.array(raw_metrics[\"intensity\"])[..., None]\n",
    "raw_reduced_images = skimage.measure.block_reduce(raw_images, block_size=(1, 2, 2))\n",
    "\n",
    "digit_indices = (raw_labels == DIGIT) if DIGIT is not None else (raw_labels == raw_labels)\n",
    "\n",
    "labels = torch.tensor(raw_labels[digit_indices]).to(torch.long).requires_grad_(False).detach()\n",
    "images = torch.tensor(raw_reduced_images[np.broadcast_to(digit_indices[..., None, None], raw_reduced_images.shape)].reshape(-1, 1, *raw_reduced_images.shape[-2:])).to(torch.float32).requires_grad_(False).detach()\n",
    "thickness = torch.tensor(raw_thickness[digit_indices]).to(torch.float32).requires_grad_(False).detach()\n",
    "intensity = torch.tensor(raw_intensity[digit_indices]).to(torch.float32).requires_grad_(False).detach()\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(4):\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.imshow(images[i, 0])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deep structural causal model\n",
    "\n",
    "The following code models morphological transformations of MNIST,\n",
    "defining a causal generative model over digits that contains endogenous\n",
    "variables to control the width $t$ and intensity $i$ of the stroke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantParamTransformModule(dist.torch_transform.TransformModule):\n",
    "    def __init__(self, transform: Transforms.Transform):\n",
    "        super().__init__()\n",
    "        self._transform = transform\n",
    "        self.domain = transform.domain\n",
    "        self.codomain = transform.codomain\n",
    "        self.bijective = transform.bijective\n",
    "        \n",
    "    @property\n",
    "    def sign(self):\n",
    "        return self._transform.sign\n",
    "        \n",
    "    def _call(self, x):\n",
    "        return self._transform(x)\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        return self._transform.inv(y)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return self._transform.log_abs_det_jacobian(x, y)\n",
    "    \n",
    "    def with_cache(self, *args):\n",
    "        return self._transform.with_cache(*args)\n",
    "\n",
    "\n",
    "class ComposeTransformModule(Transforms.ComposeTransformModule):\n",
    "    def __init__(self, transforms: List[Transforms.Transform]):\n",
    "        super().__init__([\n",
    "            ConstantParamTransformModule(t) if not isinstance(t, torch.nn.Module) else t for t in transforms\n",
    "        ])\n",
    "        \n",
    "\n",
    "class InverseConditionalTransformModule(dist.conditional.ConditionalTransformModule):\n",
    "    \n",
    "    def __init__(self, transform: dist.conditional.ConditionalTransform):\n",
    "        super().__init__()\n",
    "        self._transform = transform\n",
    "    \n",
    "    @property\n",
    "    def inv(self) -> dist.conditional.ConditionalTransform:\n",
    "        return self._transform\n",
    "    \n",
    "    def condition(self, context: torch.Tensor):\n",
    "        return self._transform.condition(context).inv\n",
    "\n",
    "\n",
    "class ConditionalComposeTransformModule(dist.conditional.ConditionalTransformModule):\n",
    "    def __init__(self, transforms: List):\n",
    "        self.transforms = [\n",
    "            dist.conditional.ConstantConditionalTransform(t)\n",
    "            if not isinstance(t, dist.conditional.ConditionalTransform)\n",
    "            else t\n",
    "            for t in transforms\n",
    "        ]\n",
    "        super().__init__()\n",
    "        # for parameter storage...  TODO is this necessary?\n",
    "        self._transforms_module = torch.nn.ModuleList([t for t in transforms if isinstance(t, torch.nn.Module)])\n",
    "        \n",
    "    @property\n",
    "    def inv(self):\n",
    "        return InverseConditionalTransformModule(self)\n",
    "\n",
    "    def condition(self, context: torch.Tensor):\n",
    "        return ComposeTransformModule([t.condition(context) for t in self.transforms]).with_cache(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThicknessTransform(ComposeTransformModule):\n",
    "    def __init__(self, thickness_size: int, weight: float, bias: float):\n",
    "        self.thickness_size = thickness_size\n",
    "        super().__init__([\n",
    "            Transforms.Spline(thickness_size, bound=1.),\n",
    "            Transforms.AffineTransform(loc=bias, scale=weight),\n",
    "            Transforms.biject_to(dist.constraints.positive),\n",
    "        ])\n",
    "\n",
    "class IntensityTransform(ConditionalComposeTransformModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        intensity_size: int,\n",
    "        thickness_size: int,\n",
    "        hidden_dims: List[int],\n",
    "        weight: float,\n",
    "        bias: float,\n",
    "        *,\n",
    "        count_bins: int = 8,\n",
    "        nonlinearity=torch.nn.ReLU(),\n",
    "    ):\n",
    "        self.intensity_size = intensity_size\n",
    "        self.thickness_size = thickness_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        \n",
    "        intensity_nn = pyro.nn.DenseNN(\n",
    "            thickness_size,\n",
    "            hidden_dims,\n",
    "            param_dims=[\n",
    "                intensity_size * count_bins,\n",
    "                intensity_size * count_bins,\n",
    "                intensity_size * (count_bins - 1),\n",
    "                intensity_size * count_bins,\n",
    "            ],\n",
    "            nonlinearity=nonlinearity,\n",
    "        )\n",
    "        super().__init__([\n",
    "            Transforms.ConditionalSpline(intensity_nn, intensity_size, count_bins),\n",
    "            Transforms.SigmoidTransform(),\n",
    "            Transforms.AffineTransform(loc=bias, scale=weight),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformation for the images is somewhat involved. Much of the neural network architecture is taken from [this PyTorch tutorial] on normalizing flows, which readers are encouraged to peruse for further background on normalizing flows in general and this architecture in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Activation function that applies ELU in both direction (inverted and plain).\n",
    "    Allows non-linearity while providing strong gradients for any input (important for final convolution)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.cat([torch.nn.functional.elu(x), torch.nn.functional.elu(-x)], dim=-3)\n",
    "\n",
    "\n",
    "class LayerNormChannels(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        This module applies layer norm across channels in an image.\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            eps - Small constant to stabilize std\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, c_in, 1, 1))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, c_in, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-3, keepdim=True)\n",
    "        var = x.var(dim=-3, unbiased=False, keepdim=True)\n",
    "        y = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = y * self.gamma + self.beta\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedConv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_hidden: int):\n",
    "        \"\"\"\n",
    "        This module applies a two-layer convolutional ResNet block with input gate\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_in, c_hidden, kernel_size=3, padding=1),\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_hidden, 2*c_in, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = out.chunk(2, dim=-3)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "\n",
    "class GatedConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_hidden: int, num_layers: int, eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Module that summarizes the previous blocks to a full convolutional neural network.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_hidden - Number of hidden dimensions to use within the network\n",
    "            c_out - Number of output channels.\n",
    "            num_layers - Number of gated ResNet blocks to apply\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_out = 2 * c_in\n",
    "        layers = []\n",
    "        layers += [torch.nn.Conv2d(c_in, c_hidden, kernel_size=3, padding=1)]\n",
    "        for layer_index in range(num_layers):\n",
    "            layers += [\n",
    "                GatedConv(c_hidden, c_hidden),\n",
    "                LayerNormChannels(c_hidden, eps=eps)\n",
    "            ]\n",
    "        layers += [\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_hidden, c_out, kernel_size=3, padding=1)\n",
    "        ]\n",
    "        self.nn = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "\n",
    "class MaskedAffineCoupling(dist.torch_transform.TransformModule):\n",
    "    bijective = True\n",
    "    domain = dist.constraints.independent(dist.constraints.real, 3)\n",
    "    codomain = dist.constraints.independent(dist.constraints.real, 3)\n",
    "\n",
    "    def __init__(self, network: torch.nn.Module, mask: torch.Tensor, c_in: int, h: int, w: int):\n",
    "        \"\"\"\n",
    "        Coupling layer inside a normalizing flow.\n",
    "        Inputs:\n",
    "            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.\n",
    "                      Output shape should be twice the channel size as the input.\n",
    "            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n",
    "                   while 1 means the latent will be used as input to the NN.\n",
    "            c_in - Number of input channels\n",
    "        \"\"\"\n",
    "        self.c_in = c_in\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.scaling_factor = torch.nn.Parameter(torch.zeros(c_in, device=mask.device))\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def with_cache(self, *args):\n",
    "        return self\n",
    "        \n",
    "    def _net(self, x):\n",
    "        s, t = self.network(x * self.mask).chunk(2, dim=-3)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform parts where self.mask == 0)\n",
    "        return s * (1 - self.mask), t * (1 - self.mask)\n",
    "        \n",
    "    def log_abs_det_jacobian(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        s, _ = self._net(x)\n",
    "        return s.sum(dim=[-1,-2,-3])\n",
    "    \n",
    "    def _inverse(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        s, t = self._net(y)\n",
    "        return (y * torch.exp(-s)) - t\n",
    "    \n",
    "    def _call(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        s, t = self._net(x)\n",
    "        return (x + t) * torch.exp(s)\n",
    "\n",
    "\n",
    "class ImageTransform(ConditionalComposeTransformModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        im_size: int,\n",
    "        input_channels: int,\n",
    "        thickness_size: int,\n",
    "        intensity_size: int,\n",
    "        num_blocks: int,\n",
    "        layers_per_block: int,\n",
    "        hidden_channels: int,\n",
    "        *,\n",
    "        num_cond_blocks: int = 1,\n",
    "        alpha: float = 1e-5,\n",
    "        bn_momentum: float = 0.05,\n",
    "        ln_momentum: float = 1e-5,\n",
    "        nonlinearity = torch.nn.ReLU(),\n",
    "    ):\n",
    "        self.im_size = im_size\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.layers_per_block = layers_per_block\n",
    "        \n",
    "        self.num_cond_blocks = num_cond_blocks\n",
    "        \n",
    "        self.flat_input_size = input_channels * im_size * im_size\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        # dequantization\n",
    "        layers += [\n",
    "            Transforms.IndependentTransform(\n",
    "                Transforms.ComposeTransform([\n",
    "                    Transforms.AffineTransform(0., 1. / 256),\n",
    "                    Transforms.AffineTransform(alpha, (1 - alpha)),\n",
    "                    Transforms.SigmoidTransform().inv,\n",
    "            ]), 3)\n",
    "        ]\n",
    "        \n",
    "        # image flow with convolutional blocks\n",
    "        for i in range(num_blocks):\n",
    "            layers += [\n",
    "                MaskedAffineCoupling(\n",
    "                    GatedConvNet(input_channels, hidden_channels, layers_per_block, eps=ln_momentum),\n",
    "                    self.create_checkerboard_mask(im_size, im_size, invert=(i%2==1)),\n",
    "                    input_channels,\n",
    "                    im_size,\n",
    "                    im_size,\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "        # conditioning on context\n",
    "        layers += [Transforms.ReshapeTransform((input_channels, im_size, im_size), (self.flat_input_size,))]\n",
    "        for i in range(self.num_cond_blocks):\n",
    "            layers += [\n",
    "                Transforms.ConditionalAffineAutoregressive(\n",
    "                    pyro.nn.ConditionalAutoRegressiveNN(\n",
    "                        self.flat_input_size,\n",
    "                        thickness_size + intensity_size,\n",
    "                        [2 * self.flat_input_size] * 2,\n",
    "                        nonlinearity=nonlinearity,\n",
    "                        skip_connections=True,\n",
    "                    ),\n",
    "                ),\n",
    "            ]  \n",
    "        layers += [Transforms.ReshapeTransform((self.flat_input_size,), (input_channels, im_size, im_size))]\n",
    "        \n",
    "        super().__init__(layers)\n",
    "            \n",
    "    @staticmethod\n",
    "    def create_checkerboard_mask(h: int, w: int, invert=False):\n",
    "        x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "        xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "        mask = torch.fmod(xx + yy, 2).to(torch.float32).view(1, 1, h, w)\n",
    "        return mask if not invert else (1. - mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all of these components defined, we can finally define the high-level causal model we'll be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"333pt\" height=\"205pt\"\n",
       " viewBox=\"0.00 0.00 332.50 205.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 201)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-201 328.5,-201 328.5,4 -4,4\"/>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"54\" cy=\"-170.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-90\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">I</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;I -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;I</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.27,-152.85C44.73,-142.55 40.1,-129.09 36.07,-117.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"39.28,-115.94 32.72,-107.62 32.66,-118.22 39.28,-115.94\"/>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"54\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"54\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;X -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>T&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.15,-152.61C59.28,-140.29 61.9,-123.17 63,-108 64.16,-92.04 64.34,-87.94 63,-72 62.28,-63.5 60.93,-54.31 59.49,-46.01\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.91,-45.29 57.65,-36.09 56.03,-46.56 62.91,-45.29\"/>\n",
       "</g>\n",
       "<!-- I&#45;&gt;X -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>I&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M33.4,-72.41C36.51,-64.34 40.33,-54.43 43.83,-45.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"47.13,-46.55 47.46,-35.96 40.6,-44.03 47.13,-46.55\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-181.8\" font-family=\"Times,serif\" font-size=\"14.00\">T ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-166.8\" font-family=\"Times,serif\" font-size=\"14.00\">I ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"107.5\" y=\"-151.8\" font-family=\"Times,serif\" font-size=\"14.00\">X ~ TransformedDistribution</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f98bfdabee0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DeepSCM(PyroModule):\n",
    "    \n",
    "    thickness_support = dist.constraints.positive\n",
    "    intensity_support = dist.constraints.positive\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        thickness_transform: ThicknessTransform,\n",
    "        intensity_transform: IntensityTransform,\n",
    "        image_transform: ImageTransform,\n",
    "        *,\n",
    "        include_thickness: bool = True,\n",
    "        include_intensity: bool = True,\n",
    "        include_image: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # mechanisms\n",
    "        self.thickness_transform = thickness_transform\n",
    "        self.intensity_transform = intensity_transform\n",
    "        self.image_transform = image_transform.inv  # generative direction is inverse\n",
    "        \n",
    "        # base dist buffers\n",
    "        self.register_buffer(\"base_loc\", torch.tensor(0.))\n",
    "        self.register_buffer(\"base_scale\", torch.tensor(1.))\n",
    "\n",
    "        # tensor sizes\n",
    "        self.thickness_size = self.thickness_transform.thickness_size\n",
    "        self.intensity_size = self.intensity_transform.intensity_size\n",
    "        self.im_size = self.image_transform._transform.im_size\n",
    "        self.im_input_channels = self.image_transform._transform.input_channels\n",
    "        \n",
    "        # prior masks\n",
    "        self.include_thickness = include_thickness\n",
    "        self.include_intensity = include_intensity\n",
    "        self.include_image = include_image\n",
    "\n",
    "    def StandardNormal(self, *event_shape: int) -> Union[dist.Independent, dist.Normal]:\n",
    "        return dist.Normal(self.base_loc, self.base_scale).expand(event_shape).to_event(len(event_shape))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cond_dist(\n",
    "        transform: Union[Transforms.Transform, dist.conditional.ConditionalTransform],\n",
    "        U_dist: dist.Distribution,\n",
    "        *contexts: torch.Tensor\n",
    "    ) -> dist.Distribution:\n",
    "        if contexts:\n",
    "            batch_shape = torch.broadcast_shapes(*(c.shape[:-1] for c in contexts))\n",
    "            U_dist = U_dist.expand(torch.broadcast_shapes(batch_shape, U_dist.batch_shape))\n",
    "            context = torch.cat([c.expand(batch_shape + (-1,)) for c in contexts], dim=-1)\n",
    "            transform = transform.condition(context)\n",
    "        return dist.TransformedDistribution(U_dist, transform)\n",
    "\n",
    "    def forward(self):\n",
    "        # Thickness:\n",
    "        UT_dist = self.StandardNormal(self.thickness_size)\n",
    "        T_dist = self.cond_dist(self.thickness_transform, UT_dist)\n",
    "        T = pyro.sample(\"T\", T_dist.mask(self.include_thickness))\n",
    "        T_unconstrained = Transforms.biject_to(self.thickness_support).inv(T)\n",
    "\n",
    "        # Intensity:\n",
    "        UI_dist = self.StandardNormal(self.intensity_size)\n",
    "        I_dist = self.cond_dist(self.intensity_transform, UI_dist, T_unconstrained)\n",
    "        I = pyro.sample(\"I\", I_dist.mask(self.include_intensity))\n",
    "        I_unconstrained = Transforms.biject_to(self.intensity_support).inv(I)\n",
    "\n",
    "        # Image:\n",
    "        UX_dist = self.StandardNormal(self.im_input_channels, self.im_size, self.im_size)\n",
    "        X_dist = self.cond_dist(self.image_transform, UX_dist, T_unconstrained, I_unconstrained)\n",
    "        X = pyro.sample(\"X\", X_dist.mask(self.include_image))\n",
    "\n",
    "        return X\n",
    "\n",
    "thickness_transform = ThicknessTransform(\n",
    "    thickness.shape[-1],\n",
    "    weight=thickness.log().mean().detach().item(),\n",
    "    bias=thickness.log().std().detach().item(),\n",
    ")\n",
    "\n",
    "intensity_transform = IntensityTransform(\n",
    "    intensity.shape[-1],\n",
    "    thickness.shape[-1],\n",
    "    hidden_dims=[16],\n",
    "    weight=intensity.min().detach().item(),\n",
    "    bias=(intensity.max() - intensity.min()).detach().item(),\n",
    "    nonlinearity=torch.nn.Identity(),\n",
    "    count_bins=4,\n",
    ")\n",
    "\n",
    "image_transform = ImageTransform(\n",
    "    images.shape[-1],\n",
    "    1,\n",
    "    thickness.shape[-1],\n",
    "    intensity.shape[-1],\n",
    "    num_blocks=8,\n",
    "    layers_per_block=3,\n",
    "    hidden_channels=16,\n",
    "    nonlinearity=torch.nn.ELU(),\n",
    ")\n",
    "\n",
    "model = DeepSCM(thickness_transform, intensity_transform, image_transform, include_thickness=True, include_intensity=True, include_image=True)\n",
    "pyro.render_model(model, render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"358pt\" height=\"252pt\"\n",
       " viewBox=\"0.00 0.00 357.50 251.50\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 247.5)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-247.5 353.5,-247.5 353.5,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_observations</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-235.5 116,-235.5 116,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">observations</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"61\" cy=\"-209.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"61\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"43\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">I</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;I -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;I</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M57.09,-191.47C54.78,-181.39 51.8,-168.39 49.18,-156.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"52.54,-155.96 46.9,-147 45.72,-157.53 52.54,-155.96\"/>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"62\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;X -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>T&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M67.29,-191.82C71.54,-179.61 76.77,-162.53 79,-147 81.27,-131.16 81.47,-126.81 79,-111 77.62,-102.18 75.02,-92.82 72.26,-84.46\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.51,-83.13 68.89,-74.86 68.9,-85.45 75.51,-83.13\"/>\n",
       "</g>\n",
       "<!-- I&#45;&gt;X -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>I&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M47.6,-111.05C49.75,-103.14 52.35,-93.54 54.76,-84.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"58.2,-85.36 57.44,-74.79 51.45,-83.52 58.2,-85.36\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">T ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\">I ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"132.5\" y=\"-190.8\" font-family=\"Times,serif\" font-size=\"14.00\">X ~ TransformedDistribution</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f98c07bf580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConditionedDeepSCM(PyroModule):\n",
    "    def __init__(self, model: DeepSCM):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, t_obs, i_obs, x_obs):\n",
    "        with pyro.condition(data={\"X\": x_obs, \"T\": t_obs, \"I\": i_obs}), \\\n",
    "                pyro.poutine.scale(scale=1 / x_obs.shape[0]), \\\n",
    "                pyro.plate(\"observations\", size=x_obs.shape[0], dim=-1):\n",
    "            return self.model()\n",
    "\n",
    "conditioned_model = ConditionedDeepSCM(model)\n",
    "pyro.render_model(conditioned_model, model_args=(thickness[:2], intensity[:2], images[:2]), render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | ConditionedDeepSCM | 607 K \n",
      "1 | guide | AutoDelta          | 0     \n",
      "2 | elbo  | ELBOModule         | 607 K \n",
      "---------------------------------------------\n",
      "607 K     Trainable params\n",
      "0         Non-trainable params\n",
      "607 K     Total params\n",
      "2.428     Total estimated model params size (MB)\n",
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56e421aff7c471e873abc3c462e90e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "adam_params = {\"lr\": 1e-5}\n",
    "batch_size = 128\n",
    "num_epochs = 400  #50000 // (images.shape[0] // batch_size)\n",
    "\n",
    "from torch.distributions.utils import _sum_rightmost\n",
    "\n",
    "class LightningSVI(pl.LightningModule):\n",
    "    def __init__(self, model: ConditionedDeepSCM, guide: PyroModule, elbo: pyro.infer.ELBO, optim_params: dict):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.elbo = elbo(self.model, self.guide)\n",
    "        self._optim_params = optim_params\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        t_obs, i_obs, x_obs = batch\n",
    "        x_obs = x_obs + torch.rand_like(x_obs)\n",
    "        loss = self.elbo(t_obs, i_obs, x_obs)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([\n",
    "            dict(params=self.model.model.thickness_transform.parameters()),\n",
    "            dict(params=self.model.model.intensity_transform.parameters()),\n",
    "            dict(params=self.guide.parameters()),\n",
    "            dict(params=self.model.model.image_transform.parameters(), lr=self._optim_params[\"lr\"] / self.model.model.im_size ** 2)\n",
    "        ], **self._optim_params)\n",
    "        return optimizer\n",
    "        \n",
    "guide = pyro.infer.autoguide.AutoDelta(conditioned_model)\n",
    "elbo = pyro.infer.Trace_ELBO()\n",
    "lightning_svi = LightningSVI(conditioned_model, guide, elbo, adam_params)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.TensorDataset(thickness, intensity, images),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"gpu\",\n",
    "    default_root_dir=os.path.join(\"./lightning_logs/deepscm_ckpt\", \"deepscm_joint\"),\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
    "    ],\n",
    ")\n",
    "%pdb off\n",
    "trainer.fit(model=lightning_svi, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictive = pyro.infer.Predictive(pyro.condition(model, dict(T=thickness[0:1].cuda(), I=intensity[0:1].cuda())), guide=guide, num_samples=4)\n",
    "img = predictive()[\"X\"].cpu()\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(2 * 2):\n",
    "    fig.add_subplot(2, 2, i + 1)\n",
    "    plt.imshow(img[i].reshape(*img.shape[-2:]))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query: counterfactual data generation\n",
    "\n",
    "Next we ask a *counterfactual* question: given an observed digit $X$, what\n",
    "would the digit have been had $t$ been $t + 1$?\n",
    "\n",
    "To compute this quantity we would normally:\n",
    "   1. invert the model to find latent exogenous noise $u$\n",
    "   2. construct an intervened model\n",
    "   3. re-simulate the forward model on the $u$ [@pearl2011algorithmization].  \n",
    "\n",
    "However, we can equivalently\n",
    "represent this process with inference in a single, expanded\n",
    "probabilistic program containing two copies of every deterministic\n",
    "statement (a so-called \\\"twin network\\\" representation of\n",
    "counterfactuals, first described in Chapter 7 of [@pearl] and extended\n",
    "to the PPL setting in [@tavares_2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterfactualDeepSCM(PyroModule):\n",
    "    def __init__(self, model: DeepSCM):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x_obs: torch.Tensor, i_act: torch.Tensor):\n",
    "        with MultiWorldCounterfactual(dim=-2), \\\n",
    "                plate(\"observations\", size=x_obs.shape[0], dim=-1), \\\n",
    "                do(actions={\"I\": i_act}), \\\n",
    "                condition(data={\"X\": x_obs}):\n",
    "            return model()\n",
    "\n",
    "cf_model = CounterfactualDeepSCM(model)\n",
    "pyro.render_model(cf_model, model_args=(images[:1], intensity[:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all counterfactuals, this estimand is not identified in general\n",
    "without further assumptions: learning parameters $\\theta$ that match\n",
    "observed data does not guarantee that the counterfactual distribution\n",
    "will match that of the true causal model. \n",
    "\n",
    "However, as discussed in the\n",
    "original paper [@pawlowski2020deep] in the context of modeling MRI\n",
    "images, there are a number of valid practical reasons one might wish to\n",
    "compute it anyway, such as explanation or expert evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | ImageTransform | 606 K \n",
      "-----------------------------------------\n",
      "606 K     Trainable params\n",
      "0         Non-trainable params\n",
      "606 K     Total params\n",
      "2.427     Total estimated model params size (MB)\n",
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8036262176e04d0a82623b180552cd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(1234)\n",
    "\n",
    "adam_params = {\"lr\": 1e-5}\n",
    "batch_size = 128\n",
    "num_epochs = 400  #50000 // (images.shape[0] // batch_size)\n",
    "\n",
    "class LightningRaw(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, adam_params: dict):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.register_buffer(\"base_loc\", torch.tensor(0.))\n",
    "        self.register_buffer(\"base_scale\", torch.tensor(1.))\n",
    "        self.adam_params = adam_params\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        t_obs, i_obs, x_obs = batch\n",
    "        x_obs = x_obs + torch.rand_like(x_obs)\n",
    "        #x_obs = x_obs.clamp(min=1, max=255)\n",
    "        base_dist = dist.Normal(self.base_loc, self.base_scale).expand([x_obs.shape[0], self.model.flat_input_size]).to_event(1)\n",
    "        context = torch.cat([t_obs, i_obs], dim=-1).log()\n",
    "        tfm = self.model.condition(context=context)\n",
    "        x_noise = tfm(x_obs)\n",
    "        loss = -(base_dist.log_prob(x_noise) + tfm.log_abs_det_jacobian(x_obs, x_noise)).mean() / self.model.flat_input_size\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **self.adam_params)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "raw_img_transform = ImageTransform(\n",
    "    im_size,\n",
    "    1,\n",
    "    thickness_size,\n",
    "    intensity_size,\n",
    "    num_blocks=8,\n",
    "    layers_per_block=3,\n",
    "    hidden_channels=16,\n",
    "    nonlinearity=torch.nn.ELU(),\n",
    ")\n",
    "\n",
    "raw_lightning = LightningRaw(raw_img_transform, adam_params)\n",
    "\n",
    "raw_dataloader = DataLoader(\n",
    "    torch.utils.data.TensorDataset(thickness, intensity, images),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "raw_trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator='gpu',\n",
    "    default_root_dir=os.path.join(\"./lightning_logs/deepscm_ckpt\", \"deepscm\"),\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
    "        pl.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "    ],\n",
    ")\n",
    "# raw_trainer.fit(model=raw_lightning, train_dataloaders=raw_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(256.0025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3dec521a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_lightning = raw_lightning.to(device=thickness.device)\n",
    "\n",
    "raw_img_dist = dist.TransformedDistribution(\n",
    "    dist.Normal(raw_lightning.base_loc, raw_lightning.base_scale).expand([raw_lightning.model.flat_input_size]).to_event(1),\n",
    "    raw_lightning.model.condition(\n",
    "        context=torch.cat([thickness[0:batch_size, None], intensity[0:batch_size, None]], dim=-1).log()\n",
    "    ).inv)\n",
    "raw_gen_img = raw_img_dist.sample((batch_size,)).reshape(-1, im_size, im_size)\n",
    "print(raw_gen_img.max() - raw_gen_img.min())\n",
    "\n",
    "plt.imshow(raw_gen_img[20].detach().cpu().numpy())\n",
    "#plt.imshow(images[20].detach().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bdfcdbd368c30fa36f32c37bcc33f6323158b752539e151107ac7b028e21443"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
