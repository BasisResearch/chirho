{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Deep structural causal model counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-567cd410dcf2f6fa\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-567cd410dcf2f6fa\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple, Union, TypeVar\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.poutine import condition, reparam\n",
    "from pyro.nn import PyroParam, PyroSample, PyroModule\n",
    "import pyro.distributions.transforms as Transforms\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.infer import config_enumerate\n",
    "from pyro.distributions import constraints\n",
    "\n",
    "\n",
    "import causal_pyro\n",
    "from causal_pyro.query.do_messenger import do\n",
    "from causal_pyro.counterfactual.handlers import Factual, MultiWorldCounterfactual, TwinWorldCounterfactual\n",
    "from causal_pyro.reparam.soft_conditioning import TransformInferReparam\n",
    "\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import tqdm\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import gzip\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Current device: 0\n",
      "Device count: 1\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "available = torch.cuda.is_available()\n",
    "curr_device = torch.device(\"cpu\") if not available else torch.cuda.current_device()\n",
    "print(f'Cuda available: {available}')\n",
    "print(f'Current device: {curr_device}')\n",
    "\n",
    "if available:\n",
    "    device_count = torch.cuda.device_count() \n",
    "    device_name =  torch.cuda.get_device_name(0)\n",
    "    print(f'Device count: {device_count}')\n",
    "    print(f'Device name: {device_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background: Normalizing flows and counterfactuals\n",
    "\n",
    "Much of the causal inference literature has focused on relatively simple\n",
    "causal models with low dimensional data. In order to perform\n",
    "counterfactual reasoning in more complex domains with high dimensional\n",
    "data, Palowski et al. [@pawlowski2020deep] introduced *deep structural\n",
    "causal models* (Deep SCMs): SCMs with neural networks as the functional\n",
    "mechanisms between variables.\n",
    "\n",
    "Specifically, the neural networks are\n",
    "*normalizing flows*. A normalizing flow transforms a base probability\n",
    "distribution (often a simple distribution, such as a multivariate\n",
    "Gaussian) through a sequence of invertible transformations into a more\n",
    "complex distribution (such as a distribution over images). When used\n",
    "within a Deep SCM, the flow's base distribution is an exogenous noise\n",
    "variable, and its output is an endogenous variable.\n",
    "\n",
    "A salient property\n",
    "of normalizing flows is that computing the likelihood of data can be\n",
    "done both exactly and efficiently, and hence training a flow to model a\n",
    "data distribution through maximum likelihood is straightforward. In\n",
    "addition, the inverse of a normalizing flow can also typically be\n",
    "efficiently computed, which renders the abduction step of a\n",
    "counterfactual---inferring the posterior over exogenous variables given\n",
    "evidence---trivial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Morpho-MNIST\n",
    "\n",
    "We consider a synthetic dataset based on MNIST, where the image of each digit ($X$) depends on stroke thickness ($T$) and brightness ($I$) of the image and the thickness depends on brightness as well.\n",
    "\n",
    "We assume we know full causal structure (i.e., there are no unconfounded variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_idx(path: str) -> np.ndarray:\n",
    "    with (gzip.open(path, 'rb') if path.endswith('.gz') else open(path, 'rb')) as f:\n",
    "        idx_dtype, ndim = struct.unpack('BBBB', f.read(4))[2:]\n",
    "        shape = struct.unpack('>' + 'I' * ndim, f.read(4 * ndim))\n",
    "        buffer_length = int(np.prod(shape))\n",
    "        return np.frombuffer(f.read(buffer_length), dtype=np.uint8).reshape(shape).astype(np.float32)\n",
    "    \n",
    "path = os.path.join(os.getcwd(), \"../datasets/morphomnist/\")\n",
    "raw_labels = load_idx(path+\"train-labels-idx1-ubyte.gz\")\n",
    "raw_images = load_idx(path+\"train-images-idx3-ubyte.gz\")\n",
    "raw_metrics = pd.read_csv(path + \"train-morpho.csv\", index_col= 'index')\n",
    "raw_thickness = raw_metrics[\"thickness\"]\n",
    "raw_intensity = raw_metrics[\"intensity\"]\n",
    "\n",
    "reduced_images = skimage.measure.block_reduce(raw_images, block_size=(1, 2, 2))\n",
    "\n",
    "im_size = reduced_images.shape[-1]\n",
    "thickness_size = 1\n",
    "intensity_size = 1\n",
    "\n",
    "digit = 5\n",
    "indices = raw_labels == digit\n",
    "\n",
    "images = torch.tensor(reduced_images[np.broadcast_to(indices[..., None, None], reduced_images.shape)].reshape(-1, *reduced_images.shape[-2:]), dtype=torch.float32).requires_grad_(False)\n",
    "labels = torch.tensor(raw_labels[indices], dtype=torch.float32).requires_grad_(False)\n",
    "thickness = torch.tensor(raw_metrics[\"thickness\"][indices], dtype=torch.float32).requires_grad_(False)\n",
    "intensity = torch.tensor(raw_metrics[\"intensity\"][indices], dtype=torch.float32).requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 13.5, 13.5, -0.5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "rows = 2\n",
    "columns = 2\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(images[0].cpu())\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(images[1].cpu())\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(images[2].cpu())\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "plt.imshow(images[3].cpu())\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: deep structural causal model\n",
    "\n",
    "The following code models morphological transformations of MNIST,\n",
    "defining a causal generative model over digits that contains endogenous\n",
    "variables to control the width $t$ and intensity $i$ of the stroke:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConstantParamTransformModule(dist.torch_transform.TransformModule):\n",
    "    def __init__(self, transform: Transforms.Transform):\n",
    "        super().__init__()\n",
    "        self._transform = transform\n",
    "        self.domain = transform.domain\n",
    "        self.codomain = transform.codomain\n",
    "        self.bijective = transform.bijective\n",
    "        \n",
    "    @property\n",
    "    def sign(self):\n",
    "        return self._transform.sign\n",
    "        \n",
    "    def _call(self, x):\n",
    "        return self._transform(x)\n",
    "    \n",
    "    def _inverse(self, y):\n",
    "        return self._transform.inv(y)\n",
    "    \n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        return self._transform.log_abs_det_jacobian(x, y)\n",
    "\n",
    "\n",
    "class ComposeTransformModule(Transforms.ComposeTransformModule):\n",
    "    def __init__(self, transforms: List[Transforms.Transform]):\n",
    "        super().__init__([\n",
    "            ConstantParamTransformModule(t) if not isinstance(t, torch.nn.Module) else t for t in transforms\n",
    "        ])\n",
    "        # for parameter storage...\n",
    "        self._part_modules = torch.nn.ModuleList(self.parts)\n",
    "\n",
    "\n",
    "class ConditionalComposeTransformModule(dist.conditional.ConditionalTransformModule):\n",
    "    def __init__(self, transforms: List):\n",
    "        self.transforms = [\n",
    "            dist.conditional.ConstantConditionalTransform(t)\n",
    "            if not isinstance(t, dist.conditional.ConditionalTransform)\n",
    "            else t\n",
    "            for t in transforms\n",
    "        ]\n",
    "        super().__init__()\n",
    "        # for parameter storage...\n",
    "        self._transforms_module = torch.nn.ModuleList([t for t in transforms if isinstance(t, torch.nn.Module)])\n",
    "\n",
    "    def condition(self, context: torch.Tensor):\n",
    "        return ComposeTransformModule([t.condition(context) for t in self.transforms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatELU(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Activation function that applies ELU in both direction (inverted and plain).\n",
    "    Allows non-linearity while providing strong gradients for any input (important for final convolution)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return torch.cat([torch.nn.functional.elu(x), torch.nn.functional.elu(-x)], dim=-3)\n",
    "\n",
    "\n",
    "class LayerNormChannels(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        This module applies layer norm across channels in an image.\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            eps - Small constant to stabilize std\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(1, c_in, 1, 1))\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(1, c_in, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-3, keepdim=True)\n",
    "        var = x.var(dim=-3, unbiased=False, keepdim=True)\n",
    "        y = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        y = y * self.gamma + self.beta\n",
    "        return y\n",
    "\n",
    "\n",
    "class GatedConv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_hidden: int):\n",
    "        \"\"\"\n",
    "        This module applies a two-layer convolutional ResNet block with input gate\n",
    "        Inputs:\n",
    "            c_in - Number of channels of the input\n",
    "            c_hidden - Number of hidden dimensions we want to model (usually similar to c_in)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_in, c_hidden, kernel_size=3, padding=1),\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_hidden, 2*c_in, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        val, gate = out.chunk(2, dim=-3)\n",
    "        return x + val * torch.sigmoid(gate)\n",
    "\n",
    "\n",
    "class GatedConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, c_in: int, c_hidden: int, num_layers: int, eps: float = 1e-5):\n",
    "        \"\"\"\n",
    "        Module that summarizes the previous blocks to a full convolutional neural network.\n",
    "        Inputs:\n",
    "            c_in - Number of input channels\n",
    "            c_hidden - Number of hidden dimensions to use within the network\n",
    "            c_out - Number of output channels. If -1, 2 times the input channels are used (affine coupling)\n",
    "            num_layers - Number of gated ResNet blocks to apply\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_out = 2 * c_in\n",
    "        layers = []\n",
    "        layers += [torch.nn.Conv2d(c_in, c_hidden, kernel_size=3, padding=1)]\n",
    "        for layer_index in range(num_layers):\n",
    "            layers += [\n",
    "                GatedConv(c_hidden, c_hidden),\n",
    "                LayerNormChannels(c_hidden, eps=eps)\n",
    "            ]\n",
    "        layers += [\n",
    "            ConcatELU(),\n",
    "            torch.nn.Conv2d(2*c_hidden, c_out, kernel_size=3, padding=1)\n",
    "        ]\n",
    "        self.nn = torch.nn.Sequential(*layers)\n",
    "\n",
    "        self.nn[-1].weight.data.zero_()\n",
    "        self.nn[-1].bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "class MaskedAffineCoupling(dist.torch_transform.TransformModule):\n",
    "    bijective = True\n",
    "    domain = constraints.independent(constraints.real, 3)\n",
    "    codomain = constraints.independent(constraints.real, 3)\n",
    "\n",
    "    def __init__(self, network: torch.nn.Module, mask: torch.Tensor, c_in: int, h: int, w: int):\n",
    "        \"\"\"\n",
    "        Coupling layer inside a normalizing flow.\n",
    "        Inputs:\n",
    "            network - A PyTorch nn.Module constituting the deep neural network for mu and sigma.\n",
    "                      Output shape should be twice the channel size as the input.\n",
    "            mask - Binary mask (0 or 1) where 0 denotes that the element should be transformed,\n",
    "                   while 1 means the latent will be used as input to the NN.\n",
    "            c_in - Number of input channels\n",
    "        \"\"\"\n",
    "        self.c_in = c_in\n",
    "        self.h = h\n",
    "        self.w = w\n",
    "        super().__init__()\n",
    "        self.network = network\n",
    "        self.scaling_factor = torch.nn.Parameter(torch.zeros(c_in, device=mask.device))\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def log_abs_det_jacobian(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "        nn_out = self.network(x * self.mask)\n",
    "        s, t = nn_out.chunk(2, dim=-3)\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "        s = s * (1 - self.mask)\n",
    "        return s.sum(dim=[-1,-2,-3])\n",
    "    \n",
    "    def _inverse(self, y: torch.Tensor) -> torch.Tensor:\n",
    "        # get hold of x by masking transformed parts of y\n",
    "        nn_out = self.network(y * self.mask)\n",
    "        s, t = nn_out.chunk(2, dim=-3)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform the second part)\n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "\n",
    "        # Affine transformation\n",
    "        return (y * torch.exp(-s)) - t\n",
    "    \n",
    "    def _call(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        nn_out = self.network(x * self.mask)\n",
    "        s, t = nn_out.chunk(2, dim=-3)\n",
    "\n",
    "        # Stabilize scaling output\n",
    "        s_fac = self.scaling_factor.exp().view(1, -1, 1, 1)\n",
    "        s = torch.tanh(s / s_fac) * s_fac\n",
    "\n",
    "        # Mask outputs (only transform parts where self.mask == 0)\n",
    "        s = s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "\n",
    "        # Affine transformation\n",
    "        return (x + t) * torch.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThicknessTransform(ComposeTransformModule):\n",
    "    def __init__(self, thickness_size: int, weight: torch.Tensor, bias: torch.Tensor):\n",
    "        self.thickness_size = thickness_size\n",
    "        super().__init__([\n",
    "            Transforms.Spline(thickness_size),\n",
    "            Transforms.AffineTransform(loc=bias, scale=weight, event_dim=0),\n",
    "            Transforms.ExpTransform()\n",
    "        ])\n",
    "\n",
    "\n",
    "class IntensityTransform(ConditionalComposeTransformModule):\n",
    "    def __init__(self, intensity_size: int, thickness_size: int, hidden_dims: List[int], weight: torch.Tensor, bias: torch.Tensor):\n",
    "        self.intensity_size = intensity_size\n",
    "        self.thickness_size = thickness_size\n",
    "        self.hidden_dims = hidden_dims\n",
    "        intensity_nn = pyro.nn.DenseNN(\n",
    "            thickness_size,\n",
    "            hidden_dims,\n",
    "            param_dims=[intensity_size, intensity_size],\n",
    "            nonlinearity=torch.nn.ReLU()\n",
    "        )\n",
    "        super().__init__([\n",
    "            ConditionalAffineTransform(intensity_nn, event_dim=0),\n",
    "            Transforms.SigmoidTransform(),\n",
    "            Transforms.AffineTransform(loc=bias, scale=weight, event_dim=0),\n",
    "        ])\n",
    "\n",
    "\n",
    "class ImageTransform(ConditionalComposeTransformModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        im_size: int,\n",
    "        input_channels: int,\n",
    "        thickness_size: int,\n",
    "        intensity_size: int,\n",
    "        num_blocks: int,\n",
    "        layers_per_block: int,\n",
    "        hidden_channels: int,\n",
    "        *,\n",
    "        alpha: float = 1e-5,\n",
    "        bn_momentum: float = 0.05,\n",
    "        ln_momentum: float = 1e-5,\n",
    "        nonlinearity = torch.nn.ReLU(),\n",
    "    ):\n",
    "        self.im_size = im_size\n",
    "        self.input_channels = input_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.layers_per_block = layers_per_block\n",
    "        \n",
    "        self.flat_input_size = input_channels * im_size * im_size\n",
    "        \n",
    "        layers = []\n",
    "\n",
    "        preprocess_transform = Transforms.ComposeTransform([\n",
    "            Transforms.AffineTransform(0., 1. / 2 ** 8),\n",
    "            Transforms.AffineTransform(alpha, (1 - alpha)),\n",
    "            Transforms.SigmoidTransform().inv,\n",
    "        ])\n",
    "        layers += [preprocess_transform]\n",
    "        \n",
    "        layers += [Transforms.ReshapeTransform((self.flat_input_size,), (input_channels, im_size, im_size))]\n",
    "        for i in range(num_blocks):\n",
    "            layers += [\n",
    "                MaskedAffineCoupling(\n",
    "                    GatedConvNet(input_channels, hidden_channels, layers_per_block, eps=ln_momentum),\n",
    "                    self.create_checkerboard_mask(im_size, im_size, invert=(i%2==1)),\n",
    "                    input_channels,\n",
    "                    im_size,\n",
    "                    im_size,\n",
    "                ),\n",
    "            ]\n",
    "            \n",
    "        # conditioning on context...\n",
    "        for i in range(1):\n",
    "            layers += [\n",
    "                Transforms.ReshapeTransform((input_channels, im_size, im_size), (self.flat_input_size,)),\n",
    "                Transforms.ConditionalAffineAutoregressive(\n",
    "                    pyro.nn.ConditionalAutoRegressiveNN(\n",
    "                        self.flat_input_size,\n",
    "                        thickness_size + intensity_size,\n",
    "                        [2 * self.flat_input_size] * 2,\n",
    "                        nonlinearity=nonlinearity,\n",
    "                        skip_connections=True,\n",
    "                    ),\n",
    "                ),\n",
    "                Transforms.ReshapeTransform((self.flat_input_size,), (input_channels, im_size, im_size)),\n",
    "            ]\n",
    "        \n",
    "        layers += [Transforms.ReshapeTransform((input_channels, im_size, im_size), (self.flat_input_size,))]\n",
    "        \n",
    "        super().__init__(layers)\n",
    "            \n",
    "    @staticmethod\n",
    "    def create_checkerboard_mask(h: int, w: int, invert=False):\n",
    "        x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "        xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "        mask = torch.fmod(xx + yy, 2).to(torch.float32).view(1, 1, h, w)\n",
    "        return mask if not invert else (1. - mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type           | Params\n",
      "-----------------------------------------\n",
      "0 | model | ImageTransform | 606 K \n",
      "-----------------------------------------\n",
      "606 K     Trainable params\n",
      "0         Non-trainable params\n",
      "606 K     Total params\n",
      "2.427     Total estimated model params size (MB)\n",
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8036262176e04d0a82623b180552cd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=400` reached.\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(1234)\n",
    "\n",
    "adam_params = {\"lr\": 1e-5}\n",
    "batch_size = 128\n",
    "num_epochs = 400  #50000 // (images.shape[0] // batch_size)\n",
    "\n",
    "class LightningRaw(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, model: torch.nn.Module, adam_params: dict):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.register_buffer(\"base_loc\", torch.tensor(0.))\n",
    "        self.register_buffer(\"base_scale\", torch.tensor(1.))\n",
    "        self.adam_params = adam_params\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        t_obs, i_obs, x_obs = batch\n",
    "        x_obs = x_obs + torch.rand_like(x_obs)\n",
    "        #x_obs = x_obs.clamp(min=1, max=255)\n",
    "        base_dist = dist.Normal(self.base_loc, self.base_scale).expand([x_obs.shape[0], self.model.flat_input_size]).to_event(1)\n",
    "        context = torch.cat([t_obs, i_obs], dim=-1).log()\n",
    "        tfm = self.model.condition(context=context)\n",
    "        x_noise = tfm(x_obs)\n",
    "        loss = -(base_dist.log_prob(x_noise) + tfm.log_abs_det_jacobian(x_obs, x_noise)).mean() / self.model.flat_input_size\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), **self.adam_params)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "raw_img_transform = ImageTransform(\n",
    "    im_size,\n",
    "    1,\n",
    "    thickness_size,\n",
    "    intensity_size,\n",
    "    num_blocks=8,\n",
    "    layers_per_block=3,\n",
    "    hidden_channels=16,\n",
    "    nonlinearity=torch.nn.ELU(),\n",
    ")\n",
    "\n",
    "raw_lightning = LightningRaw(raw_img_transform, adam_params)\n",
    "\n",
    "raw_dataloader = DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        thickness[..., None].detach(),\n",
    "        intensity[..., None].detach(),\n",
    "        images.reshape(-1, images.shape[-1] ** 2).detach(),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "raw_trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator='gpu',\n",
    "    default_root_dir=os.path.join(\"./lightning_logs/deepscm_ckpt\", \"deepscm\"),\n",
    "    callbacks=[\n",
    "        pl.callbacks.ModelCheckpoint(save_weights_only=True, mode=\"min\", monitor=\"train_loss\"),\n",
    "        pl.callbacks.LearningRateMonitor(\"epoch\"),\n",
    "    ],\n",
    ")\n",
    "raw_trainer.fit(model=raw_lightning, train_dataloaders=raw_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(256.0025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe3dec521a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_lightning = raw_lightning.to(device=thickness.device)\n",
    "\n",
    "raw_img_dist = dist.TransformedDistribution(\n",
    "    dist.Normal(raw_lightning.base_loc, raw_lightning.base_scale).expand([raw_lightning.model.flat_input_size]).to_event(1),\n",
    "    raw_lightning.model.condition(\n",
    "        context=torch.cat([thickness[0:batch_size, None], intensity[0:batch_size, None]], dim=-1).log()\n",
    "    ).inv)\n",
    "raw_gen_img = raw_img_dist.sample((batch_size,)).reshape(-1, im_size, im_size)\n",
    "print(raw_gen_img.max() - raw_gen_img.min())\n",
    "\n",
    "plt.imshow(raw_gen_img[20].detach().cpu().numpy())\n",
    "#plt.imshow(images[20].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSCM(PyroModule):\n",
    "    \n",
    "    thickness_support = dist.constraints.positive\n",
    "    intensity_support = dist.constraints.positive\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        thickness_transform: ThicknessTransform,\n",
    "        intensity_transform: IntensityTransform,\n",
    "        img_transform: ImageTransform,\n",
    "        *,\n",
    "        include_thickness: bool = True,\n",
    "        include_intensity: bool = True,\n",
    "        device: torch.device = curr_device,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.include_thickness = include_thickness\n",
    "        self.include_intensity = include_intensity\n",
    "        \n",
    "        self.thickness_transform = thickness_transform\n",
    "        self.intensity_transform = intensity_transform\n",
    "        self.img_transform = img_transform\n",
    "\n",
    "        # tensor sizes\n",
    "        self.thickness_size = self.thickness_transform.thickness_size\n",
    "        self.intensity_size = self.intensity_transform.intensity_size\n",
    "        self.im_size = self.img_transform.im_size\n",
    "        self.input_dim = self.im_size * self.im_size\n",
    "\n",
    "    @staticmethod\n",
    "    def StandardNormal(*event_shape: int, **kwargs) -> Union[dist.Independent, dist.Normal]:\n",
    "        return dist.Normal(\n",
    "            torch.zeros((), **kwargs),\n",
    "            torch.ones((), **kwargs),\n",
    "        ).expand(event_shape).to_event(len(event_shape))\n",
    "    \n",
    "    @staticmethod\n",
    "    def cond_dist(\n",
    "        transform: Union[Transforms.Transform, dist.conditional.ConditionalTransform],\n",
    "        U_dist: dist.Distribution,\n",
    "        *contexts: torch.Tensor\n",
    "    ) -> dist.Distribution:\n",
    "        if not contexts:\n",
    "            assert isinstance(transform, Transforms.Transform)\n",
    "            return dist.TransformedDistribution(U_dist, transform)\n",
    "        batch_shape = torch.broadcast_shapes(*(c.shape[:-1] for c in contexts))\n",
    "        context = torch.cat([c.expand(batch_shape + (-1,)) for c in contexts], dim=-1)\n",
    "        U_dist = U_dist.expand(torch.broadcast_shapes(batch_shape, U_dist.batch_shape))\n",
    "        return dist.ConditionalTransformedDistribution(U_dist, [transform]).condition(context=context)\n",
    "\n",
    "    def forward(self):\n",
    "        # Thickness:\n",
    "        UT_dist = self.StandardNormal(self.thickness_size, device=self.device)\n",
    "        T_dist = self.cond_dist(self.thickness_transform, UT_dist)\n",
    "        T = pyro.sample(\"T\", T_dist.mask(self.include_thickness))\n",
    "        T_unconstrained = Transforms.biject_to(self.thickness_support).inv(T)\n",
    "\n",
    "        # Intensity:\n",
    "        UI_dist = self.StandardNormal(self.intensity_size, device=self.device)\n",
    "        I_dist = self.cond_dist(self.intensity_transform, UI_dist, T_unconstrained)\n",
    "        I = pyro.sample(\"I\", I_dist.mask(self.include_intensity))\n",
    "        I_unconstrained = Transforms.biject_to(self.intensity_support).inv(I)\n",
    "\n",
    "        # Image:\n",
    "        UX_dist = self.StandardNormal(1, self.im_size, self.im_size, device=self.device)\n",
    "        X_dist = self.cond_dist(self.img_transform, UX_dist, T_unconstrained, I_unconstrained)\n",
    "        X = pyro.sample(\"X\", X_dist)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"634pt\" height=\"553pt\"\n",
       " viewBox=\"0.00 0.00 634.00 553.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 549)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-549 630,-549 630,4 -4,4\"/>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"27\" cy=\"-308.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"63\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M29.14,-290.32C35.17,-242.01 52.3,-104.72 59.62,-46.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"63.1,-46.45 60.87,-36.09 56.16,-45.58 63.1,-46.45\"/>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"99\" cy=\"-308.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">I</text>\n",
       "</g>\n",
       "<!-- I&#45;&gt;X -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>I&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M96.86,-290.32C90.83,-242.01 73.7,-104.72 66.38,-46.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.84,-45.58 65.13,-36.09 62.9,-46.45 69.84,-45.58\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-529.8\" font-family=\"Times,serif\" font-size=\"14.00\">T ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-514.8\" font-family=\"Times,serif\" font-size=\"14.00\">I ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-499.8\" font-family=\"Times,serif\" font-size=\"14.00\">X ~ TransformedDistribution</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-484.8\" font-family=\"Times,serif\" font-size=\"14.00\">thickness_transform$$$0.unnormalized_widths : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-469.8\" font-family=\"Times,serif\" font-size=\"14.00\">thickness_transform$$$0.unnormalized_heights : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-454.8\" font-family=\"Times,serif\" font-size=\"14.00\">thickness_transform$$$0.unnormalized_derivatives : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-439.8\" font-family=\"Times,serif\" font-size=\"14.00\">thickness_transform$$$0.unnormalized_lambdas : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-424.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.0.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-409.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.0.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-394.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.1.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-379.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.1.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-364.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.2.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-349.8\" font-family=\"Times,serif\" font-size=\"14.00\">intensity_transform$$$intensity_nn.layers.2.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-334.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$f_X.nn.layers.0.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-319.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$f_X.nn.layers.0.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-304.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$f_X.nn.layers.1.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-289.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$f_X.nn.layers.1.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-274.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm1.gamma : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-259.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm1.beta : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-244.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_affine_coupling.nn.layers.0.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-229.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_affine_coupling.nn.layers.0.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-214.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_affine_coupling.nn.layers.1.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-199.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_affine_coupling.nn.layers.1.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-184.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm2.gamma : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-169.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm2.beta : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-154.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_auto.nn.layers.0.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-139.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_auto.nn.layers.0.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_auto.nn.layers.1.weight : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-109.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$img_auto.nn.layers.1.bias : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-94.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm3.gamma : Real()</text>\n",
       "<text text-anchor=\"start\" x=\"152\" y=\"-79.8\" font-family=\"Times,serif\" font-size=\"14.00\">img_transform$$$norm3.beta : Real()</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fac4919fd90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "pyro.set_rng_seed(1234)\n",
    "\n",
    "# Thickness parameters\n",
    "thickness_size = 1\n",
    "thickness_flow_bias = thickness.log().mean()\n",
    "thickness_flow_weight = thickness.log().std()\n",
    "\n",
    "thickness_transform = ThicknessTransform(\n",
    "    thickness_size,\n",
    "    thickness_flow_weight[..., None].detach(),\n",
    "    thickness_flow_bias[..., None].detach()\n",
    ")\n",
    "\n",
    "# Intensity parameters\n",
    "intensity_size = 1\n",
    "intensity_hidden_dim = 5\n",
    "intensity_flow_bias = intensity.min()\n",
    "intensity_flow_weight = intensity.max() - intensity.min()\n",
    "\n",
    "intensity_transform = IntensityTransform(\n",
    "    intensity_size,\n",
    "    thickness_size,\n",
    "    [intensity_hidden_dim] * 2,\n",
    "    intensity_flow_weight[..., None].detach(),\n",
    "    intensity_flow_bias[..., None].detach()\n",
    ")\n",
    "\n",
    "# Image parameters\n",
    "im_size = images.shape[-1]\n",
    "input_dim = im_size ** 2\n",
    "img_hidden_dim = 10\n",
    "img_hidden_layers = 5\n",
    "alpha = 0.05\n",
    "num_bits = 8\n",
    "\n",
    "img_transform = ImageTransform(\n",
    "    im_size,\n",
    "    input_dim,\n",
    "    img_hidden_dim,\n",
    "    img_hidden_layers,\n",
    "    thickness_size,\n",
    "    intensity_size,\n",
    "    alpha=alpha,\n",
    "    num_bits=num_bits,\n",
    "    nonlinearity=torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "scm = DeepSCM(thickness_transform, intensity_transform, img_transform, include_thickness=False, include_intensity=False, device=curr_device)\n",
    "pyro.render_model(scm, render_params=True, render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"166pt\" height=\"171pt\"\n",
       " viewBox=\"0.00 0.00 166.00 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-167 162,-167 162,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_data</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-155 150,-155 150,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"126\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data</text>\n",
       "</g>\n",
       "<!-- T -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>T</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"115\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"115\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">T</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"79\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"79\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- T&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>T&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106.65,-111.76C102.29,-103.28 96.85,-92.71 91.96,-83.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"94.99,-81.44 87.3,-74.15 88.77,-84.64 94.99,-81.44\"/>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"43\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">I</text>\n",
       "</g>\n",
       "<!-- I&#45;&gt;X -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>I&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M51.35,-111.76C55.71,-103.28 61.15,-92.71 66.04,-83.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.23,-84.64 70.7,-74.15 63.01,-81.44 69.23,-84.64\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fab478ed8d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConditionedDeepSCM(PyroModule):\n",
    "    def __init__(self, model: DeepSCM):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, t_obs, i_obs, x_obs):\n",
    "        with pyro.condition(data={\"X\": x_obs, \"T\": t_obs, \"I\": i_obs}), \\\n",
    "                pyro.poutine.scale(scale=1 / x_obs.shape[0]), \\\n",
    "                pyro.plate(\"data\", size=x_obs.shape[0], dim=-1):\n",
    "            return self.model()\n",
    "\n",
    "conditioned_model = ConditionedDeepSCM(scm)\n",
    "pyro.render_model(conditioned_model, model_args=(thickness[:5][..., None], intensity[:5][..., None], images[:5].reshape(-1, im_size*im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | ConditionedDeepSCM | 1.7 M \n",
      "1 | guide | AutoDelta          | 0     \n",
      "2 | elbo  | ELBOModule         | 1.7 M \n",
      "---------------------------------------------\n",
      "1.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.7 M     Total params\n",
      "6.993     Total estimated model params size (MB)\n",
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a03c4700b97430db3e4de259b5ff84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eli/development/causal_pyro/.env/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "lr_decay = 0.99\n",
    "initial_lr = 1e-5\n",
    "adam_params = {\"lr\": initial_lr} #, \"betas\": (0.95, 0.999)}\n",
    "batch_size = 256\n",
    "num_epochs = 100\n",
    "\n",
    "class LightningSVI(pl.LightningModule):\n",
    "    def __init__(self, model: PyroModule, guide: PyroModule, elbo: pyro.infer.ELBO):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.elbo = elbo(self.model, self.guide)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        t_obs, i_obs, x_obs = batch\n",
    "        loss = self.elbo(t_obs, i_obs, x_obs)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.elbo.parameters(), **adam_params)\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, lr_decay)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "guide = pyro.infer.autoguide.AutoDelta(conditioned_model).to(device=curr_device)\n",
    "elbo = pyro.infer.Trace_ELBO()\n",
    "lightning_svi = LightningSVI(conditioned_model, guide, elbo).to(device=curr_device)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    torch.utils.data.TensorDataset(\n",
    "        thickness[..., None].detach(),\n",
    "        intensity[..., None].detach(),\n",
    "        images.reshape(-1, images.shape[-1] ** 2).detach(),\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=num_epochs, accelerator='gpu')\n",
    "trainer.fit(model=lightning_svi, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "tensor(62316704., device='cuda:0') tensor(-8755166., device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-0.5, 13.5, 13.5, -0.5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictive = pyro.infer.Predictive(condition(scm, data = {\"T\": thickness[0:1][..., None], \"I\": intensity[0:1][..., None]}), guide=guide, num_samples=4)\n",
    "img = predictive()[\"X\"]\n",
    "print((img[0] - img[1]).max(), (img[0] - img[1]).min())\n",
    "\n",
    "fig = plt.figure()\n",
    "rows = 2\n",
    "columns = 2\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(img[0].cpu().reshape((im_size, im_size)))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(img[1].cpu().reshape((im_size, im_size)))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(img[2].cpu().reshape((im_size, im_size)))\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 4)\n",
    "plt.imshow(img[3].cpu().reshape((im_size, im_size)))\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query: counterfactual data generation\n",
    "\n",
    "Next we ask a *counterfactual* question: given an observed digit $X$, what\n",
    "would the digit have been had $t$ been $t + 1$?\n",
    "\n",
    "To compute this quantity we would normally:\n",
    "   1. invert the model to find latent exogenous noise $u$\n",
    "   2. construct an intervened model\n",
    "   3. re-simulate the forward model on the $u$ [@pearl2011algorithmization].  \n",
    "\n",
    "However, we can equivalently\n",
    "represent this process with inference in a single, expanded\n",
    "probabilistic program containing two copies of every deterministic\n",
    "statement (a so-called \\\"twin network\\\" representation of\n",
    "counterfactuals, first described in Chapter 7 of [@pearl] and extended\n",
    "to the PPL setting in [@tavares_2020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = images[0]\n",
    "plt.imshow(x_obs.cpu().detach().reshape((im_size, im_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterfactualDeepSCM(PyroModule):\n",
    "    def __init__(self, model: DeepSCM):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x_obs: torch.Tensor, i_act: torch.Tensor):\n",
    "        with MultiWorldCounterfactual(dim=-1), \\\n",
    "                do(actions={\"I\": torch.tensor([190.0], device=x_obs.device)}), \\\n",
    "                condition(data={\"X\": x_obs}):\n",
    "            return model()\n",
    "\n",
    "cf_model = CounterfactualDeepSCM(scm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.title(\"Twin World Counterfactual Model\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "rows = 1\n",
    "columns = 2\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(cf_model(x_obs)[\"X\"][0][0].cpu().reshape((14, 14)))\n",
    "plt.title(\"Actual Model\")\n",
    "plt.axis('off')\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(torch.nn.functional.normalize(cf_model(x_obs)[\"X\"][0][1].cpu().reshape((14, 14))))\n",
    "plt.title(\"Intervened Model\")\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like all counterfactuals, this estimand is not identified in general\n",
    "without further assumptions: learning parameters $\\theta$ that match\n",
    "observed data does not guarantee that the counterfactual distribution\n",
    "will match that of the true causal model. \n",
    "\n",
    "However, as discussed in the\n",
    "original paper [@pawlowski2020deep] in the context of modeling MRI\n",
    "images, there are a number of valid practical reasons one might wish to\n",
    "compute it anyway, such as explanation or expert evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "2bdfcdbd368c30fa36f32c37bcc33f6323158b752539e151107ac7b028e21443"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
