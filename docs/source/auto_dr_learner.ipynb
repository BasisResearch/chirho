{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly robust estimation with Chirho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Overview: Robust Causal Inference with Cut Modules](#overview:-robust-causal-inference-with-cut-modules)\n",
    "- [Example: Synthetic data generation from a high-dimensional generalized linear model](#example:-synthetic-data-generation-from-a-high-dimensional-generalized-linear-model)\n",
    "- [Effect estimation using cut modules](#effect-estimation-using-cut-modules)\n",
    "- [References](#references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, N: int, p: int, link_fn: Callable[..., dist.Distribution] = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p,)).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.).mask(False))\n",
    "    \n",
    "    def sample_covariate_loc_scale(self):\n",
    "        loc = pyro.sample(\"covariate_loc\", dist.Normal(0., 1.).expand((self.p,)).to_event(1))\n",
    "        scale = pyro.sample(\"covariate_scale\", dist.LogNormal(0, 1).expand((self.p,)).to_event(1))\n",
    "        return loc, scale\n",
    "\n",
    "    def forward(self):\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "        with pyro.plate(\"obs\", self.N, dim=-1):\n",
    "            X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1))\n",
    "            A = pyro.sample(\"A\", dist.Bernoulli(logits=torch.einsum(\"...np,...p->...n\", X, propensity_weights)))\n",
    "            return pyro.sample(\"Y\", self.link_fn(torch.einsum(\"...np,...p->...n\", X, outcome_weights) + A * tau))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, N: int, p: int, link_fn: Callable, alpha: int, beta: int):\n",
    "        super().__init__(N, p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(4 * self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha:] = 0.\n",
    "        return propensity_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(0.)\n",
    "    \n",
    "# model here implictly already is conditioning on X_train and on particular theta\n",
    "def ATE(model: Callable[[], torch.Tensor], num_samples: int = 100) -> torch.Tensor:\n",
    "    \"\"\"Compute the average treatment effect of a model.\"\"\"\n",
    "    @pyro.plate(\"num_samples\", num_samples, dim=-2)\n",
    "    def _ate_model():\n",
    "        with MultiWorldCounterfactual():\n",
    "            with do(actions=dict(A=(torch.tensor(0.), torch.tensor(1.)))):\n",
    "                Ys = model()\n",
    "            Y0 = gather(Ys, IndexSet(A={1}), event_dim=0)\n",
    "            Y1 = gather(Ys, IndexSet(A={2}), event_dim=0)\n",
    "            return pyro.deterministic(\"ATE\", (Y1 - Y0).mean(dim=-1, keepdim=True))\n",
    "    \n",
    "    return _ate_model().mean(dim=-2, keepdim=True).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, torch.tensor]) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Flatten a dictionary of tensors into a single vector.\n",
    "    \"\"\"\n",
    "    return torch.cat([v.flatten() for k, v in d.items()])\n",
    "\n",
    "\n",
    "def unflatten_dict(x: torch.tensor, d: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Unflatten a vector into a dictionary of tensors.\n",
    "    \"\"\"\n",
    "    return collections.OrderedDict(zip(\n",
    "        d.keys(), [v_flat.reshape(v.shape) for v, v_flat in zip(d.values(), torch.split(x, [v.numel() for k, v in d.items()]))]\n",
    "    ))\n",
    "\n",
    "\n",
    "def one_step_correction(\n",
    "    target_functional: Callable[[Callable], torch.tensor],\n",
    "    model: Callable[[], torch.tensor],\n",
    "    theta_hat: Dict[str, torch.tensor],\n",
    "    X_train: Dict[str, torch.tensor],\n",
    "    X_test: Dict[str, torch.tensor],\n",
    "    *,\n",
    "    eps_fisher: float = 1e-5,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"\n",
    "    One step correction for a given target functional.\n",
    "    \"\"\"\n",
    "    theta_hat = collections.OrderedDict((k, theta_hat[k]) for k in sorted(theta_hat.keys()))\n",
    "    model_theta_hat = condition(data=theta_hat)(model)\n",
    "\n",
    "    model_theta_hat_train = condition(data=X_train)(model_theta_hat)\n",
    "    plug_in = target_functional(model_theta_hat_train) + (0 * sum(v.sum() for v in theta_hat.values()))\n",
    "    plug_in_grads = collections.OrderedDict(zip(theta_hat.keys(), torch.autograd.grad(plug_in, theta_hat.values(), create_graph=True)))\n",
    "    \n",
    "    # compute the score function for the new data\n",
    "    model_theta_hat_test = condition(data=X_test)(model_theta_hat)\n",
    "    log_likelihood_test = pyro.poutine.trace(model_theta_hat_test).get_trace().log_prob_sum() / X_test[next(iter(X_test))].shape[0]\n",
    "    scores = collections.OrderedDict(zip(theta_hat.keys(), torch.autograd.grad(log_likelihood_test, theta_hat.values(), create_graph=True)))\n",
    "\n",
    "    # compute the fisher information matrix for the model, not the target functional\n",
    "    # we use the training data to estimate the fisher information matrix along with theta_hat itself\n",
    "    def _f_hess(flat_theta: torch.tensor) -> torch.tensor:\n",
    "        theta = unflatten_dict(flat_theta, theta_hat)\n",
    "        model_theta_hat_fisher = condition(data=X_train)(condition(data=theta)(model))\n",
    "        log_likelihood_fisher = pyro.poutine.trace(model_theta_hat_fisher).get_trace().log_prob_sum() / X_train[next(iter(X_train))].shape[0]\n",
    "        return log_likelihood_fisher\n",
    "\n",
    "    fisher_info_approx = torch.autograd.functional.hessian(_f_hess, flatten_dict(theta_hat))\n",
    "\n",
    "    # compute the correction\n",
    "    plug_in_grad = flatten_dict(plug_in_grads)\n",
    "    score = flatten_dict(scores)\n",
    "    inverse_fisher_info = torch.inverse(fisher_info_approx + eps_fisher * torch.eye(fisher_info_approx.shape[0]))\n",
    "    return torch.einsum(\"i,ij,j->\", plug_in_grad, inverse_fisher_info, score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we generate synthetic data as in Figure 4b of Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 5\n",
    "beta = 5\n",
    "N_train = 200\n",
    "N_test = N_train  # 500  # TODO refactor model and ATE to not require N_test == N_train\n",
    "benchmark_model_train = BenchmarkLinearModel(N_train, p, gaussian_link, alpha, beta)\n",
    "benchmark_model_test = BenchmarkLinearModel(N_test, p, gaussian_link, alpha, beta)\n",
    "\n",
    "with pyro.poutine.trace() as train_tr:\n",
    "    benchmark_model_train()\n",
    "\n",
    "with pyro.poutine.trace() as test_tr:\n",
    "    benchmark_model_test()\n",
    "\n",
    "D_train = {k: train_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}\n",
    "D_test = {k: test_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0000] loss: 613942.1875\n",
      "[iteration 0250] loss: 143580.7656\n",
      "[iteration 0500] loss: 141576.9688\n",
      "[iteration 0750] loss: 141237.5781\n",
      "[iteration 1000] loss: 141122.4844\n",
      "[iteration 1250] loss: 141076.1719\n",
      "[iteration 1500] loss: 141055.9531\n",
      "[iteration 1750] loss: 141045.7812\n"
     ]
    }
   ],
   "source": [
    "# Fit model to training data (uncorrected)\n",
    "class ConditionModelTrain(HighDimLinearModel):\n",
    "    def forward(self):\n",
    "        with condition(data=D_train):\n",
    "            return super().forward()\n",
    "    \n",
    "model_train = ConditionModelTrain(N_train, p, gaussian_link)\n",
    "guide_train = pyro.infer.autoguide.AutoDelta(model_train)\n",
    "elbo = pyro.infer.Trace_ELBO()(model_train, guide_train)\n",
    "\n",
    "# initialize parameters\n",
    "elbo()\n",
    "\n",
    "adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(2000):\n",
    "    adam.zero_grad()\n",
    "    loss = elbo()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    if step % 250 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['outcome_weights', 'propensity_weights', 'treatment_weight', 'covariate_loc', 'covariate_scale']) tensor(0.4955, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "theta_hat = {k: v.detach().clone().requires_grad_(True) for k, v in guide_train().items()}\n",
    "print(theta_hat.keys(), theta_hat[\"treatment_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE plugin tensor(0.4952, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_test = condition(data=theta_hat)(condition(data=D_train)(HighDimLinearModel(N_test, p, gaussian_link)))\n",
    "\n",
    "ATE_plugin = ATE(model_test, num_samples=100000)\n",
    "print(\"ATE plugin\", ATE_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4952, grad_fn=<SqueezeBackward0>) tensor(4.1197, grad_fn=<ViewBackward0>) tensor(4.6149, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ATE_correction = one_step_correction(\n",
    "    lambda m: ATE(m, num_samples=100000),\n",
    "    HighDimLinearModel(N_test, p, gaussian_link),\n",
    "    theta_hat,\n",
    "    D_train,\n",
    "    D_test,\n",
    "    eps_fisher=1e-5,\n",
    ")\n",
    "ATE_onestep = ATE_plugin + ATE_correction\n",
    "print(ATE_plugin, ATE_correction, ATE_onestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
