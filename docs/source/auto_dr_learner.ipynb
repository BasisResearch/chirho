{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly robust estimation with Chirho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Setup](#setup)\n",
    "- [Overview: Robust Causal Inference with Cut Modules](#overview:-robust-causal-inference-with-cut-modules)\n",
    "- [Example: Synthetic data generation from a high-dimensional generalized linear model](#example:-synthetic-data-generation-from-a-high-dimensional-generalized-linear-model)\n",
    "- [Effect estimation using cut modules](#effect-estimation-using-cut-modules)\n",
    "- [References](#references)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from chirho.indexed.handlers import IndexPlatesMessenger\n",
    "from chirho.observational.handlers.cut import SingleStageCut\n",
    "from pyro.infer import Predictive\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int, link_fn: Callable = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "\n",
    "    def forward(self, X: torch.Tensor, A: torch.Tensor):\n",
    "        N = X.shape[0]\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        with pyro.plate(\"obs\", N):\n",
    "            pyro.sample(\"Y\", self.link_fn(X @ outcome_weights + A * tau))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, p: int, link_fn: Callable, alpha: int, beta: int):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think X_fisher should be the training data\n",
    "\n",
    "def one_step_correction(model: Callable[[torch.tensor], None], X_fisher: Dict[str, torch.tensor], X_new: Dict[str, torch.tensor], theta_hat: torch.tensor, target_functional: Callable[[Callable], torch.tensor]) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    One step correction for a given target functional.\n",
    "    \"\"\"\n",
    "    plug_in = target_functional(lambda: model(theta_hat))\n",
    "    plug_in_grad = torch.autograd.grad(plug_in, theta_hat, create_graph=True)\n",
    "    \n",
    "    # compute the fisher information matrix for the model, not the target functional\n",
    "    # we use the training data to estimate the fisher information matrix along with theta_hat itself\n",
    "    log_likelihood_fisher = pyro.poutine.trace(condition(data=X_fisher)(lambda: model(theta_hat))).get_trace().log_prob_sum() / X_fisher[next(iter(X_fisher))].shape[0]\n",
    "    fisher_info_approx = torch.autograd.hessian(log_likelihood_fisher, (theta_hat,))\n",
    "\n",
    "    # compute the score function for the new data\n",
    "    log_likelihood_new = pyro.poutine.trace(condition(data=X_new)(lambda: model(theta_hat))).get_trace().log_prob_sum() / X_new[next(iter(X_new))].shape[0]\n",
    "    score = torch.autograd.grad(log_likelihood_new, theta_hat, create_graph=True)\n",
    "\n",
    "    # compute the correction\n",
    "    inverse_fisher_info = torch.inverse(fisher_info_approx)\n",
    "    return plug_in_grad.T @ inverse_fisher_info @ score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we generate synthetic data as in Figure 4b of Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 200\n",
    "N_test = 500\n",
    "benchmark_model = BenchmarkLinearModel(p, gaussian_link, alpha, beta)\n",
    "true_propensity_weights = 1 / math.sqrt(4 * alpha) * torch.ones(p)\n",
    "true_propensity_weights[alpha:] = 0.\n",
    "X_train = dist.Normal(0., 1.).expand((N_train, p)).to_event(1).sample()\n",
    "A_train = dist.Bernoulli(logits=X_train @ true_propensity_weights).sample()\n",
    "X_test = dist.Normal(0., 1.).expand((N_test, p)).to_event(1).sample()\n",
    "A_test = dist.Bernoulli(logits=X_test @ true_propensity_weights).sample()\n",
    "\n",
    "with pyro.poutine.trace() as training_data:\n",
    "    benchmark_model(X_train, A_train)\n",
    "\n",
    "with pyro.poutine.trace() as testing_data:\n",
    "    benchmark_model(X_test, A_test)\n",
    "\n",
    "Y_train = training_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_train = {\"X\": X_train, \"A\": A_train, \"Y\": Y_train}\n",
    "\n",
    "Y_test = testing_data.trace.nodes[\"Y\"][\"value\"]\n",
    "D_test = {\"X\": X_test, \"A\": A_test, \"Y\": Y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do ridge regression on the training data\n",
    "ridge_penalty = .001\n",
    "X_tilde_train = torch.cat([A_train.reshape(-1, 1), X_train], dim=1)\n",
    "theta_ridge = torch.inverse(X_tilde_train.T @ X_tilde_train + ridge_penalty * torch.eye(p + 1)) @ X_tilde_train.T @ Y_train\n",
    "theta_true = 1 / math.sqrt(beta) * torch.ones(p + 1)\n",
    "theta_true[(beta + 1):] = 0.\n",
    "theta_true[0] = 0. # true treatment effect is zero"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1544)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((theta_ridge - theta_true) ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(theta_true ** 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0155)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_ridge[0] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGaussianModel(pyro.nn.PyroModule):\n",
    "    def __init__(p):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    \n",
    "    def forward(self, theta: Dict[torch.tensor], X = None: torch.tensor, A=None: torch.tensor, Y=None: torch.tensor):\n",
    "        if X is not None:\n",
    "            assert len(X.shape) == 1, \"X should be a vector\"\n",
    "            assert len(A.shape) == 1, \"A should be a vector\"\n",
    "            assert len(Y.shape) == 0, \"Y should be a scalar\"\n",
    "        \n",
    "        noise_scale = theta['noise_scale']\n",
    "        theta_propensity = theta['theta_propensity']\n",
    "        theta_outcome = theta['theta_outcome']\n",
    "        X = pyro.sample(\"X\", dist.Normal(0., 1.).expand((self.p, )).to_event(1), obs=X) \n",
    "        A = pyro.sample(\"A\", dist.Bernoulli(logits=X @ theta_propensity))\n",
    "        mean = A * theta_outcome[0] + X.mv(theta_outcome)\n",
    "        pyro.sample(\"Y\", dist.Normal(mean, noise_scale)) \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def linear_gaussian_model_with_cov(theta, X, A):\n",
    "    p = theta.shape[0]\n",
    "    mean = A * theta[0] + X.mv(theta)\n",
    "    pyro.sample(\"Y\", dist.Normal(mean, 1.))    \n",
    "\n",
    "X_fisher = {\"X\": X_train, \"A\": A_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_step_correction(model: Callable[[torch.tensor], None], X_fisher: Dict[str, torch.tensor], X_new: Dict[str, torch.tensor], theta_hat: torch.tensor, target_functional: Callable[[Callable], torch.tensor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
