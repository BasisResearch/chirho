{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doubly robust estimation with Chirho"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, N: int, p: int, link_fn: Callable[..., dist.Distribution] = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p,)).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "    \n",
    "    def sample_covariate_loc_scale(self):\n",
    "        loc = pyro.sample(\"covariate_loc\", dist.Normal(0., 1.).expand((self.p,)).to_event(1))\n",
    "        scale = pyro.sample(\"covariate_scale\", dist.LogNormal(0, 1).expand((self.p,)).to_event(1))\n",
    "        return loc, scale\n",
    "\n",
    "    def forward(self):\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "        with pyro.plate(\"obs\", self.N, dim=-1):\n",
    "            X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1))\n",
    "            A = pyro.sample(\"A\", dist.Bernoulli(logits=torch.einsum(\"...np,...p->...n\", X, propensity_weights)))\n",
    "            return pyro.sample(\"Y\", self.link_fn(torch.einsum(\"...np,...p->...n\", X, outcome_weights) + A * tau))\n",
    "        \n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, N: int, p: int, link_fn: Callable, alpha: int, beta: int):\n",
    "        super().__init__(N, p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(4 * self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha:] = 0.\n",
    "        return propensity_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(0.)\n",
    "    \n",
    "    def sample_covariate_loc_scale(self):\n",
    "        return torch.zeros(self.p), torch.ones(self.p)\n",
    "    \n",
    "# model here implictly already is conditioning on X_train and on particular theta\n",
    "def ATE(model: Callable[[], torch.Tensor], num_samples: int = 100) -> torch.Tensor:\n",
    "    \"\"\"Compute the average treatment effect of a model.\"\"\"\n",
    "    @pyro.plate(\"num_samples\", num_samples, dim=-2)\n",
    "    def _ate_model():\n",
    "        with MultiWorldCounterfactual():\n",
    "            with do(actions=dict(A=(torch.tensor(0.), torch.tensor(1.)))):\n",
    "                Ys = model()\n",
    "            Y0 = gather(Ys, IndexSet(A={1}), event_dim=0)\n",
    "            Y1 = gather(Ys, IndexSet(A={2}), event_dim=0)\n",
    "            return pyro.deterministic(\"ATE\", (Y1 - Y0).mean(dim=-1, keepdim=True))\n",
    "    \n",
    "    return _ate_model().mean(dim=-2, keepdim=True).squeeze()\n",
    "\n",
    "\n",
    "def ATE_2(model: Callable[[], torch.Tensor], num_samples: int = 100) -> torch.Tensor:\n",
    "    \"\"\"Compute the average treatment effect of a model.\"\"\"\n",
    "    @pyro.plate(\"num_samples\", num_samples, dim=-2)\n",
    "    def _ate_model():\n",
    "        with do(actions=dict(A=torch.tensor(0.))):\n",
    "            Y0 = model()\n",
    "        with do(actions=dict(A=torch.tensor(1.))):\n",
    "            Y1 = model()\n",
    "        return pyro.deterministic(\"ATE\", (Y1 - Y0).mean(dim=-1, keepdim=True))\n",
    "    \n",
    "    return _ate_model().mean(dim=-2, keepdim=True).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, torch.tensor]) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Flatten a dictionary of tensors into a single vector.\n",
    "    \"\"\"\n",
    "    return torch.cat([v.flatten() for k, v in d.items()])\n",
    "\n",
    "\n",
    "def unflatten_dict(x: torch.tensor, d: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Unflatten a vector into a dictionary of tensors.\n",
    "    \"\"\"\n",
    "    return collections.OrderedDict(zip(\n",
    "        d.keys(), [v_flat.reshape(v.shape) for v, v_flat in zip(d.values(), torch.split(x, [v.numel() for k, v in d.items()]))]\n",
    "    ))\n",
    "\n",
    "\n",
    "def one_step_correction(\n",
    "    target_functional: Callable[[Callable], torch.tensor],\n",
    "    model: Callable[[], torch.tensor],\n",
    "    theta_hat: Dict[str, torch.tensor],\n",
    "    X_train: Dict[str, torch.tensor],\n",
    "    X_test: Dict[str, torch.tensor],\n",
    "    *,\n",
    "    eps_fisher: float = 1e-5,\n",
    ") -> torch.tensor:\n",
    "    \"\"\"\n",
    "    One step correction for a given target functional.\n",
    "    \"\"\"\n",
    "    theta_hat = collections.OrderedDict((k, theta_hat[k]) for k in sorted(theta_hat.keys()))\n",
    "    model_theta_hat = condition(data=theta_hat)(model)\n",
    "\n",
    "    plug_in = target_functional(model_theta_hat) + (0 * sum(theta_hat[k].sum() for k in {\"propensity_weights\"}))\n",
    "    plug_in_grads = collections.OrderedDict(zip(theta_hat.keys(), torch.autograd.grad(plug_in, theta_hat.values())))\n",
    "    \n",
    "    # compute the score function for the new data\n",
    "    model_theta_hat_test = condition(data=X_test)(model_theta_hat)\n",
    "    log_likelihood_test = pyro.poutine.trace(model_theta_hat_test).get_trace().log_prob_sum() / X_test[next(iter(X_test))].shape[0]\n",
    "    scores = collections.OrderedDict(zip(theta_hat.keys(), torch.autograd.grad(log_likelihood_test, theta_hat.values())))\n",
    "\n",
    "    # compute the fisher information matrix for the model, not the target functional\n",
    "    # we use the training data to estimate the fisher information matrix along with theta_hat itself\n",
    "    def _f_hess(flat_theta: torch.tensor) -> torch.tensor:\n",
    "        theta = unflatten_dict(flat_theta, theta_hat)\n",
    "        model_theta_hat_fisher = condition(data=X_train)(condition(data=theta)(model))\n",
    "        log_likelihood_fisher = pyro.poutine.trace(model_theta_hat_fisher).get_trace().log_prob_sum() / X_train[next(iter(X_train))].shape[0]\n",
    "        return log_likelihood_fisher\n",
    "\n",
    "    fisher_info_approx = torch.autograd.functional.hessian(_f_hess, flatten_dict(theta_hat))\n",
    "\n",
    "\n",
    "    # compute the correction\n",
    "    plug_in_grad = flatten_dict(plug_in_grads)\n",
    "    print(plug_in_grads)\n",
    "    score = flatten_dict(scores)\n",
    "    inverse_fisher_info = torch.inverse(fisher_info_approx + eps_fisher * torch.eye(fisher_info_approx.shape[0]))\n",
    "    return torch.einsum(\"i,ij,j->\", plug_in_grad, inverse_fisher_info, score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we generate synthetic data as in Figure 4b of Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 200\n",
    "N_test = N_train  # 500  # TODO refactor model and ATE to not require N_test == N_train\n",
    "benchmark_model_train = BenchmarkLinearModel(N_train, p, gaussian_link, alpha, beta)\n",
    "benchmark_model_test = BenchmarkLinearModel(N_test, p, gaussian_link, alpha, beta)\n",
    "\n",
    "with pyro.poutine.trace() as train_tr:\n",
    "    benchmark_model_train()\n",
    "\n",
    "with pyro.poutine.trace() as test_tr:\n",
    "    benchmark_model_test()\n",
    "\n",
    "D_train = {k: train_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}\n",
    "D_test = {k: test_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0000] loss: 159499.7969\n",
      "[iteration 0250] loss: 140658.6406\n",
      "[iteration 0500] loss: 140658.6250\n",
      "[iteration 0750] loss: 140658.8125\n",
      "[iteration 1000] loss: 140658.9688\n",
      "[iteration 1250] loss: 140658.6875\n",
      "[iteration 1500] loss: 140658.8594\n",
      "[iteration 1750] loss: 140659.0469\n"
     ]
    }
   ],
   "source": [
    "# Fit model to training data (uncorrected)\n",
    "class ConditionModelTrain(HighDimLinearModel):\n",
    "    def forward(self):\n",
    "        with condition(data=D_train):\n",
    "            return super().forward()\n",
    "    \n",
    "model_train = ConditionModelTrain(N_train, p, gaussian_link)\n",
    "guide_train = pyro.infer.autoguide.AutoDelta(model_train)\n",
    "elbo = pyro.infer.Trace_ELBO()(model_train, guide_train)\n",
    "\n",
    "# initialize parameters\n",
    "elbo()\n",
    "\n",
    "adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(2000):\n",
    "    adam.zero_grad()\n",
    "    loss = elbo()\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    if step % 250 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['outcome_weights', 'propensity_weights', 'treatment_weight', 'covariate_loc', 'covariate_scale']) tensor(0.0322, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "theta_hat = {k: v.clone().detach().requires_grad_(True) for k, v in guide_train().items()}\n",
    "print(theta_hat.keys(), theta_hat[\"treatment_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE plugin tensor(0.0330, grad_fn=<SqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model_test = condition(data=theta_hat)(HighDimLinearModel(N_test, p, gaussian_link))\n",
    "\n",
    "ATE_plugin = ATE_2(model_test, num_samples=1000)\n",
    "print(\"ATE plugin\", ATE_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('covariate_loc', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('covariate_scale', tensor([-5.5896e-06, -4.1997e-05,  1.6489e-05, -7.0847e-05, -6.4755e-05,\n",
      "        -3.4727e-04, -2.9203e-06, -1.3285e-05,  4.7168e-06, -6.5451e-05,\n",
      "         8.6187e-05,  4.4543e-05, -8.2022e-05,  2.7587e-05,  3.4366e-06,\n",
      "         7.8227e-07, -1.0434e-04, -1.3374e-04,  5.9705e-06,  9.6449e-05,\n",
      "        -2.3579e-05,  2.2967e-05,  1.6211e-05, -4.4833e-05,  2.8342e-04,\n",
      "         7.3514e-05, -1.4667e-04,  9.1856e-05,  6.9514e-05,  5.6158e-06,\n",
      "        -3.4899e-04, -2.2881e-04, -1.2983e-05, -2.2218e-05,  1.9485e-05,\n",
      "        -3.8277e-05, -2.9039e-06, -8.7971e-06,  1.8247e-05, -2.6275e-04,\n",
      "        -3.7973e-05, -4.1647e-05, -2.8353e-06, -5.1316e-05, -1.2782e-04,\n",
      "         2.0916e-05,  7.8788e-05, -4.4063e-05, -1.1675e-05, -2.9855e-06,\n",
      "         3.8037e-05,  1.3589e-05, -1.9195e-05,  1.4950e-05,  7.2678e-06,\n",
      "        -3.7007e-05,  7.0445e-06,  1.3280e-04,  1.0107e-04,  1.3814e-05,\n",
      "         1.2789e-04,  1.5264e-05,  2.7742e-05, -1.1473e-06, -9.2727e-05,\n",
      "        -6.9534e-06,  2.7843e-05,  2.9593e-05,  9.4005e-05, -1.9485e-05,\n",
      "        -4.6837e-05,  3.8347e-06, -1.5190e-06, -1.5697e-05,  6.4766e-05,\n",
      "        -7.2864e-06, -1.5951e-06,  1.6931e-04, -8.4401e-05,  4.7085e-05,\n",
      "         7.7888e-05, -5.8463e-05, -6.5115e-06, -7.6107e-06, -1.1365e-05,\n",
      "         5.7384e-05,  8.3881e-05, -1.6594e-06,  3.5554e-05, -7.4135e-05,\n",
      "        -1.4029e-05, -3.6678e-05,  1.5775e-05, -1.7825e-04,  3.2591e-05,\n",
      "         1.4357e-04, -2.2492e-08,  3.1012e-05, -5.2138e-05,  6.9885e-05,\n",
      "        -2.3873e-05,  8.9661e-06, -5.0785e-06,  1.6050e-06,  1.1541e-06,\n",
      "         1.7963e-05, -3.1639e-05, -2.3376e-05,  1.2577e-04,  9.3241e-05,\n",
      "         8.5556e-06, -2.0663e-06, -3.4738e-05,  4.0679e-06,  3.0388e-05,\n",
      "        -9.7970e-05, -3.4599e-05, -9.7632e-06, -6.7491e-05,  4.4339e-05,\n",
      "         4.3228e-06, -4.8216e-05,  4.8357e-05,  5.3106e-05, -3.1991e-05,\n",
      "        -4.5790e-06, -2.1558e-05, -1.8086e-05, -8.2313e-05, -1.7067e-06,\n",
      "        -2.2673e-05, -5.2275e-06,  7.8460e-06, -1.4462e-06, -1.8989e-06,\n",
      "        -5.3352e-05,  1.6948e-06,  1.9900e-04, -6.9839e-05, -4.4720e-05,\n",
      "        -5.9083e-05,  6.4253e-05,  2.0969e-05, -1.8151e-06, -2.9757e-05,\n",
      "        -7.9263e-06, -8.8739e-05, -2.0785e-05, -2.2678e-05, -2.6340e-05,\n",
      "         7.9767e-08, -1.9740e-05, -9.9636e-06,  8.9338e-05,  5.0880e-05,\n",
      "         2.8062e-05,  1.3726e-05, -4.3889e-05,  3.3199e-06,  3.2763e-06,\n",
      "        -1.5829e-05, -5.8537e-05, -3.5009e-05, -6.7927e-06, -5.5434e-05,\n",
      "        -2.4783e-06,  1.9941e-06, -4.7793e-05,  6.3138e-06, -1.4624e-05,\n",
      "         1.6739e-06,  6.7744e-05,  1.8371e-05,  3.7956e-06, -1.6599e-05,\n",
      "         5.1347e-05,  9.8389e-07, -2.9714e-05, -4.0253e-05, -1.2560e-04,\n",
      "        -4.6447e-05, -2.8612e-06,  1.8793e-04,  2.6898e-06,  1.7436e-06,\n",
      "         8.5807e-06,  5.3777e-05, -4.6317e-05, -7.4330e-05, -1.0041e-05,\n",
      "        -2.2121e-06,  7.3622e-06, -2.3858e-06,  3.6164e-05,  3.2226e-05,\n",
      "         1.3182e-05, -9.8888e-06, -3.9332e-05, -7.5921e-05, -1.8648e-05,\n",
      "        -3.6710e-05,  6.4357e-05, -8.2517e-05,  2.1050e-06,  4.0680e-05,\n",
      "        -4.9017e-05, -1.5231e-04,  2.6235e-04, -5.0757e-05,  4.3070e-05,\n",
      "         5.6632e-05,  5.2656e-05, -9.9325e-06,  2.3314e-05,  5.9265e-06,\n",
      "        -5.8419e-05,  2.6319e-05, -5.5127e-06, -1.2727e-04, -8.8478e-06,\n",
      "        -1.2629e-04, -1.7370e-04,  3.8936e-05, -1.4110e-04, -1.9412e-06,\n",
      "         1.2539e-06, -1.2379e-05, -1.1969e-04,  4.0523e-05,  1.3043e-05,\n",
      "        -6.9143e-05,  3.1912e-06,  5.1347e-05, -1.3421e-04, -4.2891e-05,\n",
      "         6.7978e-06,  3.5089e-06, -1.7039e-05, -3.7134e-07, -1.3424e-05,\n",
      "         4.0234e-05, -3.1170e-05,  1.0148e-04,  6.5715e-06,  5.5591e-06,\n",
      "        -1.9023e-05,  6.0669e-06,  1.6588e-05, -1.6198e-05, -1.2211e-04,\n",
      "        -2.5575e-05, -2.2926e-05, -6.9420e-05, -8.9577e-05, -1.9553e-07,\n",
      "         8.6569e-05,  2.9409e-06,  6.4452e-05,  4.9822e-06, -4.5382e-05,\n",
      "        -3.8399e-05, -2.1668e-06,  3.9079e-05, -4.4282e-05, -6.1835e-05,\n",
      "         2.2517e-05, -4.0104e-05, -1.5452e-06,  1.3666e-05, -5.2975e-06,\n",
      "        -1.0703e-04,  2.1495e-05,  3.8735e-05,  6.2622e-06, -3.4447e-06,\n",
      "        -6.1936e-06, -3.3311e-06, -3.6035e-06,  1.7075e-05,  3.6137e-05,\n",
      "        -3.8501e-05,  7.0951e-06, -1.4844e-05,  1.7948e-06,  4.9275e-07,\n",
      "         6.3070e-06, -3.2610e-06, -2.1987e-06, -3.6509e-07,  4.6633e-05,\n",
      "         4.3483e-05, -1.5988e-04,  2.4441e-05,  1.7716e-05,  3.1457e-05,\n",
      "         2.3606e-06, -1.8077e-05, -5.5974e-06,  3.5324e-05, -2.4198e-06,\n",
      "         3.0344e-06,  2.2923e-05, -1.7275e-06,  1.9709e-06,  6.4771e-05,\n",
      "        -6.3492e-05,  4.3369e-05,  3.8083e-05,  1.5538e-05, -1.8155e-06,\n",
      "        -2.4744e-05, -1.5962e-05,  2.2433e-04, -2.8790e-05, -1.1594e-05,\n",
      "        -8.1366e-05, -2.6961e-05,  6.9068e-05, -6.5563e-06, -4.2778e-07,\n",
      "         1.5543e-05,  4.3792e-05,  3.3577e-05, -5.0671e-06,  1.4143e-06,\n",
      "         2.2207e-05,  2.3379e-07, -2.0319e-04, -1.1533e-04,  1.4026e-04,\n",
      "        -6.5694e-05, -2.5245e-06,  3.3786e-05,  1.3686e-05,  4.3096e-05,\n",
      "        -1.1030e-04, -4.5157e-06, -2.4301e-06, -1.0876e-05, -8.6819e-07,\n",
      "        -3.6212e-05,  5.4213e-05,  1.8608e-06, -1.4622e-05,  4.8644e-05,\n",
      "         1.3913e-05,  1.1773e-05, -1.4142e-06,  7.6244e-06,  1.1892e-05,\n",
      "        -2.4176e-05, -1.4107e-06, -2.8072e-05,  1.1485e-04, -7.9182e-05,\n",
      "        -5.8166e-06,  4.6039e-05, -3.6465e-05,  8.1207e-06,  3.4012e-06,\n",
      "         1.2608e-07, -3.9801e-05, -4.5494e-05,  1.9905e-08, -6.9342e-05,\n",
      "         9.2211e-05, -1.8939e-05,  2.8212e-05, -6.4140e-06, -2.4308e-05,\n",
      "        -4.4229e-05,  2.9959e-05, -5.0896e-06,  5.1860e-06,  6.1645e-06,\n",
      "        -9.5446e-06,  4.9201e-06,  2.2892e-05,  8.2433e-06,  7.6194e-05,\n",
      "         2.2392e-04,  5.8692e-06,  1.4222e-04, -6.0427e-05,  4.8421e-06,\n",
      "        -2.1961e-05,  6.9315e-06,  8.5234e-05,  2.8695e-05, -3.9612e-05,\n",
      "         1.7166e-04, -9.3084e-06,  9.6927e-05,  2.7645e-05,  2.7667e-06,\n",
      "         5.3013e-05,  1.7815e-05, -6.9436e-06,  1.4804e-05,  4.9581e-06,\n",
      "        -3.7135e-05,  6.2072e-06,  3.2531e-05,  4.0722e-06,  5.5716e-05,\n",
      "         1.2626e-04, -2.0691e-05, -1.4976e-04,  9.9952e-05, -1.1196e-06,\n",
      "        -1.5260e-05, -2.3805e-06, -5.8376e-05,  4.0007e-05, -2.5844e-07,\n",
      "         4.1986e-05, -6.6744e-05,  2.2067e-06,  1.0071e-04, -9.0185e-07,\n",
      "         2.4454e-06, -2.7235e-05,  1.8928e-05, -1.8688e-05,  5.4765e-05,\n",
      "         1.3548e-05, -1.9573e-05,  1.0320e-06,  1.1247e-05, -3.4132e-06,\n",
      "         7.9631e-07, -2.6027e-06, -2.6707e-05,  3.7356e-05, -2.0471e-05,\n",
      "        -1.2853e-04, -7.0053e-05, -8.8340e-06, -2.0795e-05, -2.9885e-06,\n",
      "        -7.2206e-05,  2.9847e-05,  1.6844e-04, -8.6205e-05,  8.6815e-06,\n",
      "        -2.5224e-05, -5.6144e-05, -1.4389e-05, -1.7089e-05,  1.3895e-05,\n",
      "         3.9829e-05,  3.8000e-07, -8.9501e-06, -1.4715e-05,  1.5553e-04,\n",
      "         1.0293e-06, -8.2162e-06, -8.4225e-06, -8.4034e-07,  1.3988e-05,\n",
      "         1.4368e-05, -2.9282e-05,  5.6887e-06, -6.1079e-05,  8.1874e-05,\n",
      "        -3.6564e-05, -5.3346e-06,  7.5219e-06,  1.4804e-05,  3.4722e-05,\n",
      "        -2.6354e-05, -7.4949e-05, -7.4928e-05,  7.7095e-07,  5.7978e-05,\n",
      "         3.3875e-06,  3.1515e-06, -1.5869e-05,  3.5858e-06, -3.7719e-05,\n",
      "        -4.3153e-05, -1.2512e-07,  1.5845e-05,  1.1676e-05,  3.8957e-05,\n",
      "         1.0470e-06, -9.4113e-05,  4.6936e-05, -1.9589e-05, -3.8612e-05,\n",
      "         2.6463e-05,  2.8829e-05, -1.8632e-05,  9.9822e-05,  5.8845e-05,\n",
      "         9.1792e-06, -6.9144e-06, -1.0827e-04,  2.4784e-05, -2.2588e-05])), ('outcome_weights', tensor([-2.8485e-04, -2.2892e-03,  2.2636e-03, -2.9222e-03, -1.8607e-03,\n",
      "        -8.0068e-03, -8.8634e-05,  4.8877e-03,  6.1674e-03, -2.8419e-03,\n",
      "         2.3834e-03, -4.8181e-03, -2.5661e-03,  6.4141e-04,  1.1721e-04,\n",
      "        -5.5370e-04, -4.4055e-03, -4.8881e-03, -7.8411e-04,  3.9608e-03,\n",
      "        -1.4069e-03,  1.2655e-03,  7.8078e-04, -9.5814e-04,  5.4106e-03,\n",
      "         1.1958e-03, -3.2640e-03,  4.6678e-03,  1.4688e-03,  8.9897e-04,\n",
      "        -5.6818e-03, -4.4737e-03, -7.3864e-04, -3.8778e-03,  4.2636e-03,\n",
      "         7.5232e-03, -9.5896e-05, -5.6447e-04,  1.6569e-03, -7.4748e-03,\n",
      "        -9.7644e-04, -1.5032e-03, -1.1580e-03, -5.6207e-03, -2.3296e-03,\n",
      "         5.2020e-04,  1.8373e-03, -1.3815e-03, -4.6181e-04, -8.5553e-05,\n",
      "         1.7100e-03,  4.0484e-04, -5.8520e-04,  4.0373e-03,  8.4554e-04,\n",
      "         4.6221e-03, -1.3160e-03,  6.9213e-03, -3.1203e-03, -3.2650e-03,\n",
      "         4.0342e-03, -1.3672e-03,  9.2895e-04,  8.4088e-04,  5.2301e-03,\n",
      "         3.2644e-04,  3.2183e-03, -8.1445e-03, -3.8247e-03,  3.1404e-03,\n",
      "         2.6643e-03, -4.9651e-04, -9.4417e-05,  9.1568e-04, -1.8982e-03,\n",
      "         1.7444e-03, -4.9033e-04, -5.2090e-03,  3.2759e-03, -2.9355e-03,\n",
      "        -5.1864e-03, -2.0778e-03,  9.0437e-04,  2.3862e-03, -3.7658e-03,\n",
      "        -2.3279e-03, -2.9252e-03,  1.5850e-03,  3.0568e-03, -1.7164e-03,\n",
      "         5.9414e-04,  2.1815e-03,  1.9024e-03,  6.3850e-03,  2.3994e-03,\n",
      "         7.8969e-03,  3.1143e-06,  2.7168e-03, -2.8005e-03, -4.2077e-03,\n",
      "        -7.0538e-04,  5.2823e-04,  1.7622e-04,  1.3871e-04,  3.5059e-04,\n",
      "         5.1259e-03, -6.4079e-03,  2.8252e-03,  4.2805e-03, -3.0749e-03,\n",
      "        -3.8792e-04, -2.0783e-04,  9.9485e-04,  1.8296e-04, -1.5688e-03,\n",
      "        -2.8079e-03, -3.0053e-03,  1.4973e-03, -2.5806e-03,  2.2885e-03,\n",
      "        -2.1961e-03, -1.9449e-03, -5.4048e-03, -2.8512e-03, -1.6144e-03,\n",
      "         9.0070e-04,  9.6020e-04, -1.9884e-03, -4.2996e-03,  1.0633e-03,\n",
      "         1.9356e-03,  1.1738e-03, -7.2276e-04,  5.5298e-05, -4.8133e-04,\n",
      "        -1.5623e-03, -1.5917e-03,  5.2562e-03, -2.7943e-03, -2.1887e-03,\n",
      "        -1.5785e-03, -6.0701e-03,  3.9107e-03, -2.9240e-04,  1.5725e-03,\n",
      "         6.2903e-03,  2.3483e-03, -1.8539e-03,  1.3135e-03, -9.5060e-04,\n",
      "         1.8105e-05, -3.6900e-03,  2.4903e-04,  6.3149e-03, -3.9618e-03,\n",
      "        -3.7178e-03, -1.7616e-03,  2.5843e-03, -1.6731e-04,  1.2743e-03,\n",
      "        -1.2704e-03,  4.1221e-03,  2.4292e-03, -5.0306e-04, -2.5939e-03,\n",
      "         2.0887e-04, -5.7593e-05,  5.1827e-03,  1.1908e-03,  4.3179e-03,\n",
      "         8.5485e-04, -7.8405e-03,  2.9599e-03, -1.9322e-03, -7.7064e-04,\n",
      "         3.0978e-03,  7.1177e-04, -1.6472e-03, -1.6090e-03, -4.0214e-03,\n",
      "         2.5389e-03, -6.2064e-04, -9.5366e-03,  3.3594e-04,  1.8526e-04,\n",
      "        -2.7862e-03, -4.6071e-03, -2.4290e-03,  4.0168e-03,  3.5813e-04,\n",
      "         1.2810e-03,  1.7047e-03,  1.7390e-03, -1.8819e-03, -2.6646e-03,\n",
      "         7.4482e-04,  1.4180e-03, -4.1868e-03,  3.4972e-03,  3.4891e-03,\n",
      "         4.3428e-03,  2.4481e-03, -4.3234e-03, -4.0261e-04, -2.0607e-03,\n",
      "        -2.6604e-03,  6.9771e-03, -6.9887e-03, -4.3490e-03, -5.7796e-03,\n",
      "         3.4924e-03, -5.7367e-03,  3.1675e-03,  5.0959e-04, -6.3199e-04,\n",
      "        -6.0384e-03, -1.6415e-03,  1.3054e-03, -4.5417e-03,  3.5632e-04,\n",
      "        -4.5335e-03, -5.1658e-03,  5.2511e-03,  5.2194e-03,  8.2679e-04,\n",
      "        -1.8260e-03, -7.6765e-04,  3.7138e-03, -5.8544e-03, -1.2674e-03,\n",
      "        -3.2932e-03,  2.0530e-03, -6.3632e-03,  4.8918e-03, -2.1653e-03,\n",
      "         2.1132e-03, -7.0041e-04, -1.7466e-03,  5.2508e-04,  3.9601e-03,\n",
      "        -6.3254e-03, -2.6575e-03,  4.0052e-03,  1.0848e-03,  9.6907e-04,\n",
      "         1.6843e-03, -1.4639e-03, -3.3282e-03,  2.1500e-03,  7.8867e-03,\n",
      "         1.0774e-03, -3.8042e-03,  1.2334e-03, -4.5500e-03, -1.0978e-05,\n",
      "        -2.8365e-03,  4.0081e-04, -2.6019e-03, -1.0892e-03,  9.0555e-04,\n",
      "        -8.0968e-03,  1.1010e-04,  2.9183e-03,  3.7121e-03, -5.2836e-03,\n",
      "         7.4889e-04, -5.9456e-03, -1.2247e-04,  1.4965e-03, -3.1987e-04,\n",
      "        -5.2133e-03, -4.9987e-03,  3.8246e-03, -2.7384e-03, -2.5101e-04,\n",
      "         1.0459e-03, -1.6059e-03,  3.1889e-04,  2.5469e-03,  6.8574e-03,\n",
      "        -3.0501e-03, -1.0366e-03, -1.1514e-03,  1.6165e-04,  2.4379e-04,\n",
      "         5.2782e-04, -1.4485e-03, -3.8364e-04, -4.4674e-05,  3.5028e-03,\n",
      "         3.1609e-03,  5.8593e-03, -1.3005e-03, -4.3042e-03, -1.7274e-03,\n",
      "         5.2443e-03, -1.9408e-03, -3.4155e-03,  1.4886e-03, -3.3961e-04,\n",
      "        -6.8294e-04,  1.3899e-03,  1.2985e-03,  3.3663e-03,  3.0589e-03,\n",
      "        -3.7492e-03,  1.8416e-03, -1.8927e-03, -8.4490e-04, -9.9887e-04,\n",
      "         1.8487e-03, -1.1718e-03,  7.0355e-03, -2.3916e-03,  2.0962e-03,\n",
      "        -2.8987e-03, -1.5172e-03, -5.5256e-03, -2.7092e-04,  1.0205e-03,\n",
      "         3.6675e-03, -2.0444e-03, -7.6793e-03, -3.2608e-04,  3.5745e-04,\n",
      "         1.6660e-03, -1.2360e-03, -5.8489e-03,  7.9423e-03,  5.4123e-03,\n",
      "        -3.0911e-03,  5.6639e-04, -1.5492e-03,  9.2827e-04,  2.2748e-03,\n",
      "         4.0288e-03, -2.1630e-03,  9.6131e-04, -5.5459e-04,  7.9643e-05,\n",
      "         3.2154e-03, -2.5386e-03, -1.7366e-03, -3.4189e-03, -2.7055e-03,\n",
      "        -1.3300e-03, -1.2538e-03,  6.3369e-04, -8.9924e-04, -3.7953e-04,\n",
      "        -3.9458e-03,  3.8296e-03,  1.0692e-03, -3.4486e-03,  3.7800e-03,\n",
      "        -7.3401e-04, -3.2224e-03, -2.2080e-03,  8.9442e-04, -4.1934e-03,\n",
      "        -2.0429e-05,  3.4905e-03, -1.6070e-03, -1.4678e-06, -6.2981e-03,\n",
      "         4.0216e-03,  2.6423e-03,  2.9892e-03,  6.4629e-04,  2.3143e-03,\n",
      "        -2.8821e-03, -3.5574e-03,  2.0579e-04,  2.5137e-03,  2.0960e-03,\n",
      "         6.2440e-04,  3.0187e-04, -1.5760e-03,  2.0383e-04,  5.2462e-03,\n",
      "         6.7261e-03, -4.6009e-03,  8.4842e-03, -4.0034e-03, -7.6434e-04,\n",
      "        -2.4857e-03, -1.1711e-03, -2.6551e-03, -3.5856e-03,  3.9821e-03,\n",
      "         6.7254e-03,  4.8724e-04, -4.0668e-03,  3.4537e-03,  9.4236e-04,\n",
      "         2.7492e-03,  2.8341e-03, -9.6171e-04,  1.8872e-03,  5.7278e-04,\n",
      "         3.6933e-03, -1.9311e-03,  4.3543e-03, -5.6961e-04,  4.8400e-03,\n",
      "        -4.9383e-03, -2.6427e-03,  4.4732e-03, -3.2602e-03,  5.3857e-05,\n",
      "         1.1574e-03, -1.1203e-03,  5.4281e-03,  2.9339e-03,  1.4395e-04,\n",
      "         2.6105e-03,  2.4582e-03,  1.0686e-04,  4.4097e-03,  9.3536e-05,\n",
      "        -7.2765e-03, -1.6645e-03,  3.7558e-03, -2.4148e-03,  2.3637e-03,\n",
      "        -9.1027e-04, -3.1415e-03, -3.3543e-05, -8.0908e-04,  2.4624e-03,\n",
      "         4.0504e-04, -4.7470e-03, -2.8860e-03, -8.0559e-03,  2.6831e-03,\n",
      "        -6.0710e-03,  2.1697e-03, -1.3973e-03,  1.8647e-03, -1.2826e-04,\n",
      "         2.5070e-03,  2.0634e-03,  4.0733e-03,  3.9066e-03,  2.5400e-03,\n",
      "        -3.7094e-03,  3.3587e-03, -6.4881e-04,  1.2011e-03,  9.6199e-04,\n",
      "         1.8388e-03,  5.6490e-05,  3.1066e-04, -3.4861e-03, -5.3810e-03,\n",
      "         1.3705e-04, -2.2216e-03, -5.0648e-04, -2.5143e-04,  4.8181e-03,\n",
      "        -3.4538e-03, -7.8208e-03,  6.0640e-04,  3.0978e-03,  3.2856e-03,\n",
      "        -1.1357e-03, -5.6768e-03,  3.0587e-03, -4.3810e-03,  1.7736e-03,\n",
      "         4.3230e-03,  3.8143e-03,  3.2654e-03, -7.8328e-05, -2.8200e-03,\n",
      "         4.9246e-04, -3.4302e-04,  6.4125e-04, -2.1434e-03, -2.6984e-03,\n",
      "         1.8081e-03,  6.6850e-04, -1.3389e-03,  7.8873e-04,  1.6371e-03,\n",
      "         1.8725e-04, -3.3296e-03, -3.1419e-03,  2.2939e-03, -7.1655e-03,\n",
      "         1.4544e-03, -2.6467e-03,  1.6609e-03,  2.9007e-03, -2.8864e-03,\n",
      "        -9.9383e-04,  1.0416e-03,  3.9479e-03,  1.3012e-03,  5.2614e-03])), ('propensity_weights', tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])), ('treatment_weight', tensor(1.0000))])\n",
      "tensor(0.0330, grad_fn=<SqueezeBackward0>) tensor(-0.4115) tensor(-0.3785, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ATE_correction = one_step_correction(\n",
    "    lambda m: ATE_2(m, num_samples=1000),\n",
    "    HighDimLinearModel(N_test, p, gaussian_link),\n",
    "    theta_hat,\n",
    "    D_train,\n",
    "    D_test,\n",
    "    eps_fisher=1e-10,\n",
    ")\n",
    "ATE_onestep = ATE_plugin + ATE_correction\n",
    "print(ATE_plugin, ATE_correction, ATE_onestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
