{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated doubly robust estimation with ChiRho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal\n",
    "from pyro.infer import Predictive\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_link = lambda mu: dist.Normal(mu, 1.)\n",
    "bernoulli_link = lambda mu: dist.Bernoulli(logits=mu)\n",
    "\n",
    "class HighDimLinearModel(pyro.nn.PyroModule):\n",
    "    def __init__(self, p: int, link_fn: Callable[..., dist.Distribution] = gaussian_link):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\"outcome_weights\", dist.Normal(0.,  1./math.sqrt(self.p)).expand((self.p, )).to_event(1))\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\"propensity_weights\", dist.Normal(0., 1./math.sqrt(self.p)).expand((self.p,)).to_event(1))\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0., 1.))\n",
    "    \n",
    "    def sample_covariate_loc_scale(self):\n",
    "        loc = pyro.sample(\"covariate_loc\", dist.Normal(0., 1.).expand((self.p,)).to_event(1))\n",
    "        scale = pyro.sample(\"covariate_scale\", dist.LogNormal(0, 1).expand((self.p,)).to_event(1))\n",
    "        return loc, scale\n",
    "    \n",
    "    def forward(self, N: int):\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "        with pyro.plate(\"obs\", N, dim=-1):\n",
    "            X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1))\n",
    "            A = pyro.sample(\"A\", dist.Bernoulli(logits=torch.einsum(\"...np,...p->...n\", X, propensity_weights)))\n",
    "            return pyro.sample(\"Y\", self.link_fn(torch.einsum(\"...np,...p->...n\", X, outcome_weights) + A * tau))\n",
    "        \n",
    "        \n",
    "class KnownCovariateDistModel(HighDimLinearModel):\n",
    "    def sample_covariate_loc_scale(self):\n",
    "        return torch.zeros(self.p), torch.ones(self.p)\n",
    "\n",
    "\n",
    "class BenchmarkLinearModel(HighDimLinearModel):\n",
    "    def __init__(self, p: int, link_fn: Callable, alpha: int, beta: int, treatment_weight: float = 0.):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha # sparsity of propensity weights\n",
    "        self.beta = beta # sparisty of outcome weights\n",
    "        self.treatment_weight = treatment_weight\n",
    "    \n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta:] = 0.\n",
    "        return outcome_weights\n",
    "    \n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(4 * self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha:] = 0.\n",
    "        return propensity_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(self.treatment_weight)\n",
    "    \n",
    "    def sample_covariate_loc_scale(self):\n",
    "        return torch.zeros(self.p), torch.ones(self.p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(d: Dict[str, torch.tensor]) -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Flatten a dictionary of tensors into a single vector.\n",
    "    \"\"\"\n",
    "    return torch.cat([v.flatten() for k, v in d.items()])\n",
    "\n",
    "\n",
    "def unflatten_dict(x: torch.tensor, d: Dict[str, torch.tensor]) -> Dict[str, torch.tensor]:\n",
    "    \"\"\"\n",
    "    Unflatten a vector into a dictionary of tensors.\n",
    "    \"\"\"\n",
    "    return collections.OrderedDict(zip(\n",
    "        d.keys(), [v_flat.reshape(v.shape) for v, v_flat in zip(d.values(), torch.split(x, [v.numel() for k, v in d.items()]))]\n",
    "    ))\n",
    "\n",
    "\n",
    "def monte_carlo_fisher_info_of_model(\n",
    "    unconditioned_model: Callable[[], torch.tensor], # simulates data\n",
    "    conditioned_model: Callable[[], torch.tensor], # computes log likelihood\n",
    "    theta_hat: Dict[str, torch.tensor], \n",
    "    obs_names: List[str], \n",
    "    N_monte_carlo: int = None\n",
    ") -> torch.tensor:\n",
    "    \"\"\"\n",
    "    Compute the monte carlo estimate of the fisher information matrix.\n",
    "    \"\"\"\n",
    "    flat_theta = flatten_dict(theta_hat)\n",
    "    theta_dim = flat_theta.shape[0]\n",
    "    model_theta_hat_unconditioned = condition(data=theta_hat)(unconditioned_model)\n",
    "    if N_monte_carlo is None:\n",
    "        N_monte_carlo = 10 * theta_dim # 10 samples per parameter\n",
    "    else:\n",
    "        assert N_monte_carlo >= theta_dim, \"N_monte_carlo must be at least as large as the number of parameters\"\n",
    "        if N_monte_carlo < 10 * theta_dim:\n",
    "            print(\"Warning: N_monte_carlo is less than 10 times the number of parameters. This may lead to inaccurate estimates.\")\n",
    "    \n",
    "    # Generate N_monte_carlo samples from the model\n",
    "    with pyro.poutine.trace() as model_tr:\n",
    "        model_theta_hat_unconditioned(N=N_monte_carlo)\n",
    "    D_model = {k: model_tr.trace.nodes[k][\"value\"] for k in obs_names}\n",
    "\n",
    "    # Compute fisher information matrix from these samples    \n",
    "    def _log_prob_at_datapoints(flat_theta: torch.tensor):\n",
    "        # Need to duplicate conditioning on theta for pytorch to register gradients (TODO: any fix?)\n",
    "        theta = unflatten_dict(flat_theta, theta_hat)\n",
    "        model_theta_hat_conditioned = condition(data=theta_hat)(conditioned_model)\n",
    "        log_like_trace = pyro.poutine.trace(model_theta_hat_conditioned).get_trace(D_model)\n",
    "        log_like_trace.compute_log_prob()\n",
    "        log_prob_at_datapoints = torch.zeros(N_monte_carlo)\n",
    "        for name in obs_names:\n",
    "            log_prob_at_datapoints += log_like_trace.nodes[name][\"log_prob\"]\n",
    "        return log_prob_at_datapoints\n",
    "        \n",
    "    log_prob_grads = torch.autograd.functional.jacobian(_log_prob_at_datapoints, flat_theta)\n",
    "    assert log_prob_grads.shape[0] == N_monte_carlo\n",
    "    assert log_prob_grads.shape[1] == theta_dim\n",
    "    return 1 / N_monte_carlo * log_prob_grads.T.mm(log_prob_grads)\n",
    "\n",
    "\n",
    "def one_step_correction(\n",
    "    target_functional: Callable[[Callable], torch.tensor],\n",
    "    unconditioned_model: Callable[[], torch.tensor], # simulates data\n",
    "    conditioned_model: Callable[[], torch.tensor], # computes log likelihood\n",
    "    obs_names: List[str],\n",
    "    theta_hat: Dict[str, torch.tensor],\n",
    "    X_test: Dict[str, torch.tensor],\n",
    "    *,\n",
    "    eps_fisher: float = 1e-8,\n",
    "    N_monte_carlo: int = None\n",
    ") -> torch.tensor:\n",
    "    \"\"\"\n",
    "    One step correction for a given target functional.\n",
    "    \"\"\"\n",
    "    theta_hat = collections.OrderedDict((k, theta_hat[k]) for k in sorted(theta_hat.keys()))\n",
    "    flat_theta = flatten_dict(theta_hat)\n",
    "    model_theta_hat_unconditioned = condition(data=theta_hat)(unconditioned_model)\n",
    "    model_theta_hat_conditioned = condition(data=theta_hat)(conditioned_model)\n",
    "\n",
    "    plug_in = target_functional(model_theta_hat_unconditioned) # + (0 * flat_theta.sum()) # hack to make sure we get full gradient vector\n",
    "    plug_in_grads = torch.autograd.grad(plug_in, flat_theta, allow_unused=True)\n",
    "    \n",
    "    # compute the score function for the new data\n",
    "    log_likelihood_test = pyro.poutine.trace(model_theta_hat_conditioned).get_trace(X_test).log_prob_sum() / X_test[next(iter(X_test))].shape[0]\n",
    "    scores = torch.autograd.grad(log_likelihood_test, flat_theta, allow_unused=True)\n",
    "\n",
    "    # compute inverse fisher information matrix\n",
    "    fisher_info_approx = monte_carlo_fisher_info_of_model(unconditioned_model, conditioned_model, theta_hat, obs_names, N_monte_carlo)\n",
    "    inverse_fisher_info = torch.inverse(fisher_info_approx + eps_fisher * torch.eye(fisher_info_approx.shape[0]))\n",
    "\n",
    "    # compute the correction\n",
    "    print(plug_in_grads)\n",
    "    print(inverse_fisher_info)\n",
    "    print(plug_in_grads, inverse_fisher_info.shape, scores.shape)\n",
    "    return torch.einsum(\"i,ij,j->\", plug_in_grads, inverse_fisher_info, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 500\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 200\n",
    "N_test = 500\n",
    "benchmark_model = BenchmarkLinearModel(p, gaussian_link, alpha, beta)\n",
    "\n",
    "with pyro.poutine.trace() as train_tr:\n",
    "    benchmark_model(N=N_train)\n",
    "\n",
    "with pyro.poutine.trace() as test_tr:\n",
    "    benchmark_model(N=N_test)\n",
    "\n",
    "D_train = {k: train_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}\n",
    "D_test = {k: test_tr.trace.nodes[k][\"value\"] for k in [\"X\", \"A\", \"Y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0000] loss: 140162.1875\n",
      "[iteration 0250] loss: 139938.5625\n",
      "[iteration 0500] loss: 139938.5938\n",
      "[iteration 0750] loss: 139938.9688\n",
      "[iteration 1000] loss: 139938.9375\n",
      "[iteration 1250] loss: 139938.7969\n",
      "[iteration 1500] loss: 139938.6875\n",
      "[iteration 1750] loss: 139939.2969\n"
     ]
    }
   ],
   "source": [
    "# Fit model to training data (uncorrected)\n",
    "class ConditionedModel(KnownCovariateDistModel):\n",
    "    def forward(self, D):\n",
    "        with condition(data=D):\n",
    "            # Assume first dimension corresponds to # of datapoints\n",
    "            N = D[next(iter(D))].shape[0]\n",
    "            return super().forward(N=N)\n",
    "\n",
    "conditioned_model = ConditionedModel(p, gaussian_link)\n",
    "guide_train = pyro.infer.autoguide.AutoDelta(conditioned_model)\n",
    "elbo = pyro.infer.Trace_ELBO()(conditioned_model, guide_train)\n",
    "\n",
    "# initialize parameters\n",
    "elbo(D_train)\n",
    "\n",
    "adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "\n",
    "# Do gradient steps\n",
    "for step in range(2000):\n",
    "    adam.zero_grad()\n",
    "    loss = elbo(D_train)\n",
    "    loss.backward()\n",
    "    adam.step()\n",
    "    if step % 250 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['outcome_weights', 'propensity_weights', 'treatment_weight']) tensor(0.4516, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "theta_hat = {k: v.clone().detach().requires_grad_(True) for k, v in guide_train().items()}\n",
    "print(theta_hat.keys(), theta_hat[\"treatment_weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE plugin tensor([0.4626], grad_fn=<ExpandBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def ATE(model: Callable[[], torch.Tensor], num_samples: int = 100) -> torch.Tensor:\n",
    "    \"\"\"Compute the average treatment effect of a model.\"\"\"\n",
    "    @pyro.plate(\"num_samples\", num_samples, dim=-2)\n",
    "    def _ate_model():\n",
    "        with MultiWorldCounterfactual():\n",
    "            with do(actions=dict(A=(torch.tensor(0.), torch.tensor(1.)))):\n",
    "                Ys = model()\n",
    "            Y0 = gather(Ys, IndexSet(A={1}), event_dim=0)\n",
    "            Y1 = gather(Ys, IndexSet(A={2}), event_dim=0)\n",
    "            return pyro.deterministic(\"ATE\", (Y1 - Y0).mean(dim=-1, keepdim=True))\n",
    "    \n",
    "    return _ate_model().mean(dim=-2, keepdim=True).squeeze()\n",
    "\n",
    "\n",
    "# TODO: check this with samples removed\n",
    "def ATE_2(model: Callable[[], torch.Tensor], num_samples: int = 100) -> torch.Tensor:\n",
    "    \"\"\"Compute the average treatment effect of a model.\"\"\"\n",
    "    with do(actions=dict(A=torch.tensor(0.))):\n",
    "        Y0 = model(N=num_samples)\n",
    "    with do(actions=dict(A=torch.tensor(1.))):\n",
    "        Y1 = model(N=num_samples)\n",
    "    \n",
    "    return pyro.deterministic(\"ATE\", (Y1 - Y0).mean(dim=-1, keepdim=True))\n",
    "\n",
    "\n",
    "unconditioned_model = KnownCovariateDistModel(p, gaussian_link)\n",
    "model_cond_theta = condition(data=theta_hat)(unconditioned_model)\n",
    "\n",
    "ATE_plugin = ATE_2(model_cond_theta, num_samples=10000)\n",
    "print(\"ATE plugin\", ATE_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None,)\n",
      "tensor([[1.0000e+10, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+10, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+10,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+10, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0000e+10,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.0000e+10]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ATE_correction \u001b[39m=\u001b[39m one_step_correction(\n\u001b[1;32m      2\u001b[0m     \u001b[39mlambda\u001b[39;49;00m m: ATE_2(m, num_samples\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m),\n\u001b[1;32m      3\u001b[0m     unconditioned_model,\n\u001b[1;32m      4\u001b[0m     conditioned_model,\n\u001b[1;32m      5\u001b[0m     [\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mA\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mY\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      6\u001b[0m     theta_hat,\n\u001b[1;32m      7\u001b[0m     D_test,\n\u001b[1;32m      8\u001b[0m     eps_fisher\u001b[39m=\u001b[39;49m\u001b[39m1e-10\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m ATE_onestep \u001b[39m=\u001b[39m ATE_plugin \u001b[39m+\u001b[39m ATE_correction\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(ATE_plugin, ATE_correction, ATE_onestep)\n",
      "Cell \u001b[0;32mIn[48], line 93\u001b[0m, in \u001b[0;36mone_step_correction\u001b[0;34m(target_functional, unconditioned_model, conditioned_model, obs_names, theta_hat, X_test, eps_fisher, N_monte_carlo)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mprint\u001b[39m(plug_in_grads)\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m(inverse_fisher_info)\n\u001b[0;32m---> 93\u001b[0m \u001b[39mprint\u001b[39m(plug_in_grads, inverse_fisher_info\u001b[39m.\u001b[39mshape, scores\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39meinsum(\u001b[39m\"\u001b[39m\u001b[39mi,ij,j->\u001b[39m\u001b[39m\"\u001b[39m, plug_in_grads, inverse_fisher_info, scores)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ATE_correction = one_step_correction(\n",
    "    lambda m: ATE_2(m, num_samples=1000),\n",
    "    unconditioned_model,\n",
    "    conditioned_model,\n",
    "    [\"X\", \"A\", \"Y\"],\n",
    "    theta_hat,\n",
    "    D_test,\n",
    "    eps_fisher=1e-10,\n",
    ")\n",
    "ATE_onestep = ATE_plugin + ATE_correction\n",
    "print(ATE_plugin, ATE_correction, ATE_onestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
