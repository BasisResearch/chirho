{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated doubly robust estimation with ChiRho - TMLE Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "- [Setup](#setup)\n",
    "\n",
    "- [Overview: Systematically adjusting for observed confounding](#overview:-systematically-adjusting-for-observed-confounding)\n",
    "    - [Task: Treatment effect estimation with observational data](#task:-treatment-effect-estimation-with-observational-data)\n",
    "    - [Challenge: Confounding](#challenge:-confounding)\n",
    "    - [Assumptions: All confounders observed](#assumptions:-all-confounders-observed)\n",
    "    - [Intuition: Statistically adjusting for confounding](#intuition:-statistically-adjusting-for-confounding)\n",
    "\n",
    "- [Causal Probabilistic Program](#causal-probabilistic-program)\n",
    "    - [Model description](#model-description)\n",
    "    - [Generating data](#generating-data)\n",
    "    - [Fit parameters via maximum likelihood](#fit-parameters-via-maximum-likelihood)\n",
    "\n",
    "- [Causal Query: average treatment effect (ATE)](#causal-query:-average-treatment-effect-\\(ATE\\))\n",
    "    - [Defining the target functional](#defining-the-target-functional)\n",
    "    - [Closed form doubly robust correction](#closed-form-doubly-robust-correction)\n",
    "    - [Computing automated doubly robust correction via Monte Carlo](#computing-automated-doubly-robust-correction-via-monte-carlo)\n",
    "    - [Results](#results)\n",
    "\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we install the necessary Pytorch, Pyro, and ChiRho dependencies for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import Predictive\n",
    "import pyro.contrib.gp as gp\n",
    "\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.robust.internals.utils import ParamDict\n",
    "from chirho.robust.handlers.estimators import one_step_corrected_estimator, tmle\n",
    "from chirho.robust.handlers.predictive import PredictiveModel \n",
    "\n",
    "pyro.settings.set(module_local_params=True)\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "pyro.set_rng_seed(321) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, we will use ChiRho to estimate the average treatment effect (ATE) from observational data. We will use a simple example to illustrate the basic concepts of doubly robust estimation and how ChiRho can be used to automate the process for more general summaries of interest. \n",
    "\n",
    "There are five main steps to our doubly robust estimation procedure but only the last step is different from a standard probabilistic programming workflow:\n",
    "1. Write model of interest\n",
    "    - Define probabilistic model of interest using Pyro\n",
    "2. Feed in data\n",
    "    - Observed data used to train the model\n",
    "3. Run inference\n",
    "    - Use Pyro's rich inference library to fit the model to the data\n",
    "4. Define target functional\n",
    "    - This is the model summary of interest (e.g. average treatment effect)\n",
    "5. Compute robust estimate\n",
    "    - Use ChiRho to compute the doubly robust estimate of the target functional\n",
    "    - Importantly, this step is automated and does not require refitting the model for each new functional\n",
    "\n",
    "\n",
    "Our proposed automated robust inference pipeline is summarized in the figure below.\n",
    "\n",
    "![fig1](figures/robust_pipeline.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Probabilistic Program\n",
    "\n",
    "### Model Description\n",
    "In this example, we will focus on a cannonical model `CausalGLM` consisting of three types of variables: binary treatment (`A`), confounders (`X`), and response (`Y`). For simplicitly, we assume that the response is generated from a generalized linear model with link function $g$. The model is described by the following generative process:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X &\\sim \\text{Normal}(0, I_p) \\\\\n",
    "A &\\sim \\text{Bernoulli}(\\pi(X)) \\\\\n",
    "\\mu &= \\beta_0 + \\beta_1^T X + \\tau A \\\\\n",
    "Y &\\sim \\text{ExponentialFamily}(\\text{mean} = g^{-1}(\\mu))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $p$ denotes the number of confounders, $\\pi(X)$ is the probability of treatment conditional on confounders $X$, $\\beta_0$ is the intercept, $\\beta_1$ is the confounder effect, and $\\tau$ is the treatment effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalGLM(pyro.nn.PyroModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p: int,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        prior_scale: Optional[float] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        self.link_fn = link_fn\n",
    "        if prior_scale is None:\n",
    "            self.prior_scale = 1 / math.sqrt(self.p)\n",
    "        else:\n",
    "            self.prior_scale = prior_scale\n",
    "\n",
    "    def sample_outcome_weights(self):\n",
    "        return pyro.sample(\n",
    "            \"outcome_weights\",\n",
    "            dist.Normal(0.0, self.prior_scale).expand((self.p,)).to_event(1),\n",
    "        )\n",
    "\n",
    "    def sample_intercept(self):\n",
    "        return pyro.sample(\"intercept\", dist.Normal(0.0, 1.0))\n",
    "\n",
    "    def sample_propensity_weights(self):\n",
    "        return pyro.sample(\n",
    "            \"propensity_weights\",\n",
    "            dist.Normal(0.0, self.prior_scale).expand((self.p,)).to_event(1),\n",
    "        )\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return pyro.sample(\"treatment_weight\", dist.Normal(0.0, 1.0))\n",
    "\n",
    "    def sample_covariate_loc_scale(self):\n",
    "        return torch.zeros(self.p), torch.ones(self.p)\n",
    "\n",
    "    def forward(self):\n",
    "        intercept = self.sample_intercept()\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "        X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1))\n",
    "        A = pyro.sample(\n",
    "            \"A\",\n",
    "            dist.Bernoulli(\n",
    "                logits=torch.einsum(\"...i,...i->...\", X, propensity_weights)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return pyro.sample(\n",
    "            \"Y\",\n",
    "            self.link_fn(\n",
    "                torch.einsum(\"...i,...i->...\", X, outcome_weights) + A * tau + intercept\n",
    "            ),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will condition on both treatment and confounders to estimate the causal effect of treatment on the outcome. We will use the following causal probabilistic program to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionedCausalGLM(CausalGLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X: torch.Tensor,\n",
    "        A: torch.Tensor,\n",
    "        Y: torch.Tensor,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        prior_scale: Optional[float] = None,\n",
    "    ):\n",
    "        p = X.shape[1]\n",
    "        super().__init__(p, link_fn, prior_scale)\n",
    "        self.X = X\n",
    "        self.A = A\n",
    "        self.Y = Y\n",
    "\n",
    "    def forward(self):\n",
    "        intercept = self.sample_intercept()\n",
    "        outcome_weights = self.sample_outcome_weights()\n",
    "        propensity_weights = self.sample_propensity_weights()\n",
    "        tau = self.sample_treatment_weight()\n",
    "        x_loc, x_scale = self.sample_covariate_loc_scale()\n",
    "        with pyro.plate(\"__train__\", size=self.X.shape[0], dim=-1):\n",
    "            X = pyro.sample(\"X\", dist.Normal(x_loc, x_scale).to_event(1), obs=self.X)\n",
    "            A = pyro.sample(\n",
    "                \"A\",\n",
    "                dist.Bernoulli(\n",
    "                    logits=torch.einsum(\"ni,i->n\", self.X, propensity_weights)\n",
    "                ),\n",
    "                obs=self.A,\n",
    "            )\n",
    "            pyro.sample(\n",
    "                \"Y\",\n",
    "                self.link_fn(\n",
    "                    torch.einsum(\"ni,i->n\", X, outcome_weights)\n",
    "                    + A * tau\n",
    "                    + intercept\n",
    "                ),\n",
    "                obs=self.Y,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 6.0.2 (20221011.1828)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"563pt\" height=\"304pt\"\n",
       " viewBox=\"0.00 0.00 563.39 304.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-300 559.39,-300 559.39,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster___train__</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"93.6,-8 93.6,-155 235.6,-155 235.6,-8 93.6,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"201.6\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">__train__</text>\n",
       "</g>\n",
       "<!-- intercept -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>intercept</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"41.6\" cy=\"-129\" rx=\"41.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.6\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">intercept</text>\n",
       "</g>\n",
       "<!-- Y -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>Y</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"200.6\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.6\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">Y</text>\n",
       "</g>\n",
       "<!-- intercept&#45;&gt;Y -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>intercept&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.66,-115.65C97.53,-103.38 140.19,-84.59 169.18,-71.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170.71,-74.98 178.45,-67.75 167.89,-68.58 170.71,-74.98\"/>\n",
       "</g>\n",
       "<!-- outcome_weights -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>outcome_weights</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"318.6\" cy=\"-129\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.6\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">outcome_weights</text>\n",
       "</g>\n",
       "<!-- outcome_weights&#45;&gt;Y -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>outcome_weights&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.82,-112.12C273.12,-101.02 248.19,-86.23 229.12,-74.92\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"230.67,-71.77 220.28,-69.68 227.1,-77.79 230.67,-71.77\"/>\n",
       "</g>\n",
       "<!-- propensity_weights -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>propensity_weights</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"128.6\" cy=\"-239.5\" rx=\"79.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.6\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\">propensity_weights</text>\n",
       "</g>\n",
       "<!-- A -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"128.6\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.6\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">A</text>\n",
       "</g>\n",
       "<!-- propensity_weights&#45;&gt;A -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>propensity_weights&#45;&gt;A</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.6,-221.07C128.6,-203.8 128.6,-177.12 128.6,-157.09\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"132.1,-157.03 128.6,-147.03 125.1,-157.03 132.1,-157.03\"/>\n",
       "</g>\n",
       "<!-- treatment_weight -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>treatment_weight</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"482.6\" cy=\"-129\" rx=\"72.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"482.6\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">treatment_weight</text>\n",
       "</g>\n",
       "<!-- treatment_weight&#45;&gt;Y -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>treatment_weight&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M433.15,-115.73C376.52,-101.67 285.18,-79 235.5,-66.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"236.11,-63.21 225.56,-64.2 234.42,-70 236.11,-63.21\"/>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"200.6\" cy=\"-129\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"200.6\" y=\"-125.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- X&#45;&gt;Y -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M200.6,-110.7C200.6,-102.98 200.6,-93.71 200.6,-85.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"204.1,-85.1 200.6,-75.1 197.1,-85.1 204.1,-85.1\"/>\n",
       "</g>\n",
       "<!-- A&#45;&gt;Y -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>A&#45;&gt;Y</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M143.17,-113.83C153.35,-103.94 167.12,-90.55 178.63,-79.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"181.07,-81.87 185.8,-72.38 176.19,-76.85 181.07,-81.87\"/>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-280.8\" font-family=\"Times,serif\" font-size=\"14.00\">intercept ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-265.8\" font-family=\"Times,serif\" font-size=\"14.00\">outcome_weights ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-250.8\" font-family=\"Times,serif\" font-size=\"14.00\">propensity_weights ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-235.8\" font-family=\"Times,serif\" font-size=\"14.00\">treatment_weight ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-220.8\" font-family=\"Times,serif\" font-size=\"14.00\">X ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-205.8\" font-family=\"Times,serif\" font-size=\"14.00\">A ~ Bernoulli</text>\n",
       "<text text-anchor=\"start\" x=\"234.1\" y=\"-190.8\" font-family=\"Times,serif\" font-size=\"14.00\">Y ~ Normal</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x187b07dd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the model\n",
    "pyro.render_model(\n",
    "    ConditionedCausalGLM(torch.zeros(1, 1), torch.zeros(1), torch.zeros(1)),\n",
    "    render_params=True, \n",
    "    render_distributions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data\n",
    "\n",
    "For evaluation, we generate `N_datasets` datasets, each with `N` samples. We compare vanilla estimates of the target functional with the double robust estimates of the target functional across the `N_sims` datasets. We use a similar data generating process as in Kennedy (2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruthModel(CausalGLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        p: int,\n",
    "        alpha: int,\n",
    "        beta: int,\n",
    "        link_fn: Callable[..., dist.Distribution] = lambda mu: dist.Normal(mu, 1.0),\n",
    "        treatment_weight: float = 0.0,\n",
    "    ):\n",
    "        super().__init__(p, link_fn)\n",
    "        self.alpha = alpha  # sparsity of propensity weights\n",
    "        self.beta = beta  # sparsity of outcome weights\n",
    "        self.treatment_weight = treatment_weight\n",
    "\n",
    "    def sample_outcome_weights(self):\n",
    "        outcome_weights = 1 / math.sqrt(self.beta) * torch.ones(self.p)\n",
    "        outcome_weights[self.beta :] = 0.0\n",
    "        return outcome_weights\n",
    "\n",
    "    def sample_propensity_weights(self):\n",
    "        propensity_weights = 1 / math.sqrt(self.alpha) * torch.ones(self.p)\n",
    "        propensity_weights[self.alpha :] = 0.0\n",
    "        return propensity_weights\n",
    "\n",
    "    def sample_treatment_weight(self):\n",
    "        return torch.tensor(self.treatment_weight)\n",
    "\n",
    "    def sample_intercept(self):\n",
    "        return torch.tensor(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_datasets = 5\n",
    "simulated_datasets = []\n",
    "\n",
    "# Data configuration\n",
    "p = 100\n",
    "alpha = 50\n",
    "beta = 50\n",
    "N_train = 500\n",
    "N_test = 500\n",
    "\n",
    "true_model = GroundTruthModel(p, alpha, beta)\n",
    "\n",
    "for _ in range(N_datasets):\n",
    "    # Generate data\n",
    "    D_train = Predictive(\n",
    "        true_model, num_samples=N_train, return_sites=[\"X\", \"A\", \"Y\"]\n",
    "    )()\n",
    "    D_test = Predictive(\n",
    "        true_model, num_samples=N_test, return_sites=[\"X\", \"A\", \"Y\"]\n",
    "    )()\n",
    "    simulated_datasets.append((D_train, D_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit parameters via maximum likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params = []\n",
    "for i in range(N_datasets):\n",
    "    # Generate data\n",
    "    D_train = simulated_datasets[i][0]\n",
    "\n",
    "    # Fit model using maximum likelihood\n",
    "    conditioned_model = ConditionedCausalGLM(\n",
    "        X=D_train[\"X\"], A=D_train[\"A\"], Y=D_train[\"Y\"]\n",
    "    )\n",
    "    \n",
    "    guide_train = pyro.infer.autoguide.AutoDelta(conditioned_model)\n",
    "    elbo = pyro.infer.Trace_ELBO()(conditioned_model, guide_train)\n",
    "\n",
    "    # initialize parameters\n",
    "    elbo()\n",
    "    adam = torch.optim.Adam(elbo.parameters(), lr=0.03)\n",
    "\n",
    "    # Do gradient steps\n",
    "    for _ in range(2000):\n",
    "        adam.zero_grad()\n",
    "        loss = elbo()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "\n",
    "    theta_hat = {\n",
    "        k: v.clone().detach().requires_grad_(True) for k, v in guide_train().items()\n",
    "    }\n",
    "    fitted_params.append(theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Query: Average treatment effect (ATE)\n",
    "\n",
    "The average treatment effect summarizes, on average, how much the treatment changes the response, $ATE = \\mathbb{E}[Y|do(A=1)] - \\mathbb{E}[Y|do(A=0)]$. The `do` notation indicates that the expectations are taken according to *intervened* versions of the model, with $A$ set to a particular value. Note from our [tutorial](tutorial_i.ipynb) that this is different from conditioning on $A$ in the original `causal_model`, which assumes $X$ and $T$ are dependent.\n",
    "\n",
    "\n",
    "To implement this query in ChiRho, we define the `ATEFunctional` class which take in a `model` and `guide` and returns the average treatment effect by simulating from the posterior predictive distribution of the model and guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the target functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATEFunctional(torch.nn.Module):\n",
    "    def __init__(self, model: Callable, *, num_monte_carlo: int = 100):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.num_monte_carlo = num_monte_carlo\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        with MultiWorldCounterfactual():\n",
    "            with pyro.plate(\"monte_carlo_functional\", size=self.num_monte_carlo, dim=-2):\n",
    "                with do(actions=dict(A=(torch.tensor(0.0), torch.tensor(1.0)))):\n",
    "                    Ys = self.model(*args, **kwargs)\n",
    "                Y0 = gather(Ys, IndexSet(A={1}), event_dim=0)\n",
    "                Y1 = gather(Ys, IndexSet(A={2}), event_dim=0)\n",
    "        ate = (Y1 - Y0).mean(dim=-2, keepdim=True).mean(dim=-1, keepdim=True).squeeze()\n",
    "        return pyro.deterministic(\"ATE\", ate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closed form doubly robust correction\n",
    "\n",
    "For the average treatment effect functional, there exists a closed-form analytical formula for the doubly robust correction. This formula is derived in Kennedy (2022) and is implemented below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form expression\n",
    "def closed_form_doubly_robust_ate_correction(X_test, theta) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    X = X_test[\"X\"]\n",
    "    A = X_test[\"A\"]\n",
    "    Y = X_test[\"Y\"]\n",
    "    pi_X = torch.sigmoid(X.mv(theta[\"propensity_weights\"]))\n",
    "    mu_X = (\n",
    "        X.mv(theta[\"outcome_weights\"])\n",
    "        + A * theta[\"treatment_weight\"]\n",
    "        + theta[\"intercept\"]\n",
    "    )\n",
    "    analytic_eif_at_test_pts = (A / pi_X - (1 - A) / (1 - pi_X)) * (Y - mu_X)\n",
    "    analytic_correction = analytic_eif_at_test_pts.mean()\n",
    "    return analytic_correction, analytic_eif_at_test_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing automated doubly robust correction via Monte Carlo\n",
    "\n",
    "While the doubly robust correction term is known in closed-form for the average treatment effect functional, our `one_step_correction` function in `ChiRho` works for a wide class of other functionals. We focus on the average treatment effect functional here so that we have a ground truth to compare `one_step_correction` against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analytic tensor(-0.1875, grad_fn=<MeanBackward0>)\n",
      "plug-in tensor(0.2130, grad_fn=<ExpandBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Helper class to create a trivial guide that returns the maximum likelihood estimate\n",
    "class MLEGuide(torch.nn.Module):\n",
    "    def __init__(self, mle_est: ParamDict):\n",
    "        super().__init__()\n",
    "        self.names = list(mle_est.keys())\n",
    "        for name, value in mle_est.items():\n",
    "            setattr(self, name + \"_param\", torch.nn.Parameter(value))\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        for name in self.names:\n",
    "            value = getattr(self, name + \"_param\")\n",
    "            pyro.sample(\n",
    "                name, pyro.distributions.Delta(value, event_dim=len(value.shape))\n",
    "            )\n",
    "\n",
    "# Compute doubly robust ATE estimates using both the automated and closed form expressions\n",
    "plug_in_ates = []\n",
    "analytic_corrections = []\n",
    "automated_monte_carlo_corrections = []\n",
    "for i in range(N_datasets):\n",
    "    theta_hat = fitted_params[i]\n",
    "    D_test = simulated_datasets[i][1]\n",
    "    mle_guide = MLEGuide(theta_hat)\n",
    "    functional = functools.partial(ATEFunctional, num_monte_carlo=10000)\n",
    "    ate_plug_in = functional(\n",
    "        PredictiveModel(CausalGLM(p), mle_guide)\n",
    "    )()\n",
    "    analytic_correction, analytic_eif_at_test_pts = closed_form_doubly_robust_ate_correction(D_test, theta_hat)\n",
    "    print(\"analytic\", analytic_correction.detach().item())\n",
    "    print(\"plug-in\", ate_plug_in.detach().item())\n",
    "    automated_monte_carlo_correction = tmle(\n",
    "        functional, \n",
    "        D_test,\n",
    "        learning_rate=5e-5,\n",
    "        n_grad_steps=1000,\n",
    "        n_tmle_steps=1,\n",
    "        num_nmc_samples=1000,\n",
    "        num_grad_samples=N_test,\n",
    "        verbose=True,\n",
    "        num_samples_outer=max(10000, 100 * p), \n",
    "        num_samples_inner=1,\n",
    "    )(PredictiveModel(CausalGLM(p), mle_guide))()\n",
    "\n",
    "    plug_in_ates.append(ate_plug_in.detach().item())\n",
    "    analytic_corrections.append(ate_plug_in.detach().item() + analytic_correction.detach().item())\n",
    "    automated_monte_carlo_corrections.append(automated_monte_carlo_correction.detach().item())\n",
    "\n",
    "plug_in_ates = np.array(plug_in_ates)\n",
    "analytic_corrections = np.array(analytic_corrections)\n",
    "automated_monte_carlo_corrections = np.array(automated_monte_carlo_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(\n",
    "    {\n",
    "        \"plug_in_ate\": plug_in_ates,\n",
    "        \"analytic_correction\": analytic_corrections,\n",
    "        \"automated_monte_carlo_correction\": automated_monte_carlo_corrections,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The true treatment effect is 0, so a mean estimate closer to zero is better\n",
    "results.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.kdeplot(\n",
    "    results['plug_in_ate'], \n",
    "    label=\"Plug-in\", ax=ax\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    results['automated_monte_carlo_correction'], \n",
    "    label=\"DR-Monte Carlo\", ax=ax\n",
    ")\n",
    "\n",
    "sns.kdeplot(\n",
    "    results['analytic_correction'], \n",
    "    label=\"DR-Analytic\", ax=ax\n",
    ")\n",
    "\n",
    "ax.axvline(0, color=\"black\", label=\"True ATE\", linestyle=\"--\")\n",
    "ax.set_yticks([])\n",
    "sns.despine()\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"ATE Estimate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    results['automated_monte_carlo_correction'],\n",
    "    results['analytic_correction'],\n",
    ")\n",
    "plt.plot(np.linspace(-.2, .5), np.linspace(-.2, .5), color=\"black\", linestyle=\"dashed\")\n",
    "plt.xlabel(\"DR-Monte Carlo\")\n",
    "plt.ylabel(\"DR-Analytic\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Kennedy, Edward. \"Towards optimal doubly robust estimation of heterogeneous causal effects\", 2022. https://arxiv.org/abs/2004.14497."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
