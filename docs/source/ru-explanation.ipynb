{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from typing import Dict, List, Optional,  Union, Callable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "from causal_pyro.indexed.ops import IndexSet, gather, indices_of, scatter\n",
    "from causal_pyro.interventional.handlers import do\n",
    "from causal_pyro.counterfactual.handlers import MultiWorldCounterfactual, Preemptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalpernPearlModifiedApproximate:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: Callable,\n",
    "        antecedents: Union[Dict[str, torch.Tensor], List[str]],\n",
    "        outcome: str,\n",
    "        witness_candidates: List[str],\n",
    "        observations: Optional[Dict[str, torch.Tensor]],\n",
    "        sample_size: int = 100,\n",
    "        event_dim: int = 0\n",
    "        ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.antecedents = antecedents\n",
    "        self.outcome = outcome\n",
    "        self.witness_candidates = witness_candidates\n",
    "        self.observations = observations\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        self.antecedents_dict = (\n",
    "            self.antecedents if isinstance(self.antecedents, dict)\n",
    "            else self.revert_antecedents(self.antecedents)\n",
    "        )\n",
    "    \n",
    "        self.preemptions = {candidate: functools.partial(self.preempt_with_factual,\n",
    "                                             antecedents = self.antecedents) for \n",
    "                                             candidate in self.witness_candidates}\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def revert_antecedents(antecedents: List[str]) -> Dict[str, Callable[[torch.Tensor], torch.Tensor]]:\n",
    "        return {antecedent: (lambda v: 1 - v) for antecedent in antecedents}\n",
    "\n",
    "    @staticmethod   \n",
    "    def preempt_with_factual(value: torch.Tensor, *,\n",
    "                          antecedents: List[str] = None, event_dim: int = 0):\n",
    "    \n",
    "        if antecedents is None:\n",
    "            antecedents = []\n",
    "\n",
    "        antecedents = [a for a in antecedents if a in indices_of(value, event_dim=event_dim)]\n",
    "\n",
    "        factual_value = gather(value, IndexSet(**{antecedent: {0} for antecedent in antecedents}),\n",
    "                                event_dim=event_dim)\n",
    "            \n",
    "        return scatter({\n",
    "            IndexSet(**{antecedent: {0} for antecedent in antecedents}): factual_value,\n",
    "            IndexSet(**{antecedent: {1} for antecedent in antecedents}): factual_value,\n",
    "        }, event_dim=event_dim)\n",
    "        \n",
    "        \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        with pyro.poutine.trace() as trace:\n",
    "            with MultiWorldCounterfactual():\n",
    "                with do(actions=self.antecedents_dict):\n",
    "                    with Preemptions(actions = self.preemptions):\n",
    "                        with pyro.condition(data={k: torch.as_tensor(v) for k, v in self.observations.items()}):\n",
    "                            with pyro.plate(\"plate\", self.sample_size):\n",
    "                                self.consequent = self.model()[self.outcome]\n",
    "                                self.intervened_consequent = gather(self.consequent, IndexSet(**{ant: {1} for ant in self.antecedents}))\n",
    "                                self.observed_consequent = gather(self.consequent, IndexSet(**{ant: {0} for ant in self.antecedents}))\n",
    "                                self.consequent_differs = self.intervened_consequent != self.observed_consequent   \n",
    "                                pyro.factor(\"consequent_differs\", torch.where(self.consequent_differs, torch.tensor(0.0), torch.tensor(-1e8)))\n",
    "                            \n",
    "        self.trace = trace.trace\n",
    "\n",
    "        # slightly hacky solution for odd witness candidate sets\n",
    "        if  isinstance(self.consequent_differs.squeeze().tolist(), bool):\n",
    "            self.existential_but_for = self.consequent_differs.squeeze()\n",
    "        else:\n",
    "            #if (len(self.consequent_differs.squeeze().tolist() )>1):\n",
    "            self.existential_but_for = any(self.consequent_differs.squeeze().tolist()                )  \n",
    "\n",
    "            \n",
    "\n",
    "        witness_dict = dict()\n",
    "        if self.witness_candidates:\n",
    "            witness_keys = [\"__split_\" + candidate for candidate in self.witness_candidates]\n",
    "            witness_dict = {key: self.trace.nodes[key]['value']  for key in witness_keys}\n",
    "            \n",
    "        witness_dict['observed'] = self.observed_consequent.squeeze()\n",
    "        witness_dict['intervened'] = self.intervened_consequent.squeeze()\n",
    "        witness_dict['consequent_differs'] = self.consequent_differs.squeeze()\n",
    "\n",
    "        # slightly hacky as above\n",
    "        self.witness_df = pd.DataFrame(witness_dict) if self.witness_candidates else witness_dict\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_conjunctive():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "    match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                       u_match_dropped, event_dim=0)\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "    forest_fire = pyro.deterministic(\"forest_fire\", torch.logical_and(match_dropped, lightning), event_dim=0).float()\n",
    "\n",
    "    return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n",
    "\n",
    "def ff_disjunctive():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.5))\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.5))\n",
    "\n",
    "    match_dropped = pyro.deterministic(\"match_dropped\",\n",
    "                                       u_match_dropped, event_dim=0)\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "    forest_fire = pyro.deterministic(\"forest_fire\", torch.logical_or(match_dropped, lightning), event_dim=0).float()\n",
    "\n",
    "    return {\"match_dropped\": match_dropped, \"lightning\": lightning,\n",
    "            \"forest_fire\": forest_fire}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def factivity_check(model, antecedents_dict, outcome_dict, observations):\n",
    "    \n",
    "    with pyro.condition(data={k: torch.as_tensor(v) for k, v in observations.items()}):\n",
    "        output = model()\n",
    "        factivity_tensors = {k: torch.as_tensor(v) for k, v in list(antecedents_dict.items()) + list(outcome_dict.items())}\n",
    "        return all([factivity_tensors[key] == output[key] for key in factivity_tensors.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_minimal_cause(model, antecedents, outcome, nodes, observations, runs_n):\n",
    "\n",
    "    cache = []\n",
    "    existential_but_fors = []\n",
    "    subset_is_a_minimal_causes = []\n",
    "    minimal_antecedents = []\n",
    "\n",
    "    for step in range(1,runs_n):\n",
    "        if outcome in nodes:\n",
    "            nodes.remove(outcome)\n",
    "\n",
    "        companion_size = random.randint(0,len(nodes))\n",
    "        companion_candidates = random.sample(nodes, companion_size)\n",
    "\n",
    "        if set(companion_candidates) in cache:\n",
    "            continue\n",
    "        \n",
    "        cache.append(set(companion_candidates))\n",
    "\n",
    "        witness_candidates = [node for node in nodes if \n",
    "                                node not in antecedents and \n",
    "                                node != outcome and \n",
    "                                    node not in companion_candidates]\n",
    "        \n",
    "        HPM = HalpernPearlModifiedApproximate(\n",
    "        model = model,\n",
    "        antecedents = companion_candidates,\n",
    "        outcome =  outcome,\n",
    "        witness_candidates = witness_candidates,\n",
    "        observations = observations,\n",
    "        sample_size = 1000)\n",
    "    \n",
    "        HPM()\n",
    "\n",
    "        existential_but_fors.append(HPM.existential_but_for)\n",
    "        if  not HPM.existential_but_for:\n",
    "            #existential_but_fors.append(HPM.existential_but_for)\n",
    "\n",
    "            continue\n",
    "        \n",
    "        subset_is_a_minimal_cause = any([s.issubset(set(HPM.antecedents)) for s in minimal_antecedents])\n",
    "        subset_is_a_minimal_causes.append(subset_is_a_minimal_cause)     \n",
    "\n",
    "        if subset_is_a_minimal_cause:\n",
    "            continue\n",
    "        minimal_antecedents.append(set(HPM.antecedents))\n",
    "\n",
    "        \n",
    "        for s in minimal_antecedents:\n",
    "            if set(HPM.antecedents).issubset(s) and s != set(HPM.antecedents):\n",
    "                minimal_antecedents.remove(s)  \n",
    "\n",
    "\n",
    "    return {\"sufficient_cause\": any([set(antecedents).issubset(s) for s in minimal_antecedents]),\n",
    "            \"actual_cause\": set(antecedents) in minimal_antecedents,\n",
    "                \"minimal_antecedents\" : minimal_antecedents, \"cache\": cache,\n",
    "                \"existential_but_fors\": existential_but_fors,\n",
    "                \"subset_is_a_minimal_causes\": subset_is_a_minimal_causes}\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sufficient_cause': True,\n",
       " 'actual_cause': False,\n",
       " 'minimal_antecedents': [{'lightning', 'match_dropped'}],\n",
       " 'cache': [set(),\n",
       "  {'lightning'},\n",
       "  {'lightning', 'match_dropped'},\n",
       "  {'match_dropped'}],\n",
       " 'existential_but_fors': [False, False, tensor(True), tensor(False)],\n",
       " 'subset_is_a_minimal_causes': [False]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.set_rng_seed(21)\n",
    "part_of_minimal_cause(model = ff_disjunctive, \n",
    "                        antecedents = ['lightning'],\n",
    "                        outcome =  'forest_fire',\n",
    "                        nodes =  ['match_dropped', 'lightning'],\n",
    "                        observations = {\"u_match_dropped\": 1., \"u_lightning\": 1.},\n",
    "                        runs_n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sufficient_cause': True,\n",
       " 'actual_cause': True,\n",
       " 'minimal_antecedents': [{'lightning'}, {'match_dropped'}],\n",
       " 'cache': [set(),\n",
       "  {'lightning'},\n",
       "  {'lightning', 'match_dropped'},\n",
       "  {'match_dropped'}],\n",
       " 'existential_but_fors': [False, True, tensor(True), tensor(True)],\n",
       " 'subset_is_a_minimal_causes': [False, True, False]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyro.set_rng_seed(21)\n",
    "part_of_minimal_cause(model = ff_conjunctive, \n",
    "                        antecedents = ['lightning'],\n",
    "                        outcome =  'forest_fire',\n",
    "                        nodes =  ['match_dropped', 'lightning'],\n",
    "                        observations = {\"u_match_dropped\": 1., \"u_lightning\": 1.},\n",
    "                        runs_n = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensurer': True,\n",
       " 'settings_cache': [[0.0, 0.0], [0.0, 1.0], [1.0, 1.0], [1.0, 0.0]],\n",
       " 'intervened_consequent': [1.0, 1.0, 1.0, 1.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensurer(model, exogenous_variables, antecedents_dict, outcome_dict, excluded_contexts = None,  runs_n = 100):\n",
    "\n",
    "    if excluded_contexts is None:\n",
    "        excluded_contexts = []  \n",
    "\n",
    "    settings_cache = []\n",
    "    intervened_consequent = []\n",
    "\n",
    "    outcome = list(outcome_dict.keys())[0]\n",
    "    antecedents = [key for key in antecedents_dict.keys()]\n",
    "\n",
    "    for step in range(1,runs_n):\n",
    "        \n",
    "        random_setting = [random.choice([0., 1.]) for _ in range(len(exogenous_variables))]\n",
    "        if random_setting in settings_cache or random_setting in excluded_contexts:\n",
    "            continue\n",
    "        \n",
    "        settings_cache.append(random_setting)\n",
    "\n",
    "        observations = {var: val for var, val in zip(exogenous_variables, random_setting)}\n",
    "\n",
    "        with pyro.condition(data={k: torch.as_tensor(v) for k, v in observations.items()}):\n",
    "                with MultiWorldCounterfactual():\n",
    "                    with do(actions=antecedents_dict):\n",
    "                        intervened_consequent.append(\n",
    "                             gather(model()[outcome], \n",
    "                                    IndexSet(**{ant: {1} for ant in antecedents})).squeeze().item())\n",
    "                        \n",
    "    return {\"ensurer\": all(intervened_consequent),\n",
    "            \"settings_cache\": settings_cache, \n",
    "            \"intervened_consequent\": intervened_consequent}\n",
    "\n",
    "    print(settings_cache)\n",
    "    print(observations)     \n",
    "    print(intervened_consequent)   \n",
    "    print(all(intervened_consequent))\n",
    "\n",
    "\n",
    "ensurer(model = ff_conjunctive,\n",
    "        exogenous_variables = [\"u_match_dropped\", \"u_lightning\"],\n",
    "        antecedents_dict = {\"match_dropped\": 1., \"lightning\": 1.},\n",
    "        outcome_dict = {\"forest_fire\": 1.},\n",
    "                        runs_n = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u_match_dropped': 1.0, 'u_lightning': 1.0}\n",
      "True\n",
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "settings_cache = []\n",
    "HPMs = []\n",
    "minimal_antecedents_cache = []\n",
    "nontrivial = False\n",
    "\n",
    "\n",
    "model = ff_disjunctive\n",
    "exogenous_variables = [\"u_match_dropped\", \"u_lightning\"]\n",
    "antecedents_dict = {\"match_dropped\": 1., \"lightning\": 1.}\n",
    "outcome_dict = {\"forest_fire\": 1.}\n",
    "nodes = [\"match_dropped\", \"lightning\"]\n",
    "witness_candidates = []\n",
    "excluded_contexts = None\n",
    "\n",
    "if excluded_contexts is None:\n",
    "        excluded_contexts = []  \n",
    "\n",
    "antecedents = [key for key in antecedents_dict.keys()]\n",
    "outcome = list(outcome_dict.keys())[0]\n",
    "if outcome in nodes:\n",
    "    nodes.remove(outcome)\n",
    "\n",
    "\n",
    "pyro.set_rng_seed(0)\n",
    "random_setting = [random.choice([0., 1.]) for _ in range(len(exogenous_variables))]\n",
    "\n",
    "#if random_setting in settings_cache or random_setting in excluded_contexts:\n",
    "#    continue\n",
    "\n",
    "settings_cache.append(random_setting)\n",
    "\n",
    "observations = {var: val for var, val in zip(exogenous_variables, random_setting)}\n",
    "\n",
    "print(observations)\n",
    "\n",
    "# nontriviality check\n",
    "with pyro.condition(data={k: torch.as_tensor(v) for k, v in observations.items()}):\n",
    "    output = model()\n",
    "    nontriviality_tensors = {k: torch.as_tensor(v) for k, v in list(antecedents_dict.items())}\n",
    "    nontrivial = not all([nontriviality_tensors[key] == output[key] for key in nontriviality_tensors.keys()])\n",
    "\n",
    "\n",
    "factivity = factivity_check(model = model,\n",
    "                antecedents_dict = antecedents_dict,\n",
    "                outcome_dict = outcome_dict, \n",
    "                observations = observations)\n",
    "\n",
    "\n",
    "print(factivity)\n",
    "#if not factivity:\n",
    "#    continue\n",
    "\n",
    "part_of_minimal = part_of_minimal_cause(model = model,\n",
    "                        antecedents = antecedents,\n",
    "                        outcome =  outcome,\n",
    "                        nodes = nodes,\n",
    "                        observations = observations,\n",
    "                        runs_n = 20)['sufficient_cause']\n",
    "\n",
    "print(part_of_minimal)\n",
    "\n",
    "ensured = ensurer(model = model,\n",
    "        exogenous_variables = exogenous_variables,\n",
    "        antecedents_dict = antecedents_dict,\n",
    "        outcome_dict = outcome_dict,\n",
    "                        runs_n = 50)['ensurer']\n",
    "\n",
    "print(ensured)\n",
    "\n",
    "print(nontrivial)\n",
    "\n",
    "explanation = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
