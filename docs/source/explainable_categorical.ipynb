{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable reasoning with ChiRho (categorical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Explainable Reasoning with ChiRho** package aims to provide a systematic, unified approach to causal explanation computations in terms of different probabilistic queries over expanded causal models that are constructed from a single generic program transformation applied to an arbitrary causal model represented as a ChiRho program. The approach of reducing causal queries to probabilistic computations on transformed causal models is the foundational idea behind all of ChiRho. The key strategy underlying \"causal explanation\" queries is their use of auxiliary variables representing uncertainty about what the proposed interventions are and which interventions or preemptions to apply, implicitly inducing a search space over counterfactuals.\n",
    "\n",
    "The goal of this notebook is to illustrate how the package can be used to provide an approximate method of answering a range of causal explanation queries with respect to models in which categorical variables play the key role. As the key tool will involve sampling-based posterior probability estimation, a lot of what will be said *mutatis mutandis* applies to more general settings where variables are continuous (to which we will devote another tutorial).\n",
    "\n",
    "In yet [another notebook](https://basisresearch.github.io/chirho/actual_causality.html) we illustrate how the module allows for a faithful reconstruction of a particular notion of local explanation (the so-called Halpern-Pearl modified definition of actual causality [(J. Halpern, MIT Press, 2016)](https://mitpress.mit.edu/9780262537131/actual-causality/)), which inspired some of the conceptual steps underlying the current implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "[Causal explanation and counterfactual thinking](#causal-explanation-and-counterfactual-thinking)     \n",
    "\n",
    "\n",
    "[Witness nodes and context sensitivity](#witness-nodes-and-context-sensitivity)\n",
    "\n",
    "[Probability of causation and responsibility](#probability-of-causation-and-responsibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal explanation and counterfactual thinking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following causality-related queries:\n",
    "\n",
    "- **Friendly Fire:** On March 24, 2002, A B-52 bomber fired a Joint Direct Attack Munition at a US battalion command post, killing three and injuring twenty special forces soldiers. Out of multiple potential contributing factors, which were actually responsible for the incident?\n",
    "\n",
    "- **Overshoot:** In dealing with an epidemic, multiple different policies were imposed, leading to the overshoot (the number of those who became infected after the peak of the epidemic) rising from around 15% in the unintervened model to around 25%. Which of the policies caused the overshoot and to what extent?\n",
    "\n",
    "- **Explainable AI:** Your pre-trial release has been refused based on your [COMPAS score](https://en.wikipedia.org/wiki/COMPAS_(software)). The decision was made using a proprietary predictive model. All you have access to is the questionnaire that was used, and perhaps some demographic information about a class of human beings subjected to this evaluation. But which of these factors resulted in your score being what it is, and what were their contributions?\n",
    "\n",
    "\n",
    "Questions of this sort are more local than those pertaining to average treatment effects, as they pertain to actual cases that come with their own contexts. Being able to answer them is useful for understanding how we can prevent undesirable outcomes similar to ones that we have observed, or promote the occurrence of desirable outcomes in contexts similar to the ones in which they had been observed. These context-sensitive causality questions are also an essential element of blame and responsibility assignments. If the phenomenon we're trying to explain is the behavior of a predictive model, we are dealing with a problem in explainable AI; but the underlying intuition behind the workings of **Explainable Reasoning with ChiRho** is that causally explaining the behavior of an opaque model is not that much different from providing a causal explanation of other real-world phenomena: we need to address such queries in a principled manner employing some approximate but hopefully reliable causal model of how things work (be that events outside of computers, or a predicitive model's behavior). **Explainable Reasoning with ChiRho** package aims to provide a unified general approach to the relevant causal explanation computations.\n",
    "\n",
    "At some level of generality, a useful point of departure is a general counterfactual one. On one hand, we can ask whether the event would have occurred had a given candidate cause not taken place. This is sometimes called  the *but-for test*, has a tradition of being used as a tool for answering causality and attribution queries. \n",
    "\n",
    "- It is often used in [the law of torts](https://plato.stanford.edu/entries/causation-law/) to determine if a defendant's conduct was the cause of a particular harm. The test is often formulated as follows: \"But for the defendant's conduct, would the harm have occurred?\" \n",
    "- A major philosophical position in the analysis of causality is that the definition of causal dependence should be formulated in terms of counterfactual conditionals (Lewis, 1973. “Causation”, Journal of Philosophy, 70: 556–67). On this approach, $e$ causally depends on $c$ if and only if, if $c$ were not to occur $e$ would not occur. (The view does not remain uncontested, see the [SEP entry on counterfactual theories of causation](https://plato.stanford.edu/entries/causation-counterfactual/)).\n",
    "- At least a few major approaches to explainable AI (such as [LIME](https://arxiv.org/abs/1602.04938),  or [Shapley values](https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html))  are based on the idea that explanations can be obtained by perturbing or shifting the input values and observing the changes in the output. This to a large extent can be thought of as a way of evaluating the but-for condition: if the input value was different, would the output value change? \n",
    " \n",
    "More generally, we can ask about the probability with which an alterantive intervention would lead to a cahnge in the outcome (perhaps while conditioning on other items of information), in line with the ideas present in Pearl's *Probabilities of causation...* and Chapter 9 of Pearl's *Causality*. While immensely useful, the but-for condition is not fine-grained enough to answer all the questions we are interested in or to give us the intended answers in cases in which the underlying causal model is non-trivial. We will illustrate this observation in this tutorial. \n",
    "\n",
    "\n",
    "On the other hand, we can ask whether given our model (and perhaps conditioning on other pieces of information we posses), intervening on a given candidate cause to have a given value results in the outcome being as observed (or, more generally, the probability of that outcome being as observed) - this is conceptually similar to Pearl's probability of sufficiency. \n",
    "\n",
    "We will start with these two approaches, but soon we will notice that often our explanatory questions are more local and a more fine-grained tool is needed. The general intuition (inspired by Halpern's *Actual Causality*) that we implemented is that when we ask local explanatory questions, we need to keep some part of the actual context fixed and consider alternative scenarios insofar as potential causes are involved. That is, we (i) search through possible alternative interventions that could be performed on the candidate cause nodes, (ii) search through possible context nodes that are to be intervened to be at their factual values even in the counterfactual worlds, (iii) see how these options play out in intervened worlds, and (iv) investigate and meaningfully summarize what happens with the outcome nodes of interest in all those counterfactual worlds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a very simple model, in which a forest fire can be caused by exactly one of two things: a match being dropped (`match_dropped`), or a lightning strike (`lightning`), and either of these factors alone is already deterministically sufficient for the `forest_fire` to occur. A match being dropped is more likely than a lightning strike (we use fairly large probabilities for the sake of example transparency). For the sake of illustration, we also include a causally irrelevant site representing whether a ChiRho developer smiles, `smile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=-1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=-1\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import math\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import torch\n",
    "from chirho.counterfactual.handlers.counterfactual import \\\n",
    "    MultiWorldCounterfactual\n",
    "from chirho.explainable.handlers import ExtractSupports, SearchForExplanation\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "from chirho.observational.handlers import condition\n",
    "\n",
    "pyro.settings.set(module_local_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_infer(\n",
    "    model: Optional[Callable] = None, *, num_samples: int\n",
    "):\n",
    "    \n",
    "    if model is None:\n",
    "        return lambda m: importance_infer(m, num_samples=num_samples)\n",
    "\n",
    "    def _wrapped_model(\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        guide = pyro.poutine.block(hide_fn=lambda msg: msg[\"is_observed\"])(model)\n",
    "\n",
    "        max_plate_nesting = 9  # TODO guess\n",
    "\n",
    "        with pyro.poutine.block(), MultiWorldCounterfactual() as mwc:\n",
    "            log_weights, importance_tr, _ = pyro.infer.importance.vectorized_importance_weights(\n",
    "                model,\n",
    "                guide,\n",
    "                *args,\n",
    "                num_samples=num_samples,\n",
    "                max_plate_nesting=max_plate_nesting,\n",
    "                normalized=False,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "            # resample using importance weights to get posterior samples\n",
    "            idx = dist.Categorical(logits=log_weights).sample((num_samples,))\n",
    "            for name, node in importance_tr.nodes.items():\n",
    "                if node[\"type\"] != \"sample\" or pyro.poutine.util.site_is_subsample(node) or node[\"is_observed\"]:\n",
    "                    continue\n",
    "                importance_tr.nodes[name][\"value\"] = torch.index_select(\n",
    "                    importance_tr.nodes[name][\"value\"],\n",
    "                    -max_plate_nesting - 1 - len(importance_tr.nodes[name][\"fn\"].event_shape),\n",
    "                    idx,\n",
    "                )\n",
    "\n",
    "        with pyro.poutine.replay(trace=importance_tr), mwc:\n",
    "            trace = pyro.poutine.trace(model).get_trace(*args, **kwargs)\n",
    "\n",
    "        return torch.logsumexp(log_weights, dim=0) - math.log(num_samples), trace, mwc\n",
    "\n",
    "    return _wrapped_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"438pt\" height=\"116pt\"\n",
       " viewBox=\"0.00 0.00 437.99 116.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 112)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-112 433.99,-112 433.99,4 -4,4\"/>\n",
       "<!-- u_match_dropped -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>u_match_dropped</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"92.94\" cy=\"-90\" rx=\"92.88\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"92.94\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">u_match_dropped</text>\n",
       "</g>\n",
       "<!-- match_dropped -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>match_dropped</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"86.94\" cy=\"-18\" rx=\"82.59\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"86.94\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">match_dropped</text>\n",
       "</g>\n",
       "<!-- u_match_dropped&#45;&gt;match_dropped -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>u_match_dropped&#45;&gt;match_dropped</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.46,-71.7C90.8,-63.98 90,-54.71 89.27,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"92.75,-45.77 88.41,-36.1 85.77,-46.37 92.75,-45.77\"/>\n",
       "</g>\n",
       "<!-- forest_fire -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>forest_fire</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"244.94\" cy=\"-18\" rx=\"57.69\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.94\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">forest_fire</text>\n",
       "</g>\n",
       "<!-- u_match_dropped&#45;&gt;forest_fire -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>u_match_dropped&#45;&gt;forest_fire</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M127.43,-73.12C150.4,-62.54 180.66,-48.6 204.72,-37.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"206.32,-40.64 213.94,-33.28 203.4,-34.28 206.32,-40.64\"/>\n",
       "</g>\n",
       "<!-- u_lightning -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>u_lightning</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"366.94\" cy=\"-90\" rx=\"63.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"366.94\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">u_lightning</text>\n",
       "</g>\n",
       "<!-- lightning -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>lightning</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" stroke-dasharray=\"5,2\" cx=\"372.94\" cy=\"-18\" rx=\"52.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"372.94\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">lightning</text>\n",
       "</g>\n",
       "<!-- u_lightning&#45;&gt;lightning -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>u_lightning&#45;&gt;lightning</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M368.42,-71.7C369.09,-63.98 369.88,-54.71 370.62,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"374.11,-46.37 371.48,-36.1 367.13,-45.77 374.11,-46.37\"/>\n",
       "</g>\n",
       "<!-- u_lightning&#45;&gt;forest_fire -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>u_lightning&#45;&gt;forest_fire</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M340.17,-73.64C322.49,-63.49 299.13,-50.09 279.98,-39.1\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"281.65,-36.03 271.23,-34.09 278.17,-42.1 281.65,-36.03\"/>\n",
       "</g>\n",
       "<!-- smile -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>smile</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"244.94\" cy=\"-90\" rx=\"36\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"244.94\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">smile</text>\n",
       "</g>\n",
       "<!-- smile&#45;&gt;forest_fire -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>smile&#45;&gt;forest_fire</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M244.94,-71.7C244.94,-63.98 244.94,-54.71 244.94,-46.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"248.44,-46.1 244.94,-36.1 241.44,-46.1 248.44,-46.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7477a59094b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forest_fire_model():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.7))\n",
    "    match_dropped = pyro.deterministic(\n",
    "        \"match_dropped\", u_match_dropped, event_dim=0\n",
    "    )  # notice uneven probs here\n",
    "\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.4))\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "\n",
    "    # this is a causally irrelevant site\n",
    "    smile = pyro.sample(\"smile\", dist.Bernoulli(0.5))\n",
    "\n",
    "    forest_fire = pyro.deterministic(\n",
    "        \"forest_fire\", torch.max(match_dropped, lightning) + (0 * smile), event_dim=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"match_dropped\": match_dropped,\n",
    "        \"lightning\": lightning,\n",
    "        \"forest_fire\": forest_fire,\n",
    "    }\n",
    "\n",
    "with ExtractSupports() as extract_supports:\n",
    "    forest_fire_model()\n",
    "    forest_fire_supports = {k: constraints.boolean for k in extract_supports.supports}\n",
    "\n",
    "pyro.render_model(forest_fire_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we assume all nodes are binary and use $'$ as negation. Once we specify (i) the distributions for the nodes we use (`supports`), (ii) candidate causes $X_i = x_i$ (`antecedents`) (iii) their alternative values ($X_i = x_i'$), (iv) elements of the current context (`witnesses`), and  (v) the `consequents` of interest $Y=y$. The  `SearchForExplanation` handler transforms the original model into one in which interventions and alternative interventions on the antecedents are applied in parallel counterfactual worlds stochastically preempted and context elements are stochastically selected and preempted to be kept at the factual values in all counterfactual worlds.\n",
    "\n",
    "First, let's go back to our original query. Let $F$ be the `forest_fire`, $f$ stand for $F=1$, $f'$ for $F=0$, $M$ stand for `match_dropped`, with analogous conventions. We also place interventions conditioned on in subscripts, so that, for example\n",
    "$P(f_m')$ stands for $P(F=1\\vert do(M=0))$.\n",
    "\n",
    "We are currently interested in $P(f'_{m'}, f_m)$, that is the probability of both forest fire not occurring if we intervene on the match to not be dropped, and forest fire occurring if we intervene on the match to be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, suppose we are interested in asking the question of whether dropping a match has causal power over whether the forest fire occurs. We assume all relevant nodes are binary. The potential cause (`antecedent`) we're considering is `match_dropped=1`, we contrast it with what would happen if we intervened on it to not happen (`alternatives`). We are interested in whether an outcome variable (`consequent`) has value 1 under these two interventions. The counterfactual world in which we intervene with `alternatives` is world 1, and the counterfactual world in which we intervene with `antecedents` is world 2. We will be interested in cases in which none of these interventions have been preempted (more about this later), so we will sample with appropriate masks as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3002)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={}, # potential context elements, we leave them empty for now\n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(forest_fire_model)\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interestingly, in cases of overdetermination, a similar estimation would lead us to assign no causal role to any of to co-contributing factors. This can be seen in the context in which both causes occurred. Trivially, if lightning occurred, then had no match been dropped, the forest fire, caused by lighning, would still occur (a symmetric reasoning goes through for the lightning as well), $P(f'_{m'}\\vert m, l) = P(f'_{l'}\\vert m, l)=0$. Intuitively, these quantities are not good guides to the causal role of `match_dropped` and `lightning`, as we think they did played a causal role. This is the first illustration of why the but-for analysis is not fine-grained enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={}, # potential context elements, we leave them empty for now\n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(condition(\n",
    "    data={\"match_dropped\": torch.tensor(1.0), \"lightning\": torch.tensor(1.0)}\n",
    ")(forest_fire_model))\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Witness nodes and context sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these intuitions in the forest fire example may be salvaged by considering a two-membered antecedent set, estimating $P(f'_{m',l'}, f_{m,l})$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4375)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": 1.0, \"lightning\": 1.0},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={},\n",
    "    alternatives={\"match_dropped\": 0.0, \"lightning\": 0.0},\n",
    ")(forest_fire_model)\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This already suggests a more complicated picture, as it turns out that we need to pay attention to membership in larger antecedent sets that would make a difference (that is one reason why we need stochasticity in antecedent candidate preemption: to search for such sets).\n",
    "\n",
    "But even then, the but-for analysis does not pay sufficient attention to the granularity of a given problem and its causal structure. There are asymmetric cases where the efficiency of one cause prevents the efficiency of another, in which our causal attributions should also be asymmetric, but \"being a member of the same larger antecedent set\" isn't.\n",
    "\n",
    "A simple example is breaking a bottle. Suppose Sally and Bob throw a rock at a bottle, and Sally does so a little earlier than Bob. Suppose both are perfectly accurate, and the bottle shatters when hit. Sally hits, and the bottle shatters, but Bob doesn't hit it because the bottle is no longer there.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"960pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 959.93 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-328 955.93,-328 955.93,4 -4,4\"/>\n",
       "<!-- prob_sally_throws -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>prob_sally_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"527.44\" cy=\"-306\" rx=\"94.78\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.44\" y=\"-302.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_sally_throws</text>\n",
       "</g>\n",
       "<!-- sally_throws -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>sally_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"527.44\" cy=\"-234\" rx=\"68.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.44\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">sally_throws</text>\n",
       "</g>\n",
       "<!-- prob_sally_throws&#45;&gt;sally_throws -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>prob_sally_throws&#45;&gt;sally_throws</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M527.44,-287.7C527.44,-279.98 527.44,-270.71 527.44,-262.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"530.94,-262.1 527.44,-252.1 523.94,-262.1 530.94,-262.1\"/>\n",
       "</g>\n",
       "<!-- prob_bill_throws -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>prob_bill_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"174.44\" cy=\"-234\" rx=\"87.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"174.44\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bill_throws</text>\n",
       "</g>\n",
       "<!-- bill_throws -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>bill_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"226.44\" cy=\"-162\" rx=\"61.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"226.44\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">bill_throws</text>\n",
       "</g>\n",
       "<!-- prob_bill_throws&#45;&gt;bill_throws -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>prob_bill_throws&#45;&gt;bill_throws</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.03,-216.05C193.29,-207.63 200.98,-197.28 207.9,-187.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"210.82,-189.9 213.97,-179.79 205.2,-185.73 210.82,-189.9\"/>\n",
       "</g>\n",
       "<!-- prob_sally_hits -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>prob_sally_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"360.44\" cy=\"-234\" rx=\"79.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"360.44\" y=\"-230.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_sally_hits</text>\n",
       "</g>\n",
       "<!-- sally_hits -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>sally_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"360.44\" cy=\"-162\" rx=\"53.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"360.44\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">sally_hits</text>\n",
       "</g>\n",
       "<!-- prob_sally_hits&#45;&gt;sally_hits -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>prob_sally_hits&#45;&gt;sally_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M360.44,-215.7C360.44,-207.98 360.44,-198.71 360.44,-190.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"363.94,-190.1 360.44,-180.1 356.94,-190.1 363.94,-190.1\"/>\n",
       "</g>\n",
       "<!-- prob_bill_hits -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>prob_bill_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"73.44\" cy=\"-162\" rx=\"73.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"73.44\" y=\"-158.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bill_hits</text>\n",
       "</g>\n",
       "<!-- bill_hits -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>bill_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"255.44\" cy=\"-90\" rx=\"47.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"255.44\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">bill_hits</text>\n",
       "</g>\n",
       "<!-- prob_bill_hits&#45;&gt;bill_hits -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>prob_bill_hits&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M111.15,-146.5C141.03,-135.01 182.74,-118.96 213.63,-107.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.18,-110.24 223.26,-103.38 212.67,-103.7 215.18,-110.24\"/>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_sally -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>prob_bottle_shatters_if_sally</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"517.44\" cy=\"-90\" rx=\"143.77\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"517.44\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bottle_shatters_if_sally</text>\n",
       "</g>\n",
       "<!-- bottle_shatters -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>bottle_shatters</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"431.44\" cy=\"-18\" rx=\"81.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"431.44\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">bottle_shatters</text>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_sally&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>prob_bottle_shatters_if_sally&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M496.63,-72.05C485.56,-63.05 471.79,-51.84 459.77,-42.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"461.74,-39.15 451.77,-35.55 457.32,-44.57 461.74,-39.15\"/>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_bill -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>prob_bottle_shatters_if_bill</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"815.44\" cy=\"-90\" rx=\"136.48\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"815.44\" y=\"-86.3\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bottle_shatters_if_bill</text>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_bill&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>prob_bottle_shatters_if_bill&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M738.66,-75C669.65,-62.42 569.12,-44.1 502.07,-31.87\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"502.48,-28.39 492.02,-30.04 501.23,-35.28 502.48,-28.39\"/>\n",
       "</g>\n",
       "<!-- sally_throws&#45;&gt;sally_hits -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>sally_throws&#45;&gt;sally_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M492.43,-218.33C466.08,-207.28 429.87,-192.1 402.04,-180.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"403.15,-177.11 392.58,-176.47 400.45,-183.56 403.15,-177.11\"/>\n",
       "</g>\n",
       "<!-- bill_throws&#45;&gt;bill_hits -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>bill_throws&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M233.46,-144.05C236.73,-136.18 240.69,-126.62 244.34,-117.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.69,-118.85 248.29,-108.28 241.23,-116.17 247.69,-118.85\"/>\n",
       "</g>\n",
       "<!-- sally_hits&#45;&gt;bill_hits -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>sally_hits&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M337.4,-145.64C322.53,-135.72 302.99,-122.7 286.72,-111.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"288.33,-108.72 278.07,-106.09 284.45,-114.54 288.33,-108.72\"/>\n",
       "</g>\n",
       "<!-- sally_hits&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>sally_hits&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M357.19,-143.87C354.51,-125.12 352.81,-94.72 364.44,-72 370.85,-59.48 381.71,-48.96 392.83,-40.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"395.11,-43.37 401.33,-34.79 391.12,-37.62 395.11,-43.37\"/>\n",
       "</g>\n",
       "<!-- bill_hits&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>bill_hits&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M286.91,-76.49C313.94,-65.73 353.32,-50.07 384.15,-37.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"385.83,-40.91 393.83,-33.96 383.24,-34.4 385.83,-40.91\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7477e9d46830>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stones_model():\n",
    "    prob_sally_throws = pyro.sample(\"prob_sally_throws\", dist.Beta(1, 1))\n",
    "    prob_bill_throws = pyro.sample(\"prob_bill_throws\", dist.Beta(1, 1))\n",
    "    prob_sally_hits = pyro.sample(\"prob_sally_hits\", dist.Beta(1, 1))\n",
    "    prob_bill_hits = pyro.sample(\"prob_bill_hits\", dist.Beta(1, 1))\n",
    "    prob_bottle_shatters_if_sally = pyro.sample(\n",
    "        \"prob_bottle_shatters_if_sally\", dist.Beta(1, 1)\n",
    "    )\n",
    "    prob_bottle_shatters_if_bill = pyro.sample(\n",
    "        \"prob_bottle_shatters_if_bill\", dist.Beta(1, 1)\n",
    "    )\n",
    "\n",
    "    sally_throws = pyro.sample(\"sally_throws\", dist.Bernoulli(prob_sally_throws))\n",
    "    bill_throws = pyro.sample(\"bill_throws\", dist.Bernoulli(prob_bill_throws))\n",
    "\n",
    "    # if Sally throws, she hits with probability prob_sally_hits\n",
    "    # hits with pr=0 otherwise\n",
    "    new_shp = torch.where(sally_throws == 1, prob_sally_hits, 0.0)\n",
    "\n",
    "    sally_hits = pyro.sample(\"sally_hits\", dist.Bernoulli(new_shp))\n",
    "\n",
    "    # if Bill throws, he hits with probability prob_bill_hits\n",
    "    # if sally doesn't hit sooner,\n",
    "    # misses otherwise\n",
    "    new_bhp = torch.where(\n",
    "        bill_throws.bool() & (~sally_hits.bool()),\n",
    "        prob_bill_hits,\n",
    "        torch.tensor(0.0),\n",
    "    )\n",
    "\n",
    "    bill_hits = pyro.sample(\"bill_hits\", dist.Bernoulli(new_bhp))\n",
    "\n",
    "    # you can use a analogous move to model the bottle shattering\n",
    "    # if being hit by a stone doesn't deterministically\n",
    "    # shatter the bottle\n",
    "    new_bsp = torch.where(\n",
    "        bill_hits.bool(),\n",
    "        prob_bottle_shatters_if_bill,\n",
    "        torch.where(\n",
    "            sally_hits.bool(),\n",
    "            prob_bottle_shatters_if_sally,\n",
    "            torch.tensor(0.0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    bottle_shatters = pyro.sample(\"bottle_shatters\", dist.Bernoulli(new_bsp))\n",
    "\n",
    "    return {\n",
    "        \"sally_throws\": sally_throws,\n",
    "        \"bill_throws\": bill_throws,\n",
    "        \"sally_hits\": sally_hits,\n",
    "        \"bill_hits\": bill_hits,\n",
    "        \"bottle_shatters\": bottle_shatters,\n",
    "    }\n",
    "\n",
    "\n",
    "with ExtractSupports() as extract_supports:\n",
    "    stones_model()\n",
    "    stones_supports = {k: constraints.boolean if not k.startswith(\"prob_\") else v for k, v in extract_supports.supports.items()}\n",
    "\n",
    "pyro.render_model(stones_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let us assume that the relevant probabilities are 1 (this forces both Sally and Bill to throw stones, makes them perfectly accurate and makes the bottle always shatter if hit). Let us start with the type of analysis we performed for the forest fire case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={},\n",
    "    alternatives={\"sally_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sally's throw does not satisfy the but-for condition: if she hadn't thrown the rock, the bottle would still have shattered. Of course, the combined event of Sally throwing a rock and Bob throwing a rock is a but-for cause of the bottle shattering. But that doesn't capture the clear asymmetry at work here. Intuitively, Sally's throw is the (actual) cause of the bottle breaking in a way that Bob's throw isn't.  Sally's throw actually caused the bottle to shatter and Bob's throw didn't, in part because Bob's stone didn't actually hit the bottle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive solution to the problem, inspired by the  Pearl-Halpern definition of actual causality (which we discuss in [another notebook](https://basisresearch.github.io/chirho/actual_causality.html)) is to say that **in answering actual causality queries, we need to consider what happens when part of the actual context is kept fixed.** For instance, in the bottle shattering example, given the observed fact that Bob’s stone didn’t hit, in the counterfactual world in which we keep this observed fact fixed, if Sally nad not thrown the stone, the bottle in fact would not have shattered. \n",
    "\n",
    "\n",
    "For this reason, our handler allows not only stochastic preemption of interventions (to approximate the search through possible antecedent sets) but also stochastic witness preemption of those nodes that are considered part of the context (these needn't exclude each other). In a witness preemption, we ensure that the counterfactual value is identical to the factual one (and by applying it randomly to candidate witness nodes, we approximate a search through all possible context sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2521)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"bill_hits\": None},\n",
    "    alternatives={\"sally_throws\": torch.tensor(0.0)},\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, our search through contexts is very simple and degenerate, as the only part of the actual context which stochastically is kept fixed at the factual value is `bill_hits`. But already with this search, sally throwing is diagnosed as having non-null probability. In fact, the definition of actual causality in Halpern's book (*Actual causality*) contains an existential quantifier: a variable is an actual cause if there is at least one context in which a change in the outcome variable would result from changing the antecedent to have an alternative value, so our search provides a correct diagnosis here.\n",
    "\n",
    "Crucally, as intended, an analogous inference for whether `bill_throws` is a cause of the bottle shattering, yields a different\n",
    "result and assigns null causal role to bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"bill_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"sally_hits\": None},\n",
    "    alternatives={\"bill_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=10000)(query)()\n",
    "print(logp.exp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of causation and responsibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might use non-trivial probabilities and be interested in more involved queries. Suppose we aren't sure what part of the context we want to hold fixed, allowing both `sally_hits` and `bill_hits` to be witness candidates, so we attach equal weights to all four possible context sets. \n",
    "\n",
    "Suppose also that beyond knowing the non-degenerate probabilities involved, we don't know who threw the stone, and we only observed the bottle has been shattered. We can use the handler to estimate the answer to a somewhat different question involving the probabilities that changing the value of `sally_throws` or changing the value of `billy_throws` (whatever these are in the factual world) would lead to a change in the outcome variables, and that fixing them to be at the factual values would result in the outcome variable having the same value. We also allow both `sally_hits` and `bill_hits` as potential witnesses.\n",
    "\n",
    "For example, we can sample to estimate quantities such as the fraction of possible causes of the bottle shattering in which Sally and Billy are each responsibile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree of responsibility of Sally: tensor(0.7581)\n",
      "Degree of responsibility of Billy: tensor(0.6069)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": None, \"bill_throws\": None},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"sally_hits\": None, \"bill_hits\": None},\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(0.8),\n",
    "        \"prob_bill_throws\": torch.tensor(0.7),\n",
    "        \"prob_sally_hits\": torch.tensor(0.9),\n",
    "        \"prob_bill_hits\": torch.tensor(0.8),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(0.9),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(0.8),\n",
    "        \"bottle_shatters\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc = importance_infer(num_samples=20000)(query)()\n",
    "\n",
    "nodes = trace.nodes[\"_RETURN\"][\"value\"]\n",
    "with mwc:\n",
    "    st_responsible = gather(nodes[\"sally_throws\"], IndexSet(sally_throws={1})) != \\\n",
    "        gather(nodes[\"sally_throws\"], IndexSet(sally_throws={2}))\n",
    "    bt_responsible = gather(nodes[\"bill_throws\"], IndexSet(bill_throws={1})) != \\\n",
    "        gather(nodes[\"bill_throws\"], IndexSet(bill_throws={2}))\n",
    "\n",
    "print(\"Degree of responsibility of Sally:\", st_responsible.sum() / st_responsible.numel())\n",
    "print(\"Degree of responsibility of Billy:\", bt_responsible.sum() / bt_responsible.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we assumed Sally to be more likely to throw, more likely to hit, and more likely to shatter the bottle if she hits. For this reason, we expect her to be more likely to be causally responsible for the outcome. Conceptually, these estimates are impacted by some hyperparameters, such as witness preemption probabilities, so perhaps a bit more clarity on can be gained if we think we have a complete list of potential causes and normalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro1.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
