{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable reasoning with ChiRho (categorical variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Explainable Reasoning with ChiRho** package aims to provide a systematic, unified approach to causal explanation computations. The package provides a single generic program transformation that can be applied to any arbitrary causal model representable as a Chirho program. This program transformation allows several causal explanation queries to be modeled in terms of probabilistic queries. This approach of reducing causal queries to probabilistic computations on transformed causal models is the foundational idea behind all of ChiRho and in this module, has been leveraged for causal explanations as well.\n",
    "\n",
    "The goal of this notebook is to illustrate how the package can be used to provide an approximate method of answering a range of causal explanation queries in causal models with only categorical variables. As the key tool will involve sampling-based posterior probability estimation, a lot of what will be said *mutatis mutandis* applies to more general settings where variables are continuous (to which we will devote another tutorial).\n",
    "\n",
    "In yet [another notebook](https://basisresearch.github.io/chirho/actual_causality.html) we illustrate how the module allows for a faithful reconstruction of a particular notion of local explanation (the so-called Halpern-Pearl modified definition of actual causality [(J. Halpern, MIT Press, 2016)](https://mitpress.mit.edu/9780262537131/actual-causality/)), which inspired some of the conceptual steps underlying the current implementation.\n",
    "\n",
    "Before proceeding, the readers should go through the introductory tutorials on [causal reasoning in Chirho](https://basisresearch.github.io/chirho/tutorial_i.html). They might also find a notebook on [actual causality](https://basisresearch.github.io/chirho/actual_causality.html) helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outline**\n",
    "\n",
    "[Motivation](#motivation)\n",
    "\n",
    "[Setup](#setup)\n",
    "\n",
    "[But-for Causal Explanations](#but-for-causal-explanations)     \n",
    "\n",
    "[Context-sensitive Causal Explanations](#context-sensitive-causal-explanations)\n",
    "\n",
    "[Probability of causation and responsibility](#probability-of-causation-and-responsibility)\n",
    "\n",
    "[Further Discussion](#further-discussion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Consider the following causality-related queries:\n",
    "\n",
    "- **Friendly Fire:** On March 24, 2002, A B-52 bomber fired a Joint Direct Attack Munition at a US battalion command post, killing three and injuring twenty special forces soldiers. Out of multiple potential contributing factors, which were actually responsible for the incident?\n",
    "\n",
    "- **Overshoot:** In dealing with an epidemic, multiple different policies were imposed, leading to the overshoot (the number of those who became infected after the peak of the epidemic) rising from around 15% in the unintervened model to around 25%. Which of the policies caused the overshoot and to what extent?\n",
    "\n",
    "- **Explainable AI:** Your pre-trial release has been refused based on your [COMPAS score](https://en.wikipedia.org/wiki/COMPAS_(software)). The decision was made using a proprietary predictive model. All you have access to is the questionnaire that was used, and perhaps some demographic information about a class of human beings subjected to this evaluation. But which of these factors resulted in your score being what it is, and what were their contributions?\n",
    "\n",
    "Questions of this sort are more specific and local as they pertain to actual cases that come with their own contexts, unlike average treatment effects discussed in an earlier [tutorial](https://github.com/BasisResearch/chirho/blob/master/docs/source/tutorial_i.ipynb). Being able to answer such context-sensitive questions is useful for understanding how we can prevent undesirable outcomes and promote desirable ones in contexts similar to the ones in which they had been observed. Moreover, these context-sensitive causality questions are also an essential element of blame and responsibility assignments. \n",
    "\n",
    "In this notebook, we demonstrate the use of `SearchForExplanation`, a handler that provides a unified approach to answering such questions on a wide range of levels of granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=-1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=-1\n",
    "from typing import Callable, Dict, List, Optional\n",
    "\n",
    "import math\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.distributions.constraints as constraints\n",
    "import torch\n",
    "from chirho.counterfactual.handlers.counterfactual import \\\n",
    "    MultiWorldCounterfactual\n",
    "from chirho.explainable.handlers import ExtractSupports, SearchForExplanation\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "from chirho.observational.handlers import condition\n",
    "from chirho.observational.handlers.soft_conditioning import soft_eq, KernelSoftConditionReparam\n",
    "\n",
    "pyro.settings.set(module_local_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first setup the essentials for performing probabilistic inference on the transformed causal models. We define a function for performing importance sampling on a model and a few other utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_infer(\n",
    "    model: Optional[Callable] = None, *, num_samples: int\n",
    "):\n",
    "    \n",
    "    if model is None:\n",
    "        return lambda m: importance_infer(m, num_samples=num_samples)\n",
    "\n",
    "    def _wrapped_model(\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        guide = pyro.poutine.block(hide_fn=lambda msg: msg[\"is_observed\"])(model)\n",
    "\n",
    "        max_plate_nesting = 9  # TODO guess\n",
    "\n",
    "        with pyro.poutine.block(), MultiWorldCounterfactual() as mwc:\n",
    "            log_weights, importance_tr, _ = pyro.infer.importance.vectorized_importance_weights(\n",
    "                model,\n",
    "                guide,\n",
    "                *args,\n",
    "                num_samples=num_samples,\n",
    "                max_plate_nesting=max_plate_nesting,\n",
    "                normalized=False,\n",
    "                **kwargs\n",
    "            )\n",
    "\n",
    "        return torch.logsumexp(log_weights, dim=0) - math.log(num_samples), importance_tr, mwc, log_weights\n",
    "\n",
    "    return _wrapped_model\n",
    "\n",
    "# The following functions are needed for conditioning on random variables defined using `pyro.deterministic`\n",
    "def _soft_eq(v1: torch.Tensor, v2: torch.Tensor) -> torch.Tensor:\n",
    "    return soft_eq(constraints.boolean, v1, v2, scale=0.001)\n",
    "\n",
    "def reparam_config(data):\n",
    "    return {i: KernelSoftConditionReparam(_soft_eq) for i in data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But-for Causal Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a very simple model, in which a forest fire can be caused by any of the two things: a match being dropped (`match_dropped`), or a lightning strike (`lightning`), and either of these factors alone is already deterministically sufficient for the `forest_fire` to occur. A match being dropped is more likely than a lightning strike (we use fairly large probabilities for the sake of example transparency). For illustration, we also include a causally irrelevant site representing whether a ChiRho developer smiles, `smile`. A but-for analysis assigns causal role to a node if it having a different value would result in a different outcome. We will implement this concept and reflect on it in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (20240704.0754)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"749pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 749.27 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-40 745.27,-40 745.27,4 -4,4\"/>\n",
       "<!-- u_match_dropped -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>u_match_dropped</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"77.97\" cy=\"-18\" rx=\"77.97\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"77.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">u_match_dropped</text>\n",
       "</g>\n",
       "<!-- match_dropped -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>match_dropped</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"242.97\" cy=\"-18\" rx=\"68.75\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"242.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">match_dropped</text>\n",
       "</g>\n",
       "<!-- u_lightning -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>u_lightning</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"382.97\" cy=\"-18\" rx=\"53.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">u_lightning</text>\n",
       "</g>\n",
       "<!-- lightning -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>lightning</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"498.97\" cy=\"-18\" rx=\"44.19\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"498.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">lightning</text>\n",
       "</g>\n",
       "<!-- smile -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>smile</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"592.97\" cy=\"-18\" rx=\"31.39\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"592.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">smile</text>\n",
       "</g>\n",
       "<!-- forest_fire -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>forest_fire</title>\n",
       "<ellipse fill=\"gray\" stroke=\"black\" cx=\"691.97\" cy=\"-18\" rx=\"49.3\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"691.97\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">forest_fire</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1215e65c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forest_fire_model():\n",
    "    u_match_dropped = pyro.sample(\"u_match_dropped\", dist.Bernoulli(0.7))\n",
    "    match_dropped = pyro.deterministic(\n",
    "        \"match_dropped\", u_match_dropped, event_dim=0\n",
    "    )  # notice uneven probs here\n",
    "\n",
    "    u_lightning = pyro.sample(\"u_lightning\", dist.Bernoulli(0.4))\n",
    "    lightning = pyro.deterministic(\"lightning\", u_lightning, event_dim=0)\n",
    "\n",
    "    # this is a causally irrelevant site\n",
    "    smile = pyro.sample(\"smile\", dist.Bernoulli(0.5))\n",
    "\n",
    "    forest_fire = pyro.deterministic(\n",
    "        \"forest_fire\", torch.max(match_dropped, lightning) + (0 * smile), event_dim=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"match_dropped\": match_dropped,\n",
    "        \"lightning\": lightning,\n",
    "        \"forest_fire\": forest_fire,\n",
    "    }\n",
    "\n",
    "with ExtractSupports() as extract_supports:\n",
    "    forest_fire_model()\n",
    "    forest_fire_supports = {k: constraints.boolean for k in extract_supports.supports}\n",
    "\n",
    "pyro.render_model(forest_fire_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we further go into causal queries, let us describe some notation. Let $F$ refer to the `forest_fire`, $f$ stand for $F=1$, $f'$ for $F=0$. The notation $M$ stands for `match_dropped`, with analogous conventions. We also place interventions conditioned on in subscripts. As an example, $f_{m'}$ stands for $F=1$ when $do(M=0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this tutorial, we consider different kinds of causal queries and compute them using a unified program transformation, which takes place using the handler `SearchForExplanation`. It takes the following inputs:\n",
    "1. the distributions for the variables we use (`supports`),\n",
    "2. the candidate causes $X_i = x_i$ (`antecedents`),\n",
    "3. their alternative values ($X_i = x_i'$) (`alternatives`),\n",
    "4. the candidate elements of the current context (`witnesses`), and  \n",
    "5. the `consequents` of interest $Y=y$. \n",
    "\n",
    "The  `SearchForExplanation` handler then takes these arguments and transforms the original model into another model in which interventions on antecedents and witnesses are applied stochastically. Once the antecedents $A \\subseteq$ `antecedents` and witnesses $W \\subseteq$ `witnesses` are chosen from the candidates via sampling, parallel counterfactual worlds are created to condition on `A` being sufficient and necessary causes for the consequent with the context `W`. For more details on `SearchForExplanation`, please refer to the [documentation](https://basisresearch.github.io/chirho/explainable.html#chirho.explainable.handlers.explanation.SearchForExplanation).\n",
    "\n",
    "Now we are ready to use `SearchForExplanation` for answering but-for causal questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Query 1** What is the probability that dropping a match has a causal impact on the forest fire?\n",
    "\n",
    "To answer the above question, we compute the probability of both the forest fire not occurring if we intervene on the match to not be dropped (the \"but-for\" part), and the forest fire occurring if we intervene on the match to be dropped, that is, $P(f'_{m'}, f_m)$. This computation can be carried out using `SearchForExplanation`.\n",
    "\n",
    "The potential cause (`antecedent`) we're considering is `match_dropped=1`. We inspect what would happen if we intervened on it to not happen (`alternatives`), and what happens if we intervene on it to happen, do(`match_dropped=1`). We are interested in whether (and with what probability) an outcome variable `forest_fire` (`consequent`) has both value 0 under the first (this is the \"but-for\" part) and value 1 under the second intervention. Note that these two interventions correspond to `match_dropped=1` being a necessary and sufficient cause for `forest_fire=1`. In this simple case, the notion corresponds to Pearl's notion of probability of necessity and sufficiency - although what `SearchForExplanation` can do goes beyond it, for instance, by allowing for the estimands to be context-sensitive, as we will illustrate later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2987)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={}, # potential context elements, we leave them empty for now\n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(forest_fire_model)\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above, strictly speaking, is not our answer yet. Remember that interventions on antecedents are chosen stochastically (with default probability $0.5$ for each candidate node). Thus the above is rather $P(f'_{m'}, f_m)P(m \\text{ was intervened on})$. To obtain $P(f'_{m'}, f_m)$ we therefore need to multiply the result by 2, obtaining $0.6$. In general, we don't need to keep track of the analytic solutions, and we can reach the similar conclusion by post-processing the samples to reject those where `match_dropped` was not intervened on. So, without knowing or using an analytic form (which in general will not be manageable), to compute $P(f'_{m'}, f_m)$, we can subselect the samples as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6000)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_match_dropped\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result above matches our intuition. `match_dropped` has a causal effect on `forest_fire` only when `lightning` is not there and the probability of that happening is $0.6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Causal Query 2** What is the probability that a Chirho developer has a causal impact on the forest fire?\n",
    "\n",
    "The intuitive answer is obviously zero, and we show that the same conclusion can be drawn using `SearchForExplanation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0011e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"smile\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={}, \n",
    "    alternatives={\"smile\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(forest_fire_model)\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above probability is already 0, and the following post-processing does not affect the result. We still provide the following code snippet for the sake of completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0011e-05)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_smile\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above show how `SearchForExplanation` can be used for but-for analysis. Note, however, that such analysis would not work in a case of overdetermination, where each of the two factors can alone cause the outcome. Consider the case where both `match_dropped` and `lightning` did occur. In this case, if we try to determine the causal role of `match_dropped`, it would come out to be zero (a symmetric reasoning works for lightning as well). This results in $P(f'_{m'}, f_m, m, l) = P(f'_{l'}, f_l, m, l)=0$. This is a canonical example of the limitations of the but-for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8055e-06)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"smile\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={}, \n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(  # We need to reparametrize as we are conditioning on deterministic nodes\n",
    "    pyro.poutine.reparam(config=reparam_config([\"match_dropped\", \"lightning\"]))(\n",
    "        condition(data={\"match_dropped\": torch.tensor(1.0), \"lightning\": torch.tensor(1.0)})\n",
    "            (forest_fire_model)\n",
    "    ))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7924e-06)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_match_dropped\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do, is to consider the set containing both `match_dropped` and `lightning`. Then we can estimate $P(f'_{m',l'}, f_{m,l}, m, l)$ to determine their joint causal role, which comes out to be greater than 0, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0670)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": torch.tensor(1.0), \"lightning\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={},\n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0), \"lightning\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(\n",
    "    pyro.poutine.reparam(config=reparam_config([\"match_dropped\", \"lightning\"]))(\n",
    "        condition(data={\"match_dropped\": torch.tensor(1.0), \"lightning\": torch.tensor(1.0)})\n",
    "            (forest_fire_model)\n",
    "    ))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to get our estimand of $P(f_{m, l}, f_{m', l'}, m, l)$, we would need to multiply our result by four, as we now have made two stochastic decisions about interventions, each with probability $0.5$. Or, we can post-process the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2772)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = (trace.nodes[\"__cause____antecedent_match_dropped\"][\"value\"] == 0) & (trace.nodes[\"__cause____antecedent_lightning\"][\"value\"] == 0)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again matches our intuition. Conditioning brings the factivity requirement into the picture, and so now what we are estimating is the probability of `m` and `l` in fact happening *and* having a causal impact on the forest fire. Since in general, the two-element set has deterministically complete control over the forest fire, the only non-trivial probability is the one brought in by the factivity requirement - the probability of both `match_dropped=1` and `lightning=1`, which is $0.28$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also be interested in computing the causal impact of the set without factoring in the factivity requirement, so that the result mirrors the intuition that the two-element set has complete control over the outcome. To get to this point, one can compute $P(f_{m, l}, f'_{m', l'} | m, l)$ as follows by subselecting the samples with `match_dropped=1` and `lightning=1`. Since {`match_dropped=1`, `lightning=1`} always leads to `forest_fire=1` and {`match_dropped=0`, `lightning=0`} always leads to `forest_fire=0`, we have $P(f_{m, l}, f'_{m', l'} | m, l) = 1$, which we get as a result of the following code snippet.\n",
    "\n",
    "Note that $P(f_{m, l}, f'_{m', l'} | m, l)$ can also be computed as $\\frac{P(f_{m, l}, f'_{m', l'}, m, l)}{P(m, l)}$ which requires computing $P(m, l)$ using the original `forest_fire_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=forest_fire_supports,\n",
    "    antecedents={\"match_dropped\": torch.tensor(1.0), \"lightning\": torch.tensor(1.0)},\n",
    "    consequents={\"forest_fire\": torch.tensor(1.0)},\n",
    "    witnesses={},\n",
    "    alternatives={\"match_dropped\": torch.tensor(0.0), \"lightning\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(forest_fire_model)\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "\n",
    "mask_intervened = (trace.nodes[\"__cause____antecedent_match_dropped\"][\"value\"] == 0) & (trace.nodes[\"__cause____antecedent_lightning\"][\"value\"] == 0) & (trace.nodes[\"match_dropped\"][\"value\"] == 1) & (trace.nodes[\"lightning\"][\"value\"] == 1)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context-sensitive Causal Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the previous example showed, the but-for analysis is not sufficient for identifying causal roles. This induces the need to pay attention to the membership of variables in larger antecedent sets that would make a difference (that is one reason why we need stochasticity in the antecedent candidate preemption: to search for such sets). But even then, the but-for analysis does not pay sufficient attention to the granularity of a given problem and its causal structure. There are asymmetric cases where the efficiency of one cause prevents the efficiency of another, in which our causal attributions should a be asymmetric, but \"being a member of the same larger antecedent set\" isn't. We illustrate using a simple example.\n",
    "\n",
    "Consider the example of breaking a bottle. Suppose Sally and Bob throw a rock at a bottle, and Sally does so a little earlier than Bob. Suppose both are perfectly accurate, and the bottle shatters when hit. Sally hits, and the bottle shatters, but Bob doesn't hit it because the bottle is no longer there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.0.0 (20240704.0754)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"818pt\" height=\"332pt\"\n",
       " viewBox=\"0.00 0.00 817.89 332.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 328)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-328 813.89,-328 813.89,4 -4,4\"/>\n",
       "<!-- prob_sally_throws -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>prob_sally_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"456.61\" cy=\"-306\" rx=\"79.5\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"456.61\" y=\"-300.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_sally_throws</text>\n",
       "</g>\n",
       "<!-- sally_throws -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>sally_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"456.61\" cy=\"-234\" rx=\"58\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"456.61\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">sally_throws</text>\n",
       "</g>\n",
       "<!-- prob_sally_throws&#45;&gt;sally_throws -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>prob_sally_throws&#45;&gt;sally_throws</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M456.61,-287.7C456.61,-280.41 456.61,-271.73 456.61,-263.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"460.11,-263.62 456.61,-253.62 453.11,-263.62 460.11,-263.62\"/>\n",
       "</g>\n",
       "<!-- prob_bill_throws -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>prob_bill_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"152.61\" cy=\"-234\" rx=\"74.38\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.61\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bill_throws</text>\n",
       "</g>\n",
       "<!-- bill_throws -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>bill_throws</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"195.61\" cy=\"-162\" rx=\"52.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"195.61\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">bill_throws</text>\n",
       "</g>\n",
       "<!-- prob_bill_throws&#45;&gt;bill_throws -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>prob_bill_throws&#45;&gt;bill_throws</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.24,-215.7C168.03,-207.9 173.8,-198.51 179.13,-189.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.06,-191.75 184.31,-181.39 176.1,-188.08 182.06,-191.75\"/>\n",
       "</g>\n",
       "<!-- prob_sally_hits -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>prob_sally_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"312.61\" cy=\"-234\" rx=\"67.73\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.61\" y=\"-228.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_sally_hits</text>\n",
       "</g>\n",
       "<!-- sally_hits -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>sally_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"312.61\" cy=\"-162\" rx=\"46.23\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"312.61\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">sally_hits</text>\n",
       "</g>\n",
       "<!-- prob_sally_hits&#45;&gt;sally_hits -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>prob_sally_hits&#45;&gt;sally_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M312.61,-215.7C312.61,-208.41 312.61,-199.73 312.61,-191.54\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"316.11,-191.62 312.61,-181.62 309.11,-191.62 316.11,-191.62\"/>\n",
       "</g>\n",
       "<!-- prob_bill_hits -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>prob_bill_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"62.61\" cy=\"-162\" rx=\"62.61\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62.61\" y=\"-156.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bill_hits</text>\n",
       "</g>\n",
       "<!-- bill_hits -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>bill_hits</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"219.61\" cy=\"-90\" rx=\"41.12\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"219.61\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">bill_hits</text>\n",
       "</g>\n",
       "<!-- prob_bill_hits&#45;&gt;bill_hits -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>prob_bill_hits&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M95.52,-146.33C120.5,-135.19 154.89,-119.86 181.13,-108.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"182.38,-111.43 190.09,-104.16 179.53,-105.04 182.38,-111.43\"/>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_sally -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>prob_bottle_shatters_if_sally</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"446.61\" cy=\"-90\" rx=\"118.4\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"446.61\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bottle_shatters_if_sally</text>\n",
       "</g>\n",
       "<!-- bottle_shatters -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>bottle_shatters</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"373.61\" cy=\"-18\" rx=\"64.66\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"373.61\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">bottle_shatters</text>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_sally&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>prob_bottle_shatters_if_sally&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M428.94,-72.05C420.06,-63.54 409.14,-53.07 399.35,-43.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"401.85,-41.23 392.21,-36.84 397.01,-46.28 401.85,-41.23\"/>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_bill -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>prob_bottle_shatters_if_bill</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"696.61\" cy=\"-90\" rx=\"113.28\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"696.61\" y=\"-84.95\" font-family=\"Times,serif\" font-size=\"14.00\">prob_bottle_shatters_if_bill</text>\n",
       "</g>\n",
       "<!-- prob_bottle_shatters_if_bill&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>prob_bottle_shatters_if_bill&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M631.64,-74.92C574.16,-62.46 490.91,-44.42 434.65,-32.23\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"435.42,-28.81 424.91,-30.12 433.94,-35.66 435.42,-28.81\"/>\n",
       "</g>\n",
       "<!-- sally_throws&#45;&gt;sally_hits -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>sally_throws&#45;&gt;sally_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M426.42,-218.33C404.44,-207.64 374.5,-193.08 350.84,-181.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"352.69,-178.59 342.16,-177.37 349.63,-184.89 352.69,-178.59\"/>\n",
       "</g>\n",
       "<!-- bill_throws&#45;&gt;bill_hits -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>bill_throws&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M201.54,-143.7C204.1,-136.24 207.16,-127.32 210.02,-118.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"213.32,-120.13 213.26,-109.54 206.7,-117.86 213.32,-120.13\"/>\n",
       "</g>\n",
       "<!-- sally_hits&#45;&gt;bill_hits -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>sally_hits&#45;&gt;bill_hits</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M291.97,-145.46C279.15,-135.82 262.49,-123.28 248.39,-112.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.86,-110.14 240.76,-106.92 246.65,-115.73 250.86,-110.14\"/>\n",
       "</g>\n",
       "<!-- sally_hits&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>sally_hits&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M310.51,-143.7C309.01,-125.08 308.86,-95.03 319.61,-72 324.79,-60.9 333.36,-50.86 342.16,-42.58\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"344.22,-45.42 349.43,-36.19 339.6,-40.16 344.22,-45.42\"/>\n",
       "</g>\n",
       "<!-- bill_hits&#45;&gt;bottle_shatters -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>bill_hits&#45;&gt;bottle_shatters</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M247.5,-76.32C270.68,-65.78 304.07,-50.61 330.66,-38.52\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"331.82,-41.84 339.48,-34.52 328.92,-35.47 331.82,-41.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1723e7c10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stones_model():\n",
    "    prob_sally_throws = pyro.sample(\"prob_sally_throws\", dist.Beta(1, 1))\n",
    "    prob_bill_throws = pyro.sample(\"prob_bill_throws\", dist.Beta(1, 1))\n",
    "    prob_sally_hits = pyro.sample(\"prob_sally_hits\", dist.Beta(1, 1))\n",
    "    prob_bill_hits = pyro.sample(\"prob_bill_hits\", dist.Beta(1, 1))\n",
    "    prob_bottle_shatters_if_sally = pyro.sample(\n",
    "        \"prob_bottle_shatters_if_sally\", dist.Beta(1, 1)\n",
    "    )\n",
    "    prob_bottle_shatters_if_bill = pyro.sample(\n",
    "        \"prob_bottle_shatters_if_bill\", dist.Beta(1, 1)\n",
    "    )\n",
    "\n",
    "    sally_throws = pyro.sample(\"sally_throws\", dist.Bernoulli(prob_sally_throws))\n",
    "    bill_throws = pyro.sample(\"bill_throws\", dist.Bernoulli(prob_bill_throws))\n",
    "\n",
    "    # if Sally throws, she hits with probability prob_sally_hits\n",
    "    # hits with pr=0 otherwise\n",
    "    new_shp = torch.where(sally_throws == 1, prob_sally_hits, 0.0)\n",
    "\n",
    "    sally_hits = pyro.sample(\"sally_hits\", dist.Bernoulli(new_shp))\n",
    "\n",
    "    # if Bill throws, he hits with probability prob_bill_hits\n",
    "    # if sally doesn't hit sooner,\n",
    "    # misses otherwise\n",
    "    new_bhp = torch.where(\n",
    "        bill_throws.bool() & (~sally_hits.bool()),\n",
    "        prob_bill_hits,\n",
    "        torch.tensor(0.0),\n",
    "    )\n",
    "\n",
    "    bill_hits = pyro.sample(\"bill_hits\", dist.Bernoulli(new_bhp))\n",
    "\n",
    "    # you can use a analogous move to model the bottle shattering\n",
    "    # if being hit by a stone doesn't deterministically\n",
    "    # shatter the bottle\n",
    "    new_bsp = torch.where(\n",
    "        bill_hits.bool(),\n",
    "        prob_bottle_shatters_if_bill,\n",
    "        torch.where(\n",
    "            sally_hits.bool(),\n",
    "            prob_bottle_shatters_if_sally,\n",
    "            torch.tensor(0.0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    bottle_shatters = pyro.sample(\"bottle_shatters\", dist.Bernoulli(new_bsp))\n",
    "\n",
    "    return {\n",
    "        \"sally_throws\": sally_throws,\n",
    "        \"bill_throws\": bill_throws,\n",
    "        \"sally_hits\": sally_hits,\n",
    "        \"bill_hits\": bill_hits,\n",
    "        \"bottle_shatters\": bottle_shatters,\n",
    "    }\n",
    "\n",
    "\n",
    "with ExtractSupports() as extract_supports:\n",
    "    stones_model()\n",
    "    stones_supports = {k: constraints.boolean if not k.startswith(\"prob_\") else v for k, v in extract_supports.supports.items()}\n",
    "\n",
    "pyro.render_model(stones_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now let us assume that the relevant probabilities are 1 (this forces both Sally and Bill to throw stones, makes them perfectly accurate and makes the bottle always shatter if hit). Let us start with the type of analysis we performed for the forest fire case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={},\n",
    "    alternatives={\"sally_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_sally_throws\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sally's throw does not satisfy the but-for condition: if she hadn't thrown the rock, the bottle would still have shattered. Of course, the combined event of Sally throwing a rock and Bob throwing a rock is a but-for cause of the bottle shattering. But that doesn't capture the clear asymmetry at work here. Intuitively, Sally's throw is the (actual) cause of the bottle breaking in a way that Bob's throw isn't.  Sally's throw actually caused the bottle to shatter and Bob's throw didn't, in part because Bob's stone didn't actually hit the bottle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An intuitive solution to the problem, inspired by the  Pearl-Halpern definition of actual causality (which we discuss in [another notebook](https://basisresearch.github.io/chirho/actual_causality.html)) is to say that **in answering actual causality queries, we need to consider what happens when part of the actual context is kept fixed.** For instance, in the bottle shattering example, given the observed fact that Bob’s stone didn’t hit, in the counterfactual world in which we keep this observed fact fixed, if Sally had not thrown the stone, the bottle in fact would not have shattered. \n",
    "\n",
    "\n",
    "For this reason, our handler allows not only stochastic preemption of interventions (to approximate the search through possible antecedent sets) but also stochastic witness preemption of those nodes that are considered part of the context (these needn't exclude each other). In a witness preemption, we ensure that the counterfactual value is identical to the factual one (and by applying it randomly to candidate witness nodes, we approximate a search through all possible context sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2513)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"bill_hits\": None},\n",
    "    alternatives={\"sally_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=100000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5019)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_sally_throws\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Admittedly, our search through contexts is simple as the only part of the actual context which stochastically is kept fixed at the factual value is `bill_hits`. But already with this search, Sally's throw is diagnosed as having impact on the bottle shattering with non-null probability. In fact, the definition of actual causality in Halpern's book (*Actual causality*) contains an existential quantifier: a variable is an actual cause if there is at least one context in which a change in the outcome variable would result from changing the antecedent to have an alternative value, so our search provides a correct diagnosis here.\n",
    "\n",
    "Crucially, as intended, an analogous inference for whether `bill_throws` is a cause of the bottle shattering, yields a different result and assigns null causal role to Bill's throw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"bill_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"sally_hits\": None},\n",
    "    alternatives={\"bill_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5,\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(1.0),\n",
    "        \"prob_bill_throws\": torch.tensor(1.0),\n",
    "        \"prob_sally_hits\": torch.tensor(1.0),\n",
    "        \"prob_bill_hits\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(1.0),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=10000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0013e-05)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = trace.nodes[\"__cause____antecedent_bill_throws\"][\"value\"] == 0\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.squeeze())/torch.sum(mask_intervened.float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of Causation and Responsibility\n",
    "\n",
    "In the examples above, we have shown how `SearchForExplanation` can be used to perform but-for analysis and context-sensitive analysis. In this section, we extend how we can combine these queries ina single model and perform more involved queries about probabilities of causation and responsibility.\n",
    "\n",
    "We take the earlier defined `stones_model` with non-trivial probabilities and the single observation that the bottle was shattered. We do not know who threw the stone and thus it is not obvious what context to hold fixed. We can capture all these different possibilities using the single program transformation performed by `SearchForExplanation` and post-process the samples to answer different queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1543)\n"
     ]
    }
   ],
   "source": [
    "query = SearchForExplanation(\n",
    "    supports=stones_supports,\n",
    "    antecedents={\"sally_throws\": torch.tensor(1.0), \"bill_throws\": torch.tensor(1.0)},\n",
    "    consequents={\"bottle_shatters\": torch.tensor(1.0)},\n",
    "    witnesses={\"bill_hits\": None, \"sally_hits\": None},\n",
    "    alternatives={\"sally_throws\": torch.tensor(0.0), \"bill_throws\": torch.tensor(0.0)},\n",
    "    consequent_scale=1e-5\n",
    ")(condition(\n",
    "    data={\n",
    "        \"prob_sally_throws\": torch.tensor(0.8),\n",
    "        \"prob_bill_throws\": torch.tensor(0.7),\n",
    "        \"prob_sally_hits\": torch.tensor(0.9),\n",
    "        \"prob_bill_hits\": torch.tensor(0.8),\n",
    "        \"prob_bottle_shatters_if_sally\": torch.tensor(0.9),\n",
    "        \"prob_bottle_shatters_if_bill\": torch.tensor(0.8),\n",
    "        \"bottle_shatters\": torch.tensor(1.0),\n",
    "    }\n",
    ")(stones_model))\n",
    "\n",
    "logp, trace, mwc, log_weights = importance_infer(num_samples=100000)(query)()\n",
    "print(torch.exp(logp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how our earlier analysis on the `stones_model` can be carried out through some analysis on the samples we get through this model where we have both `sally_throw` and `bill_throws` as candidate causes and both `bill_hits` and `sally_hits` as context nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the probability of causation for `sally_throws`. We compute the probability that the set {`sally_throws=1`} is the cause of bottle shattering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2195)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = (trace.nodes[\"__cause____antecedent_sally_throws\"][\"value\"] == 0) & (trace.nodes[\"__cause____antecedent_bill_throws\"][\"value\"] == 1)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.float().squeeze())/mask_intervened.float().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We similarly compute this probability for `bill_throws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0667)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = (trace.nodes[\"__cause____antecedent_sally_throws\"][\"value\"] == 1) & (trace.nodes[\"__cause____antecedent_bill_throws\"][\"value\"] == 0)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.float().squeeze())/mask_intervened.float().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the same model as above to compute the degree of responsibility for bill and sally as follows. We interpret the degree of responsibility assigned to sally for bottle shattering as the probability that `sally_throws=1` is part of the cause. Similarly, the degree of responsibility assigned to billy for shattering the bottle is the probability that `billy_throws=1` is a part of the cause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2777)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = (trace.nodes[\"__cause____antecedent_sally_throws\"][\"value\"] == 0)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.float().squeeze())/mask_intervened.float().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2014)\n"
     ]
    }
   ],
   "source": [
    "mask_intervened = (trace.nodes[\"__cause____antecedent_bill_throws\"][\"value\"] == 0)\n",
    "print(torch.sum(torch.exp(log_weights) * mask_intervened.float().squeeze())/mask_intervened.float().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we assumed Sally to be more likely to throw, more likely to hit, and more likely to shatter the bottle if she hits. For this reason, we expect her to be more likely to be causally responsible for the outcome and that is the result we got. Conceptually, these estimates are impacted by some hyperparameters, such as witness preemption probabilities, so perhaps a bit more clarity on can be gained if we think we have a complete list of potential causes and normalize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Discussion\n",
    "\n",
    "In this notebook, we have shown how `SearchForExplanation` can be used for fine-grained causal queries for discrete causal models. We further elaborate on its application in for different queries. \n",
    "\n",
    "**Explainable AI**: If the phenomenon we're trying to explain is the behavior of a predictive model, we are dealing with a problem in explainable AI; but the underlying intuition behind the workings of **Explainable Reasoning with ChiRho** is that causally explaining the behavior of an opaque model is not that much different from providing a causal explanation of other real-world phenomena: we need to address such queries in a principled manner employing some approximate but hopefully reliable causal model of how things work (be that events outside of computers, or a predicitive model's behavior). **Explainable Reasoning with ChiRho** package aims to provide a unified general approach to the relevant causal explanation computations. At least a few major approaches to explainable AI (such as [LIME](https://arxiv.org/abs/1602.04938),  or [Shapley values](https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html))  are based on the idea that explanations can be obtained by perturbing or shifting the input values and observing the changes in the output. This to a large extent can be thought of as a way of evaluating the but-for condition: if the input value was different, would the output value change? \n",
    "\n",
    "**Other Applications**: Causal queries, specifically but-for tests are often used in [the law of torts](https://plato.stanford.edu/entries/causation-law/) to determine if a defendant's conduct was the cause of a particular harm. The test is often formulated as follows: \"But for the defendant's conduct, would the harm have occurred?\". A major philosophical position in the analysis of causality is that the definition of causal dependence should be formulated in terms of counterfactual conditionals (Lewis, 1973. “Causation”, Journal of Philosophy, 70: 556–67). On this approach, $e$ causally depends on $c$ if and only if, if $c$ were not to occur $e$ would not occur. (The view does not remain uncontested, see the [SEP entry on counterfactual theories of causation](https://plato.stanford.edu/entries/causation-counterfactual/))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro1.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
